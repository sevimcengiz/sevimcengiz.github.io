<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Peer review on Sevim Cengiz</title><link>https://sevimcengiz.github.io/tags/Peer-review/</link><description>Recent content in Peer review on Sevim Cengiz</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 02 Jan 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://sevimcengiz.github.io/tags/Peer-review/index.xml" rel="self" type="application/rss+xml"/><item><title>How to reject a rejection</title><link>https://sevimcengiz.github.io/blog/2017/01/02/how-to-reject-a-rejection/</link><pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2017/01/02/how-to-reject-a-rejection/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>For a junior scientist, it can be a major blow when their manuscript is rejected. They have poured many months to years of their time into this project, have submitted the paper where they think it belongs, and the editor puts an end to their aspirations by rejecting the submission. However, more experienced scientists, in particular those with editorial roles at major journals, know very well that many a rejection is not final. Often, a rejection is only the first step in an ongoing negotiation with the journal, one that frequently ends with the eventual publication of the article. To level the playing field between the junior and the more senior scientists, here I’ll reveal this secret to the world: How to reject a rejection.&lt;/p>
&lt;p>There are basically two strategies that you can pursue, appeal to the editor or resubmit anyways. In the following, I’ll briefly discuss the mechanics of each option and then give my opinion of which option should be used when.&lt;/p>
&lt;div id="appeal-to-the-editor" class="section level2">
&lt;h2>Appeal to the editor&lt;/h2>
&lt;p>An appeal is a request to the editor to overturn the decision. Typically, a successful appeal will change the decision from “reject” to “major revision”, i.e., it will buy you the right to revise and resubmit. Some journals have complex and formal appeals processes while others handle appeals more informally. In all cases, you will initiate the appeal by contacting the editor and explaining why you believe the reviewer criticisms were either unwarranted or can be fully addressed in a revision. The initial contact to the editor could consist of just a brief email explaining the main issues, or it could be accompanied by a detailed point-by-point response to the reviewer comments.&lt;/p>
&lt;p>How a journal handles an appeal depends on the journal’s policies and procedures as well as the specific appeal request you are making. The journal may send out your original manuscript to another reviewer, they may send your point-by-point response to the original reviewers, or they may involve one or more editors who didn’t handle the original submission. They may also ask you for more information, such as a detailed point-by-point response to the reviewer comments (if you haven’t sent one yet) or a revised manuscript draft.&lt;/p>
&lt;p>An appeal can be a long, drawn-out procedure, in particular if you initiate it with just an email to the editor. The editor may take a week or two to respond to your original email, asking you for a detailed point-by-point response. Once you submit that, the editor may have it reviewed by multiple people (the original reviewers, new reviewers, or other editors), and this process may take as long as a typical review would take. After all this time has passed, the editor may then tell you that they need to see a revised manuscript before they can make any sort of decision. The revised manuscript will then again have to be re-reviewed, of course, and this review process will likely prompt further requests for revision, even in the best-case scenario that the appeal is ultimately successful.&lt;/p>
&lt;/div>
&lt;div id="resubmit-anyways" class="section level2">
&lt;h2>Resubmit anyways&lt;/h2>
&lt;p>As an alternative to filing a formal appeal, you can also just go ahead, revise your manuscript, and resubmit. This will have to be under the guise that you have sufficiently revised the manuscript to the point where it can now be considered a new submission. The unethical way of doing this would be to change the title, change the abstract, and hope the editor won’t notice. I do not recommend this approach. The ethical way to proceed is to submit as a new submission but state clearly in the cover letter that an earlier version of this paper was previously reviewed and rejected. You should also submit a detailed response to the reviewer comments. The journal submission system may not have a special option to do so, since it thinks you’re submitting a new article, but you can always just upload your response as a supplemental file and point to it in the cover letter.&lt;/p>
&lt;/div>
&lt;div id="which-option-is-preferable" class="section level2">
&lt;h2>Which option is preferable?&lt;/h2>
&lt;p>Given the two options of either appealing or resubmitting anyways, most people would intuitively choose to appeal. Resubmitting without prior approval feels wrong and somewhat sneaky; most people need to know that they are welcome to resubmit before they feel comfortable doing it. Further, the act of appealing feels right: The reviewers were stupid, the editors didn’t get it, and I want to protest!&lt;/p>
&lt;p>However, if you consider the two options from the perspective of the editor, you’ll see that filing an appeal is almost always the worse option. The appeal, by its very nature, creates an adversarial relationship with the editor. You’re telling the editor they were wrong and need to change their decision. This adversarial relationship can make the editor negatively predisposed towards you. An appeal only makes sense, in my opinion, when the reviews were truly biased or otherwise off (e.g., contained unprofessional ad-hominem attacks), so that there is no way you can revise the manuscript to address the reviewer comments.&lt;/p>
&lt;p>If you’re going the “submit as new manuscript” route, you’re putting the editor in a position where they are more likely to be positively predisposed towards you, even though it may not seem that way. First, note that you’re in effect asking the editor for a favor, namely the favor of connecting this new submission to the history of the previous submission and to use (some of) the previous reviewers. And asking somebody to do you a favor is a great way to get them to like you.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> Second, you’re providing the editor with a submission that is easy to handle. The editor already knows which reviewers to invite, which issues to look out for, and so on. So, as long as you appear to have made serious efforts to address the prior criticisms, the editor will likely be willing to go along and at least send the paper back out to review.&lt;/p>
&lt;/div>
&lt;div id="how-well-does-this-work" class="section level2">
&lt;h2>How well does this work?&lt;/h2>
&lt;p>What are the chances of success? After having read this post, will you now be able to publish all your work in Science and Nature? No, of course not. Appeals and uninvited resubmissions frequently are unsuccessful. However, they succeed often enough that you should at least consider going this route from time to time. If you never resubmit a rejected article you’re leaving money on the table. You can be certain that any PI who routinely publishes in high-profile journals does a lot of appealing and resubmitting of rejected articles.&lt;/p>
&lt;p>But won’t the editors just get annoyed and put you on their blacklist? I think that’s unlikely, unless you become really obnoxious, e.g. by appealing a failed appeal or by not putting an honest effort into revising your manuscript. Remember that editors fundamentally want to work with you and want to give you a positive decision. They’re not editors because they enjoy handing out rejections all day. They are doing this thankless, poorly remunerated job primarily because they want to advance their field and their community.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> Thus, they’d much rather handle a good paper they can accept than a bad paper they have to reject.&lt;/p>
&lt;p>Finally, you should know that many editors reject papers that they expect to be resubmitted. My own rule is that if the reviewers have pointed out a potential major flaw in the work, one that may require a substantial rethinking of the entire paper, then I’d rather reject than ask for major revisions. I do this because to me calling for major revisions creates the expectation that the paper will be accepted after the revisions have been made. And I don’t want to string authors along, make them revise, and then reject at the very end of this long process.&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>This is called the &lt;a href="https://en.wikipedia.org/wiki/Ben_Franklin_effect">Ben Franklin effect,&lt;/a> after Ben Franklin, who asked a rival legislator to lend him a rare book.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Yes, this statement applies even to paid, professional editors.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>This doesn’t mean I never reject revised manuscripts. It just means I try to make my initial editorial decisions such that rejections after revision are rare.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>What constitutes a citable scientific work?</title><link>https://sevimcengiz.github.io/blog/2015/01/02/what-constitutes-a-citable-scientific-work/</link><pubDate>Fri, 02 Jan 2015 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2015/01/02/what-constitutes-a-citable-scientific-work/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>There was a lively discussion on Twitter the other day regarding what constitutes a citable piece of scientific work. In particular, Matthew Hahn was concerned about where to draw the line, and he felt that unless something is traditionally published there’s &lt;a href="https://twitter.com/3rdreviewer/status/549365313331290112">no need to cite it.&lt;/a> When reading this dicussion, I felt it was muddled by the lack of clear criteria separating citable works from other forms of scientific communication. In my mind, there is a clear distinction between preprints, which I consider to be citable works, and presentation slides or tweets, which are not. To formalize this distinction, I would like to propose four conditions that need to be satisfied for a document to be considered a citable piece of scientific work. The document needs to be: (i) uniquely and unambiguously citable; (ii) available in perpetuity, in unchanged form; (iii) accessible to the public; (iv) self-contained and complete.&lt;/p>
&lt;div id="uniquely-and-unambiguously-citable" class="section level2">
&lt;h2>1. Uniquely and unambiguously citable&lt;/h2>
&lt;p>It must be possible to uniquely and unambiguously refer to the particular work in question. This condition may seem trivial, but that’s not necessarily the case. For example, during the aforementioned Twitter conversation, Matthew Hahn brought up the case where somebody might &lt;a href="https://twitter.com/3rdreviewer/status/549381166722469888">tweet an entire paper or talk.&lt;/a> Such a series of tweets would not be unambigously citable: One can cite an individual tweet but not a collection of tweets. While one could cite the first tweet in a series, assuming subsequent tweets were posted as replies, it would still remain ambiguous which specific tweets should be considered to comprise the entirety of the work. What if other users replied to the first tweet as well? And what if the original author then responded to them? The very nature of Twitter is such that the unique, citable unit is a single tweet, 140 characters or less, and that is not sufficient to convey a self-contained and complete scientific work. (Note that tweets also fail condition 2, since they can be deleted.)&lt;/p>
&lt;/div>
&lt;div id="available-in-perpetuity-in-unchanged-form" class="section level2">
&lt;h2>2. Available in perpetuity, in unchanged form&lt;/h2>
&lt;p>There needs to be some guarantee that the referenced document will not change and will be available in perpetuity. While nothing is truly forever, and works tend to get lost over time, documents hosted according to industry standards by large and established non-profit or for-profit publishing operations are not likely to disappear any time soon. This certainly includes documents posted on the preprint server &lt;a href="https://arxiv.org/">arxiv.org,&lt;/a> and probably also on the &lt;a href="https://biorxiv.org/">biorxiv&lt;/a> server. Moreover, professional publishing operations generally do not allow changes to once-published documents, though they may allow for the publication of updates or revised article versions.&lt;/p>
&lt;p>Importantly, most privately hosted web sites and blogs do not satisfy this requirement.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> If I stop paying my web-hosting bill, this blog will disappear rather quickly. Similarly, any day I could decide that I didn’t like a particular post and rewrite or delete it, or I could delete the entire blog. And the same is true for institutionally hosted pages or lab web pages. Only those publishing platforms that are built with the express purpose of allowing perpetual access provide some amount of certainty that documents won’t just disappear or change.&lt;/p>
&lt;/div>
&lt;div id="accessible-to-the-public" class="section level2">
&lt;h2>3. Accessible to the public&lt;/h2>
&lt;p>The document needs to be accessible to the public. This condition doesn’t necessarily require that access be free (though I personally would prefer it to be this way), since we have traditionally accepted that certain scientific works are only available after payment of a fee. However, anybody willing to pay the fee must be able to access the work, without any other conditions imposed. Also, libraries must be allowed to carry the work, and any library patrons must be able to peruse the work for free.&lt;/p>
&lt;p>The point of this condition is to exclude internal technical documents of companies or other organizations, in particular, documents that might require signing a non-disclosure agreement. Such documents may be useful but they do not belong into the scientific record.&lt;/p>
&lt;/div>
&lt;div id="self-contained-and-complete" class="section level2">
&lt;h2>4. Self-contained and complete&lt;/h2>
&lt;p>The document needs to be self-contained and complete. In other words, whatever the novel contribution is of a given piece of work, that contribution needs to be fully and clearly explained within the document. Many forms of scientific communication violate this condition. Consider for example the slides of a scientific presentation. They are meant merely as support to the oral presentation, and usually they cannot be fully understood without the accompanying talk. Now, if one wanted to, one could certainly write slides that are self-contained and complete. However, those slides would make for a poor talk and also would be nothing more than an awkwardly formatted preprint.&lt;/p>
&lt;p>Even if a recording of the talk is provided alongside the slides, the completeness condition will usually remain violated. For example, methodological details are frequently glossed over in presentations, as are parts of mathematical derivations in theoretical talks. However, this doesn’t mean that only written works can be scientific documents. For example, the &lt;a href="http://www.jove.com/">Journal of Visualized Experiments (JoVE)&lt;/a> publishes self-contained and complete video articles.&lt;/p>
&lt;/div>
&lt;div id="but-if-it-hasnt-been-reviewed" class="section level2">
&lt;h2>But if it hasn’t been reviewed?&lt;/h2>
&lt;p>I am a strong proponent of pre-publication review. &lt;a href="https://sevimcengiz.github.io/blog/2013/12/21/the-value-of-pre-publication-peer-review">I have said so before.&lt;/a> At the same time, I am wary of what I’d like to call “the review fetish,” the attitude that scientific works can’t be trusted until they have been reviewed, at which point they become valid contributions to the scientific literature. Whether something has been reviewed has no bearing on its validity. A work is valid or it is not, period. We all know that flawed works pass peer review and valid works get rejected. In fact, the most influential and highly cited articles often get rejected initially.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> As working scientists, we need to personally judge the validity of each and every article we read, regardless of the article’s origin or review status.&lt;/p>
&lt;p>Also, the only logical reason to require citable works to be peer reviewed would be as a means of quality control, so that bad science doesn’t get cited. However, it then immediately follows that we would have to assess the quality of peer review at each journal. What if some journals carry out sub-standard review and basically print everything? Should they be put on a blacklist of journals we can’t cite? What about contributed papers to PNAS, many of which likely haven’t received the same kind of scrutiny as articles that get edited by independent third parties? What about journals that employ professional editors, who may make decisions that aren’t always entirely driven by scientific considerations? Should we put those journals on the blacklist? In my mind, insisting on peer review for quality control reasons opens a can of worms that simply can’t be dealt with in any reasonable manner.&lt;/p>
&lt;p>Further, while the current scientific culture expects that we submit all our articles to journals for review, I think scientists should be allowed to choose not to be subjected to this process. If some scientists prefer to skip peer review and simply post their work on a preprint server, it should be their prerogative to do so. And we should take their work seriously as long as it is worthwhile and of high quality. Clearly mathematicians do so. Consider the case of &lt;a href="http://en.wikipedia.org/wiki/Grigori_Perelman">Grigori Perelman,&lt;/a> who was awarded a Fields Medal, the highest honor bestowed upon mathematicians, for work he had posted on a preprint server but never formally published.&lt;/p>
&lt;p>Finally, I would like to point out that there are document types that have traditionally been considered part of the scholarly literature, such as monographs or dissertations, that are not necessarily reviewed. &lt;a href="http://haldanessieve.org/2013/08/26/thoughts-on-mbes-preprint-citation-policy/">Journals that forbid citations to preprints&lt;/a> do not usually impose similar restrictions on the citation of books or theses.&lt;/p>
&lt;/div>
&lt;div id="concluding-thoughts" class="section level2">
&lt;h2>Concluding thoughts&lt;/h2>
&lt;p>With the four conditions I have outlined, we can easily test whether specific documents or works should be considered to be citable resources or not. Strings of tweets clearly fail the test, as do slides, recordings of talks, posters, tweets of photos of posters, or blog posts. Documents that pass the test are articles in traditional print journals, articles in most professionally operated online journals, books, book chapters, dissertations, and preprints deposited on professionally operated preprint servers. Interestingly, websites hosting scientific software will usually fail at least conditions 2 and 3, and thus would not be citable by my criteria. In fact, it is my opinion that scientific software should always be accompanied by an article introducing and explaining the software, and what we should cite is the article, not the website where the software is housed.&lt;/p>
&lt;p>Importantly, I can think of no principled test that would cleanly separate preprints from the rest of the scientific literature. The only such test I can think of is “has it been posted on a preprint server,” but it would be difficult to provide a logical reason for why this test should be applied to determine the citability of a document,&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a> other than personal preference. I might just as well not cite articles published in journals that don’t use at least 3 reviewers, or in journals where editorial decisions are made by professional editors and not by working scientists, or in journals that typeset their articles in a sans-serif font.&lt;/p>
&lt;p>&lt;strong>Update 01/02/2015:&lt;/strong> Rafael Najmanovich &lt;a href="https://twitter.com/RNajmanovich/status/551155998350901248">suggested an additional condition:&lt;/a> Attributable authorship. It should be clear who has written a specific document. While I agree with this condition in principle, I’m not sure yet whether I would go so far as arguing that anonymous documents should never be cited. If a document is anonymous but otherwise a valid contribution to science, should we ignore it? Probably not.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>Klein et al. (2014) Scholarly context not found: One in five articles suffers from reference rot. PLoS ONE 9: e115253. &lt;a href="https://doi.org/10.1371/journal.pone.0115253">doi:10.1371/journal.pone.0115253&lt;/a>&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Siler et al. (2014) Measuring the effectiveness of scientific gatekeeping. PNAS, in press. &lt;a href="https://doi.org/10.1073/pnas.1418218112">doi:10.1073/pnas.1418218112&lt;/a>&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>Keep in mind that this entire post is about the conditions that make a document a &lt;em>citable&lt;/em> contribution to the scientific literature. This is different from the question of whether a document is a preprint or a formally published article. The main services that journals provide are (i) quality control, in the form of editorial and peer review, (ii) prestige, in proportion to how selective they are, and (iii) professional typesetting, though the quality of this service has declined in recent years. In return, journals demand exclusivity. Thus, it is natural for a journal to determine whether a document has been previously published by asking whether the document has previously undergone editorial and peer review and has been professionally typeset. Importantly, when journals make this assessment, they are not concerned with the quality of peer review. Any document that has been reviewed and accepted for publication elsewhere, no matter how low the standards, would violate the exclusivity clause and hence is going to be considered published.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How to prepare an article for resubmission, Part II</title><link>https://sevimcengiz.github.io/blog/2014/12/18/how-to-prepare-an-article-for-resubmission-part-ii/</link><pubDate>Thu, 18 Dec 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/12/18/how-to-prepare-an-article-for-resubmission-part-ii/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>In my previous post on &lt;a href="https://sevimcengiz.github.io/blog/2014/11/16/how-to-prepare-an-article-for-resubmission">how to prepare an article for resubmission,&lt;/a> I failed to mention one important point: In your response to the reviewers, quote the &lt;em>entire&lt;/em> referee report, even the introductory sentences. Don’t just quote the specific comments to which you are replying. This may seem unnecessary but it is in fact crucial, in particular if the introductory sentences were largely positive. (If they were highly critical, you may want to omit them, even though in this case you probably should provide a response.)&lt;/p>
&lt;p>Keep in mind that when the revised manuscript goes back to the editor and the previous reviewers, neither will remember the exact thoughts they had when they previously looked at your manuscript. In addition, the reviewers may never actually have seen the comments of the other reviewers. And finally, most editors and reviewers will look at your response to the reviewer comments before they look at anything else related to your manuscript. Thus, this is your opportunity to remind the editor and the reviewers that your manuscript overall was judged to be interesting and valuable, even if there were some issues to be addressed. By not quoting these comments, you only highlight the critical aspects of the previous reviews. For the same reasons, it is often a good idea to start the response with a brief summary of the overall reviewer sentiments, such as: “Reviewers 1 and 2 thought the manuscript addressed an important topic and had only minor comments. Reviewer 3 was more critical but also acknowledged the timeliness of our work.”&lt;/p></description></item><item><title>How to prepare an article for resubmission</title><link>https://sevimcengiz.github.io/blog/2014/11/16/how-to-prepare-an-article-for-resubmission/</link><pubDate>Sun, 16 Nov 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/11/16/how-to-prepare-an-article-for-resubmission/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>So your latest scientific masterpiece has come back from review with the most likely outcome other than rejection: major revision. The reviewers and the editor think that your work has merit, but they also have a long list of comments and criticism that they expect you to address before the article is acceptable for publication. You read the reviews and you feel like they lay out two years worth of work. How do you best deal with this situation?&lt;/p>
&lt;div id="your-life-will-be-easier-if-you-understand-everybodys-objectives" class="section level2">
&lt;h2>Your life will be easier if you understand everybody’s objectives&lt;/h2>
&lt;p>Let’s first consider the perspective of the three groups of people involved: the editor, the reviewers, and the authors (i.e., you). The editor wants to make sure there are no major problems with your paper, in particular problems that would potentially embarrass her&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> down the line. So the editor will pay close attention to any points the reviewers raise that look like your work might be flawed. She will generally be less worried about whether you actually do every additional analysis the reviewers suggest. A good editor knows that most reviewers will suggest more changes than are strictly necessary to get the paper publication ready.&lt;/p>
&lt;p>The reviewers, primarily, will want to be recognized for their knowledge of the field. They want you to acknowledge that they noticed or knew something you didn’t. Even if it may not seem that way, most reviewer comments are written as constructive criticism, suggestions from the reviewers to you on how you could improve your work. As long as your revisions acknowledge the reviewers’ views, you should be fine. However, on occasion, a reviewer thinks that something you’re doing is fundamentally flawed. In those cases, you may have to put in some extra effort to appease the reviewer.&lt;/p>
&lt;p>I assume you know what your objective is in this interaction, but in case you had doubts I’ll tell you: You want to get the paper published with as little extra work as possible. You thought your paper was done when you first submitted, so any additional work you’re asked to do amounts to pointless busywork from your perspective.&lt;/p>
&lt;p>Now that we know what everybody’s objectives are in this game, let’s discuss some strategies for successful resubmission.&lt;/p>
&lt;/div>
&lt;div id="start-by-drafting-a-response-to-the-reviewers" class="section level2">
&lt;h2>1. Start by drafting a response to the reviewers&lt;/h2>
&lt;p>The absolute worst thing you can do after having received reviewer comments is to run back into the lab and start all the additional experiments the reviewers want you to do. This will drag you down a rabbit hole that you will find difficult to come out of, and you will waste a lot of time doing unnecessary work. You need a clear plan of what to do. The best way to develop that plan is to start drafting a response to the reviewers. Copy all the reviewer comments into a file, mark them in some color other than black (I like blue), and then start adding your responses in black.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> See how many reviewer points you can dispense with by writing a response that requires only very minor edits to your manuscript.&lt;/p>
&lt;p>For the reviewer points that require more extensive rewriting or additional experiments, write out a plan of what you will do to address these points. I like to highlight the parts in the response that I still have to address in the manuscript, and I remove the highlights once I have done so. In this way, I don’t lose track of which edits I have or haven’t done.&lt;/p>
&lt;p>From what I have seen, the winning strategy employed by some of the most experienced and successful scientists is to write a very long, detailed response and keep the actual manuscript edits to a minimum. It’s not unusual to see a 5 page response to the reviewers accompanying very minor revisions in the actual paper, a few sentences added here and there, and a few additional references thrown in for good measure. These scientists have, over the years, developed a good sense of the minimum amount of work they can get away with and still have their revisions accepted.&lt;/p>
&lt;/div>
&lt;div id="realize-that-the-reviewer-is-always-right" class="section level2">
&lt;h2>2. Realize that the reviewer is always right&lt;/h2>
&lt;p>Regardless of how inane a reviewer’s comments may seem, the reviewer is always right. You don’t gain anything from being upset about the reviewer’s incompetence or lack of knowledge in your area. Instead, think why the reviewer may have reacted the way he did. Maybe you didn’t explain something carefully enough, or you assumed something was widely known that actually isn’t. In your response to the reviewers, always acknowledge the validity of the reviewers’ comments, and then either try to explain the issue in the response or modify the manuscript appropriately.&lt;/p>
&lt;/div>
&lt;div id="take-the-reviewer-comments-seriously" class="section level2">
&lt;h2>3. Take the reviewer comments seriously&lt;/h2>
&lt;p>It’s very easy to discount reviewer comments and say “the reviewer knows nothing about this topic.” Often the reviewer knows more than you may think, and you may simply not be understanding the reviewer’s point of view. (I’ve certainly reviewed more than one paper where I felt the authors were simply not getting what I was trying to tell them.) So make a serious effort and try to figure out what exactly it is the reviewer wants and how you can make it happen.&lt;/p>
&lt;/div>
&lt;div id="cite-every-reference-the-reviewers-mention" class="section level2">
&lt;h2>4. Cite every reference the reviewers mention&lt;/h2>
&lt;p>Sometimes it’s very clear that a reviewer wants you to cite a given paper while at other times it may seem like citing certain papers is optional. (Example: “In this context, the authors could consider citing Jones et al. 1975.”) Either way, cite all mentioned papers unless they are totally inappropriate. Regardless of whether the reviewer actually is Jones himself, or only is good friends with Jones, or simply thinks that the Jones et al. paper was a breakthrough for the field, the reviewer clearly cares for Jones et al. 1975. Therefore, he will have a little more respect for you if you demonstrate that you care for Jones et al. 1975 as well.&lt;/p>
&lt;/div>
&lt;div id="openly-admit-to-your-works-limitations-and-shortcomings" class="section level2">
&lt;h2>5. Openly admit to your work’s limitations and shortcomings&lt;/h2>
&lt;p>When reviewers point out that the research performed has certain shortcomings and limitations, junior scientists will often think they have to overcome these limitations before the work can be published. However, more often than not, all that is needed is a clear statement that these limitations exist and should be addressed in future work. Between this strategy and #4 (cite additional papers), you can probably handle at least 60-70% of all reviewer comments without doing any additional experiments or analysis.&lt;/p>
&lt;/div>
&lt;div id="understand-that-reviewer-comments-are-written-as-much-for-the-editor-as-they-are-for-you" class="section level2">
&lt;h2>6. Understand that reviewer comments are written as much for the editor as they are for you&lt;/h2>
&lt;p>The reviewer doesn’t just want to criticize your work, he also wants to make a good impression in front of the editor, who may be a close colleague, former advisor, or general heavyweight in the reviewer’s field of research. For this reason, the reviewer will always come up with at least a handful of points to criticize, just so he doesn’t appear lazy or incompetent. You will have to figure out which of the comments actually address crucial limitations of your paper and which were written just to impress the editor. The latter ones can always be dispatched with a combination of strategies #4 and #5.&lt;/p>
&lt;/div>
&lt;div id="say-no-to-excessive-requests" class="section level2">
&lt;h2>7. Say “No” to excessive requests&lt;/h2>
&lt;p>Finally, be aware that it is perfectly acceptable to not do certain things the reviewers ask for. Unless the validity of your core findings is at doubt, you always have the option of saying something like “these additional analyses are beyond the scope of the current work: or”we agree that the reviewer’s suggestion should be pursued in future work, and we now say so in the Discussion." In case of doubt, don’t do the extra work, just say “No”.&lt;/p>
&lt;p>&lt;strong>Update 12/18/2014:&lt;/strong> Also read &lt;a href="https://sevimcengiz.github.io/blog/2014/12/18/how-to-prepare-an-article-for-resubmission-part-ii">my follow-up post&lt;/a> on this topic.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>In this story, the editor is female and the reviewers are male.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>There’s a reason for the specific color choices I suggest. Your goal is to visually separate the reviewer comments from the responses. You could do this by making either the reviewer comments or the responses bold or italics. However, extended sections in italics tend to be hard to read, and extended sections in bold tend to be jarring. So colors are the best option. In your color choice, keep in mind that your responses need to be more visually present than the reviewer comments, because you want the reviewers and the editor to focus on your responses, not the reviewer comments, when they evaluate your revision. So your responses need to be in black, and the reviewer comments need to be in a color that doesn’t stand out relative to black. Blue is a good choice. Maybe green or gray would also work. Red or yellow would probably be bad choices.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Should peer-review be double-blind?</title><link>https://sevimcengiz.github.io/blog/2014/10/18/should-peer-review-be-double-blind/</link><pubDate>Sat, 18 Oct 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/10/18/should-peer-review-be-double-blind/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>As part of the &lt;a href="https://sevimcengiz.github.io/blog/2014/10/12/in-defense-of-anonymous-peer-review">recent discussion on anonymous peer review,&lt;/a> several people spoke out in favor of double-blind peer review, where neither the authors nor the reviewers know who the others are. I have thought a lot about double-blind peer review, and I’m not entirely convinced, in particular when it comes to grant applications. While double-blind review might solve certain problems and remove certain biases, it would almost certainly amplify other issues, and whether the net effect would be good or bad is unclear. It would also give more power to people such as editors and program managers who operate outside the blinded process.&lt;/p>
&lt;p>So let’s discuss. I’ll first cover journal articles and then grant proposals. The two are very different, and what applies to one does not necessarily apply to the other.&lt;/p>
&lt;div id="journal-articles" class="section level2">
&lt;h2>Journal articles&lt;/h2>
&lt;p>On the face of it, double-blind peer review for journal articles seems like a no-brainer. It allows junior scientists to be judged on the merit of their work alone, and it prevents senior scientists from coasting through peer review on the basis of their good name. However, as always, the devil is in the detail. There are at least three reasons I can think of why double-blind review may not be such a great idea after all.&lt;/p>
&lt;p>First, the real power is with the editors, not the reviewers. It’s the editors who make the final decisions. And this power tends to increase with the perceived rank of the journal; editors of more prestigious journals are more likely to reject papers without review or because of a perceived lack of interest. We all know that you won’t publish in &lt;em>Nature&lt;/em> if the &lt;em>Nature&lt;/em> editors don’t like your work. And we also know that you can survive quite harsh reviewer criticisms if the &lt;em>Nature&lt;/em> editors really want to publish your work.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> Thus, one could make an argument that author names should be blinded to both reviewers and editors. But how would the editors invite unbiased reviewers if they don’t know who has a potential conflict of interest? The only practical way would be to have one editor invite the reviewers and another make the decision. This would be an interesting experiment, but I doubt any journal will go for it any time soon. Thus, as long as author names remain known to the handling editor, I doubt that double-blind review will make much of a difference.&lt;/p>
&lt;p>Second, blinding the authors of an article is incredibly hard. Just removing the author names from the first page of a paper will frequently not do. If the authors work on a unique study system, or make extensive use of their prior work, or have developed a specific software package, or have posted their data on a public repository, reviewers will likely be able to find out who they are. I once reviewed a paper where the authors had not only removed their names from the title page but also from citations to their own work in the reference list. I’m still amazed that somebody would (i) go to these lengths to conceal their identity and (ii) not realize that this action completely obliterated any anonymity they might otherwise have had. Double-blinding may create an illusion of anonymity where none actually exists.&lt;/p>
&lt;p>Third, double-blind review gives more power to scientists who are intent on submitting fraudulent work, because it is going to be harder for reviewers to identify patterns in the perpetrators’ activities. For example, I have my list of crooks with a solid portfolio on &lt;a href="http://retractionwatch.com/">Retraction Watch,&lt;/a> and when I get to review one of their papers I’ll be extra careful. These are usually scientists whose activities slipped by me the first time I reviewed one of their papers, because everything looked fine on the surface. If I regularly had to review their papers in a double-blind fashion, they would probably manage to slip by me more frequently.&lt;/p>
&lt;p>One benefit of double-blind peer review, however, could be that even if the reviewers have a sense of which lab(s) may have been involved in a given study, they still won’t be able to guess the exact author list. For example, I don’t know to what extent reviewers are biased by the gender of the first author if a paper comes from an established lab, but if they are, that bias would likely disappear in double-blind review. Reviewers may guess correctly that the paper comes out of my lab, but they won’t know which of my students wrote it. Evidence in favor of this notion comes from one experiment in which double-blind peer review &lt;a href="http://blogs.nature.com/peer-to-peer/2008/01/doubleblind_peer_review_reveal.html">increased the number of female first authors.&lt;/a> Similarly, when junior PIs continue working on research they begun in an established person’s lab, reviewers won’t be able to tell whether the paper comes from the established lab or the junior PI. This could work to the advantage of junior PIs.&lt;/p>
&lt;p>In summary, the positive and the negative aspects of double-blind review are about even, in my opinion. I have no major concerns about double-blind review, as long as I as an author am not expected to do anything more than remove my name from the author list. I’m not interested in going through my entire paper and making sure not a single sentence (e.g. “We have previously investigated…”) could give a hint at who I am. Also, we now frequently make all our data and code available in a github repository, and I’m not going to go through extra effort to conceal who I am there. Other than that, I’d be happy to support more experiments in double-blind peer review, and I’ll also be happy to support double-blind peer review more strongly if evidence in its favor continues to accumulate.&lt;/p>
&lt;/div>
&lt;div id="grant-proposals" class="section level2">
&lt;h2>Grant proposals&lt;/h2>
&lt;p>Grant proposals are an entirely different beast than journal articles, and I do not think that double-blind proposal review is a good idea. There is a fundamental difference between a journal article and a proposal. An article is the finished product. A grant proposal, by contrast, is only the promise of a future product. If one scientist writes ten times more high-profile papers than another, then she should publish ten times more frequently in high-profile journals, without question. However, just because one scientist is ten times better at writing grant proposals than another doesn’t mean he deserves ten times the funds. In fact, only if that scientist can write ten times as many papers or write papers that are ten times as important (however measured) would he deserve ten times the grant funding.&lt;/p>
&lt;p>I strongly believe that the track record of past performance needs to be considered in proposal review. If you had to hand one person a check over a million dollars, would you rather give the money to somebody who consistently delivers interesting results, even if that person’s grant application doesn’t sound overly exciting, or would you prefer to give the money to somebody who can tell a great story but about whom you know nothing beyond that story. This thought is related to the idea (&lt;a href="http://loop.nigms.nih.gov/2014/07/comment-on-proposed-pilot-to-support-nigms-investigators-overall-research-programs/">which is slowly sinking in with the NIH as well&lt;/a>) that it is generally better to fund people than projects, or at least to have a healthy mix of people-based and project-based funding. By definition, if you’re evaluating people, you cannot blind the evaluators to their identity.&lt;/p>
&lt;p>Now you could argue that the scientific review should be done blinded and the final funding decision be made by the program officer, who can take into account all the other relevant factors, such as track record, current funding of the applicant, etc. However, this would simply put more power into the hands of program officers, who might or might not use that power wisely. It’s certainly not unheard of for program officers in some agencies to preferentially fund their good buddies. The more power a program officer has to override a panel decision the more likely those situations are going to arise.&lt;/p>
&lt;p>Double-blind grant review is also open to several sorts of manipulation by applicants. First, it would be easier than it already is to base an application on dubious, sketchy, or even entirely made-up data, because nobody would ever know.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> Second, applicants could pack their proposals with prior results obtained by the biggest shot in the field, causing the reviewers to think they’re reviewing an application by that lab and rank it higher because of that. You might say that that’s exactly the point, only ideas matter, but I’ve seen too many scientists with great ideas and poor execution to feel comfortable with funding decisions based exclusively on ideas.&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>&lt;/p>
&lt;p>In conclusion, I don’t think that double-blind grant applications are the way to go. There are other ways to minimize biases in the review process. For example, panels could be given statistics on how many women and junior scientists submitted applications to a given competition, and if the composition of the top-ranked applicants deviates substantially from the overall composition of applicants then the panel could be asked to reconsider their rankings. In general, just paying attention to these kinds of biases and monitoring whether certain groups of applicants are disproportionally affected by either positive or negative decisions should prevent the most egregious biases.&lt;/p>
&lt;p>&lt;strong>Update (10/19/2014):&lt;/strong> The article I quoted &lt;a href="http://blogs.nature.com/peer-to-peer/2008/01/doubleblind_peer_review_reveal.html">claiming an increased number of female first authors&lt;/a> under double-blind review has later &lt;a href="http://www.nature.com/news/2008/080604/full/453711c.html">been called into question,&lt;/a> as comparable journals have similarly seen an increase in the number of female first authors during the same time period, without instituting double-blind review. Thanks to &lt;a href="https://twitter.com/mattjhodgkinson">Matt Hodgkinson&lt;/a> for pointing this out.&lt;/p>
&lt;p>&lt;strong>Update #2 (10/19/2014):&lt;/strong> &lt;a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.22784/full">This paper,&lt;/a> pointed out to me by Matt Hodgkinson, provides a thorough review of what is currently known about biases in peer review. It shows mixed evidence on gender bias. In particular with respect to journal articles, current evidence suggests bias isn’t that pronounced (female and male authors have comparable acceptance rates).&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>I certainly have reviewed papers for &lt;em>Nature&lt;/em> that I thought should not be published there and the editors overruled me. And this is fine; editors should have the ultimate decision power. I’m an editor myself, and on occasion I accept papers that reviewers say should be rejected. The point remains, though, that an editor who really wants to publish a paper will rarely be deterred by negative reviews, in particular if the reviews don’t call out egregious errors in the work.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Even under the current system of non-blinded review, grant applicants can include sketchy or made-up data with little risk to their career or reputation. While such activity is obviously fraudulent and will have severe consequences if discovered, the likelihood of discovery is low. First, only three to five other scientists ever see the application, and only for a short period of time. So if an applicant, for example, reuses the same data set in subsequent applications but labels the resulting figure differently, it’s very unlikely anybody would notice. Similarly, if an applicant claims preliminary data support one hypothesis and later publishes a paper supporting a different hypothesis, he could always argue that that is indeed how discovery went: first things looked one way but after more careful study it became clear the other way was right. Only the most blatantly obvious fraud, such as publishing fraudulent data in a paper and then using that paper as preliminary results in an application, has any likelihood of being discovered. For these reasons, a colleague of mine here at UT thinks that results that aren’t published or maybe at least deposited on a public archive should not be allowed in grant applications at all.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>As with everything in life, I think some balance is required here. Grant applicants should have to demonstrate some amount of prior expertise in the work they propose, but they should also be given the benefit of the doubt that some things can be worked out as the research is done.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>In defense of anonymous peer review</title><link>https://sevimcengiz.github.io/blog/2014/10/12/in-defense-of-anonymous-peer-review/</link><pubDate>Sun, 12 Oct 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/10/12/in-defense-of-anonymous-peer-review/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>In a recent blog post, Mick Watson argued that &lt;a href="http://biomickwatson.wordpress.com/2014/10/08/why-anonymous-peer-review-is-bad-for-science/">anonymous peer review is bad for science.&lt;/a> The post makes a number of insightful and valid points. However, the one point I cannot agree with is that junior scientists don’t need anonymity so they can freely speak their mind without fear of retaliation. Mick Watson argues that retaliation should be a non-issue, and that in the cases where it is not we just have to make it so. Frankly, I think this is simplistic black-and-white thinking. There are so many ways in which a senior person can make a junior person’s life more difficult; I would always recommend my graduate students and postdocs that they review anonymously unless they can write a very positive review.&lt;/p>
&lt;p>A senior person can retaliate against a junior person in a million subtle ways, and all of them are entirely ethical &lt;em>unless they are done in bad faith.&lt;/em> In fact, they are the exact same behaviors we engage in all the time to separate stronger science/scientists from weaker science/scientists. Let me just list a few examples:&lt;/p>
&lt;ul>
&lt;li>I don’t have to prominently cite Junior Person’s work in all of my papers if my papers don’t build directly on top of Junior Person’s work.&lt;/li>
&lt;li>When I give invited lectures, I don’t have to mention Junior Person’s work.&lt;/li>
&lt;li>I don’t have to invite Junior Person to give a Departmental Seminar at my institution.&lt;/li>
&lt;li>I am free to rank Junior Person’s grant as “very good” rather than “excellent.”&lt;/li>
&lt;li>If I’m on a grant panel and my colleagues are tearing apart Junior Person’s grant, I don’t have to speak up. I’m allowed to have no strong opinion.&lt;/li>
&lt;li>I don’t have to invite Junior Person to give a keynote lecture at the conference I’m organizing. In particular, if Junior Person is a man, I can always say “we needed more women speakers.”&lt;/li>
&lt;li>If Junior Person comes up for tenure, I don’t have to write a letter. I can claim I’m busy. Or, if I accept the assignment, I can weaken my statements of support, e.g., by saying “I think Junior Person would get tenure at my institution” instead of saying “Without doubt Junior Person would get tenure at my institution.”&lt;/li>
&lt;li>If I’m handling one of Junior Person’s papers as Associate Editor, I can invite reviewers who I suspect are going to be critical of Junior Person’s work.&lt;/li>
&lt;li>If I’m handling one of Junior Person’s papers as Section Editor, I can place little seeds of doubt in the mind of my Associate Editor, e.g. by forwarding the paper with a note that says “I’m not entirely sure this fits into the scope of &lt;em>Journal of Amazing Results.&lt;/em> Feel free to reject without review if you have doubts yourself.”&lt;/li>
&lt;li>When I’m reviewing a paper by somebody else, I don’t have to tell the authors that they should cite Junior Person’s work.&lt;/li>
&lt;li>When I’m reviewing Junior Person’s paper, I can place little seeds of doubt in the handling editor’s mind, e.g. by placing the following in the confidential comments to the editor: “There’s nothing technically wrong with this work, but I don’t quite see it meeting the standards of &lt;em>Journal of Amazing Results.&lt;/em>”&lt;/li>
&lt;li>When I speak informally with colleagues, I don’t have to mention how amazing Junior Person’s work is. In fact, I don’t have to mention Junior Person at all.&lt;/li>
&lt;/ul>
&lt;p>And of course, I can do the opposite of all these behaviors if I really want to promote a junior scientist. In fact, I strongly suspect that these kinds of behaviors (subtle promotion or demotion of individuals) are at the heart of observed differences in recognition of male vs. female scientists, but that’s a topic for another day. For now, it suffices to emphasize that all of these behaviors are entirely reasonable, ethical, and even desired &lt;em>if&lt;/em> they are driven by an objective assessment of the quality of one person’s science over another’s, and not by personal biases, preferences, or desire for revenge.&lt;/p>
&lt;p>Mick Watson argues that the scientific community should expel retaliating scientists. This suggestion sounds good in theory, but in practice it won’t work for any but the most egregious cases. And importantly, expelling retaliating scientists must not turn into a witch hunt. If I have to be concerned that any time I rank a grant proposal as “very good” or even just “good” somebody is going to accuse me of retaliation then I might as well stop reviewing grants or papers altogether.&lt;/p>
&lt;p>Mind you, I’m strongly in favor of &lt;em>open&lt;/em> peer review, where the entire review history is routinely published alongside the paper. It adds a lot of transparency to the process. And I think (though I have no data) that people will generally write more polite and factual reviews if they know that their reviews will become public eventually. In the end, though, science—like anything else in life—is always going to be somewhat unfair, open to manipulation, and subject to personal biases and opinion. As a minor protection against these mechanisms, in particular for junior people and women, scientists should be allowed to give anonymous feedback if they so choose.&lt;/p></description></item><item><title>Double Jeopardy</title><link>https://sevimcengiz.github.io/blog/2014/09/13/double-jeopardy/</link><pubDate>Sat, 13 Sep 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/09/13/double-jeopardy/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Lately it keeps happening to me that I try to invite somebody to review a paper and they decline, giving as reason that they have reviewed the paper already for a different journal&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> and reviewing the paper again would put the authors into a situation of double jeopardy. This got me thinking. Should reviewers really decline for that reason? As reviewer, I’ve always thought the opposite. For a paper I have reviewed already, if the authors have made a reasonable effort to address my comments and have now chosen a more adequate journal, I can keep my review short and recommend acceptance. Thus, I’m actually preventing a situation of double jeopardy. I keep the authors from facing yet another reviewer with new opinions and requests. So, which is right? Should reviewers recuse themselves if they are asked to review again for a different journal, or should they instead leap at the opportunity and give the authors a break? I’d be interested in your thoughts.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>Where the paper was rejected, presumably.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Share your preliminary work with other people, even if you think it’s crap</title><link>https://sevimcengiz.github.io/blog/2014/07/08/share-your-preliminary-work-with-other-people-even-if-you-think-its-crap/</link><pubDate>Tue, 08 Jul 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/07/08/share-your-preliminary-work-with-other-people-even-if-you-think-its-crap/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>It’s quite common for me to have students tell me “the analysis didn’t work out” or “the figure looks bad” or “I don’t have any useful results.” And it’s also quite common for the students to be wrong. Sometimes, students have amazing results but are all disappointed because the results aren’t what they had expected. These students fail to see the data for what they are. More commonly, the students may be right in that the data aren’t that great, but usually I can see something in the data that the student didn’t. In either case, it is important that we look at the data together, because jointly we will see more than either of us individually would have seen.&lt;/p>
&lt;p>However, while some students are happy to show me their “failed” analyses and complain about how nothing works, others are more reserved, sometimes to the point of reluctance. The latter students don’t feel comfortable showing me their preliminary results, or sometimes any results at all, unless they think the work is completed.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> These students are probably under the mistaken impression that I will judge them for “failed” analyses, or that I expect a complete analysis, with proper interpretation of all data points, at all times, or that having remaining open questions means I’ll think the student did a poor job. On the contrary, I want to see all these preliminary, incomplete, and confusing results. The data are the data, and I want to be able to draw my own conclusions about them.&lt;/p>
&lt;p>Furthermore, I submit that unless you know for sure you made a mistake (say, your code has a bug and you know it), you will always be better off discussing your work with other people than silently deciding it’s not good enough. And if your results seem too trivial or wrong to talk about them with your advisor, then at least talk them through with a fellow student or postdoc.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> Think about it this way: every time you generate a result or make a figure, and then you delete it before you show it to somebody else, you’re preempting the possibility that somebody else might see something useful in your work. And while a lot of what you’re doing may indeed be worthless, I’d argue that unless you’re a complete disaster, you’ll probably do at least one thing every day that is actually worthwhile. So every day, you should produce some sort of result that you then discuss with somebody else.&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>&lt;/p>
&lt;p>Importantly, the necessity of sharing preliminary work with other people doesn’t stop once you graduate. Just because you have a PhD doesn’t mean that you’re suddenly able to always see the data exactly for what they are. Sometimes even the most experienced scientists are overly attached to a particular hypothesis or miss some critical detail in their data. That’s why many experienced scientists routinely talk to their colleagues about work in progress, why they present preliminary results at conferences or seminar talks, and why they request comments on manuscript drafts and preprints. In my mind, even traditional peer review serves primarily the purpose of having another set of eyes look over the data and check whether the authors are over- or underhyping their results.&lt;a href="#fn4" class="footnote-ref" id="fnref4">&lt;sup>4&lt;/sup>&lt;/a>&lt;/p>
&lt;p>The only thing that really changes once you graduate and/or complete your postdoc is that suddenly there is no adviser anymore who makes you show him/her your latest work. Now it’s entirely on you to seek out the advice you need and to receive the feedback that will make your work better. And, if you are advising students yourself, it’s now your job to make them talk to you and show you what they’re doing, warts and all.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>If you’re my student and you think this blog post is about you, let me tell you: you’re not the only one.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>But realize that sometimes students talk each other down. Just because your fellow grad students say what you did was stupid doesn’t mean it actually was; they may just lack perspective.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>You might argue that you’re working in a field where results accumulate slowly, hence you can’t possibly discuss results from individual days. E.g., you’re collecting beetles in the field, and you need three field seasons before you have enough data to test your hypothesis. My response is that nevertheless there are tons of preliminary data that you should discuss with somebody. E.g., if you’re collecting beetles every day, you know how many you found each day, where you found them, what physical characteristics they had, etc., and how these quantities change over time. All of these are worthwhile preliminary results that you should look at, graph, and discuss with a third party.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn4">&lt;p>The second case, where reviewers see more in the data than the authors, is more common than you may think. Many papers get really good only after a solid round of peer review, and frequently the reviews in those cases are not “X is wrong” but rather “You have done X but you should also do Y, and in combination you could conclude Z which would be really cool.”&lt;a href="#fnref4" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How glamour journals rose to prominence, and why they may not be needed anymore</title><link>https://sevimcengiz.github.io/blog/2014/01/04/how-glamour-journals-rose-to-prominence-and-why-they-may-not-be-needed-anymore/</link><pubDate>Sat, 04 Jan 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/01/04/how-glamour-journals-rose-to-prominence-and-why-they-may-not-be-needed-anymore/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>In the ongoing discussion about the value of glamour journals such as Nature, Science, and Cell, I think it’s worth looking back and asking: “How did they rise to prominence?” and “Are they still serving the same role they did when they arose?” So let’s take a quick trip into the history of science communication, before the internet. Then we can ask what the internet has changed, and how we could make the best of current technology.&lt;/p>
&lt;p>I belong to the last generation of scientists that experienced science before the internet. I started doing research as an undergrad in 1995. At that time, I saw a web browser for the first time, and I sent my first email. While the internet had been around for a while by 1995,&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> its use was still very limited, and barely anybody outside academia had ever even heard of it. All this would change over the next 4-5 years, and by 2000 the internet started to become ubiquitous.&lt;/p>
&lt;p>I don’t think anybody who got into science after the year 2000 can imagine what keeping up with the literature was like before the internet. I started my postdoc in 2000. During my entire postdoc time (or since), I rarely ever went into a library. By that time, most journals had a solid online presence, including back issues. Articles that weren’t available online could be requested via inter-library loan, and they would arrive electronically. By contrast, during my PhD, I spent a lot of time at the library. I would make weekly trips to browse the latest issues of the scientific journals I was interested in. Because I was working at the interface between physics and biology, I had to visit the physics library and the biology library. For certain articles I also had to go to the chemistry library. I knew exactly which library had which journals, and which journals were available in multiple libraries. (Almost all libraries had Science, for example.) To figure out whether anybody was citing a particular paper, I had to confer with the &lt;a href="http://en.wikipedia.org/wiki/Science_Citation_Index">Science Citation Index,&lt;/a> which was a big book available at some libraries. For any article of interest, it would list by which other articles it had recently been cited. Invariably, the list of citing articles would contain articles in journals that were only available in a different library on campus, or not in any library at my university. So, after reading the Science Citation Index, I’d make the trip to a different library, or submit an inter-library loan request, or make a note for my next scheduled trip to a different library to look up a particular article. In the worst-case scenario, it could take weeks until I saw a particular article, and then I’d often find out the article wasn’t really relevant to what I was doing.&lt;/p>
&lt;p>Compare this to how literature search works today. I look up an article on Google Scholar, click on “Cited by” or “Related articles,” and find relevant related articles in seconds. For every article listed, I can get at least the title and the abstract, and for the vast majority of articles I can get the full text, all within a few seconds and while sitting at home on my couch. I can similarly browse any open-access journal, everything on Pubmed Central, and any paywalled journal my university has access to, from everywhere in the world. For all intents and purposes, the second I know a particular article exists, I can look it up and read it.&lt;/p>
&lt;p>What has any of this to do with glamour journals? I’m going to argue that in the time before the internet, glamour journals and other highly selective journals served a crucial role. In a world where looking up a reference can take between days and weeks, finding potentially interesting &lt;em>references&lt;/em> is much less valuable than finding potentially interesting &lt;em>articles.&lt;/em> Yes, you would go to the trouble of hunting down a particular article if it seemed directly relevant for your own work, but you certainly wouldn’t do so just to generally keep up with a broader field, much less all of science. Therefore, reading a journal such as Science or Nature, or even a more specialized but still fairly selective journal such as Genetics, was the only way to keep up with scientific progress. You went to the library, you took the physical copy of the journal, you browsed through it, you read the interesting articles, you learned something useful, you went home/back to your office.&lt;/p>
&lt;p>By contrast, these days, with nearly any article right at our fingertips, the process of publishing articles and of selecting articles can be decoupled. For example, I don’t consistently browse through the tables of contents of Nature or Science anymore, because I now have other means of discovering interesting articles. Any interesting article in my field I’ll come across sooner or later because Google Scholar recommends it to me, or somebody tells me about it in person, or it gets cited in a paper I read or review. For generally interesting articles, say the latest findings about global warming, I’ll likely see them mentioned on Twitter or Reddit. The advantage of all these methods of finding articles over browsing through tables of contents is that I’m not tied to the venue in which the article was published. I’m just as likely to come across an interesting article published in Nature as one in PLOS ONE. And the moment I learn about an article, I can read it. But imagine a print version of Twitter, in the time before the internet. If I had received per mail, once a week, a list of potentially interesting things published in the most random venues, I would never have followed up with reading any of them. The barrier to doing so would simply have been too high.&lt;/p>
&lt;p>Some people take this reasoning to the extreme and argue that since now everything is easily available online and search engines are powerful, we don’t need selective journals anymore at all. The best science will rise to the top, it will be cited, tweeted, mentioned on reddit, bloggers will write posts about it, and thus we might as well publish everything in PLOS ONE. I am not entirely convinced by this argument, for the following reason: It’s all well and good if other people cite and tweet your work, but what if they don’t? If you think you have done some really outstanding work, work that deserves more attention than your regular bread-and-butter efforts do, in a world where all science is published in PLOS ONE, what options to you have? In a world that has glamour journals, it’s of course obvious what you can do: You write a nice 3-6 page summary of your work, highlighting the most important findings and the broad relevance, you send it to one or more glamour journals, and you hope for the best. If you get through, you’ll have a much higher chance of getting your article cited, tweeted, etc., because people pay attention to the glamour journals, and they like to read short, clearly written articles that highlight key findings and broader relevance. But if there are no glamour journals, then you have no good option of indicating that in your own opionion, this article is more valuable than that article.&lt;/p>
&lt;p>So let me summarize the facts: (i) In the world of the internet, it doesn’t matter where something is physically published, as long as it is easily accessible through a URL. (ii) Glamour journals have lost the original purpose of making important science easily and broadly accessible. (iii) Publication venues that highlight interesting work by commenting and/or linking to it (such as Twitter, Reddit, Nature News and Views, Google Scholar, etc.) are highly valuable. (iv) Short, clearly written articles highlighting key insights and broader relevance are appreciated and highly valuable. (v) Authors have an interest in pointing out what they think are their most important works.&lt;/p>
&lt;p>These facts lead me to the following proposal: Let’s take all original science out of the glamour journals. Instead, allow authors to submit short summaries (maybe 2-3 pages) of work they have already published elsewhere, e.g. in PLOS ONE. These summaries would be reviewed editorially, and also by one or two expert scientists who’d be asked to judge whether the original article appears to be scientifically sound and noteworthy. Editors might reject a summary because it isn’t deemed sufficiently interesting or novel, and there would still be fighting and politicing about getting summaries into these glamour journals, but the system would relieve authors of several pressures: (i) Authors wouldn’t have to rewrite an article multiple times just to hope to get it published eventually in one of the selective journals. (ii) Authors could get their results out and cite them properly while still trying for that glamour slot. (iii) Since the original article of record would be in PLOS ONE or PeerJ or similar, it would become generally accepted to have even the most important work published in these journals. Nobody could look at a publication list and say “Oh, it’s just a bunch of PLOS ONE papers.” (iv) Hiring committees and granting agencies would still have the option to evaluate candidates by the number of summaries they have published in glamour journals, though I would hope they would do so to a lesser degree and pay more attention to the original articles.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>The first web browser, which started the development of the modern internet, &lt;a href="http://en.wikipedia.org/wiki/Mosaic_web_browser">was released in 1993.&lt;/a> The concept of an online journal was unthinkable before the invention of the web browser.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Is there an avalanche of low-quality research, and if so, must we stop it?</title><link>https://sevimcengiz.github.io/blog/2013/12/21/is-there-an-avalanche-of-low-quality-research-and-if-so-must-we-stop-it/</link><pubDate>Sat, 21 Dec 2013 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2013/12/21/is-there-an-avalanche-of-low-quality-research-and-if-so-must-we-stop-it/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>&lt;strong>Update:&lt;/strong> &lt;em>It turns out the article in the Chronicle is not recent, I misread the date on the page. (The Chronicle has two dates on each page, today’s date and the article publication date.) I stand by everything else I say, though.&lt;/em>&lt;/p>
&lt;p>A recent article in the Chronicle of Higher Education argues that &lt;a href="http://chronicle.com/article/We-Must-Stop-the-Avalanche-of/65890/">“we must stop the avalanche of low-quality research.”&lt;/a> The authors decry the rapid growth of the scientific literature, which (as they argue) puts increasing strain on readers, reviewers, and editors without producing much benefit. They argue that this growth is driven by an increasing pressure on scientists to publish more, and the result is increasing amounts of low-quality publications. To address the pressure on scientists, they propose three fixes, of which one is Ok and two are positively inane. Maybe what we really have to stop is the avalanche of low-quality, non-reviewed opinion pieces published on web pages?&lt;/p>
&lt;p>Reading through the article, I found it difficult not to wonder whether the authors had ever heard of the internet or of modern information-processing technology (e.g., Google). Now, to be fair, none of the authors are in the natural sciences. The authors work in English, mechanical engineering, medicine, management, and geography. I don’t really know these areas. My own work is in biology, and I’m also somewhat familiar with the publishing cultures in physics and in computer science. So, everything they say may make sense in their fields, but it doesn’t in mine. I’m not convinced we have a major crisis, and I certainly don’t think their proposed fixes are any good. Let’s take a look at their proposed solutions first.&lt;/p>
&lt;div id="limit-number-of-papers-submitted-for-job-applications-or-promotions" class="section level2">
&lt;h2>Limit number of papers submitted for job applications or promotions&lt;/h2>
&lt;p>The first fix, to limit the number of papers that applicants are allowed to submit for job applications or promotions, is actually somewhat reasonable. Applicants should be judged on the quality of their work, not on the mere quantity of output. However, I am strongly opposed to saying applicants are not even allowed to mention anything beyond their key 3-5 papers. Why should productivity be punished? What if they wrote 10 important papers? Should Ed Witten be limited to list only 5 papers? (To date, he has written &lt;a href="http://scholar.google.com/scholar?hl=en&amp;amp;q=edward+witten">over 30 papers with over 1000 citations each!&lt;/a>) While there are negative outliers in academia, people who produce huge amounts of mindless drivel, I definitely see a correlation between quantity and quality. The most interesting and influential papers are generally written by the most productive researchers. I have previously given arguments for why we would &lt;a href="https://sevimcengiz.github.io/blog/2013/11/3/no-one-reads-your-paper-either">expect such a correlation to exist.&lt;/a>&lt;/p>
&lt;p>Most job search and promotion processes that I am aware of have already found a solution to this problem, by asking applicants to submit both (i) a full list of all publications and (ii) the 3-5 most important papers, possibly with a statement explaining their impact. This is good practice that strikes a balance between quality and quantity, it allows applicants to showcase both how good they are and how consistently productive they are, and most importantly, it is already common practice. So point 1 is a non-issue, from where I stand.&lt;/p>
&lt;/div>
&lt;div id="evaluate-researchers-by-impact-factors" class="section level2">
&lt;h2>Evaluate researchers by impact factors&lt;/h2>
&lt;p>Evaluating researchers by impact factor is such an absurd and untimely suggestion, I can’t help but wonder whether the authors have been living under a rock for the last 10 years. It’s particularly ironic that the Chronicle of Higher Education would publish this statement a mere 11 days after nobel-prize winner Randy Schekman publicly proclaimed that luxury (i.e., high impact-factor) journals such as Nature, Cell, and Science &lt;a href="http://www.theguardian.com/commentisfree/2013/dec/09/how-journals-nature-science-cell-damage-science">“are damaging science.”&lt;/a> Did the authors really not see this article, &lt;a href="http://scholarlykitchen.sspnet.org/2013/12/11/this-takes-the-prize-editor-of-new-luxury-oa-journal-boycotts-luxury-subscription-journals/">nor the widespread outrage it caused over containing a cheap plug for a different luxury journal?&lt;/a> If there is one problem we have in science right now, at least in the biomedical field, it’s an over-reliance on impact factors and publications in high-profile journals. The outcry over Schekman’s article shows how sensitive of an issue this is, and how many scientists are concerned about the growing pressure to publish in only the highest-impact journals. Schekman himself addresses this in &lt;a href="http://theconversation.com/how-to-break-free-from-the-stifling-grip-of-luxury-journals-21669">his response to the criticism he received.&lt;/a> Scientists should be judged on the quality of their work, not on whether or not they published in Nature.&lt;/p>
&lt;/div>
&lt;div id="limit-the-length-of-papers-published" class="section level2">
&lt;h2>Limit the length of papers published&lt;/h2>
&lt;p>I don’t see how imposing page limits connects at all to the issue at hand. Surely, if we want fewer but higher-quality publications, the papers should be longer not shorter. Also, I strongly oppose to the split model with a brief (4-6 page) main article (i.e., advertisement) accompanied by longer supporting materials. Invariably, the supporting materials are not written as carefully as the main article, and the quality of the paper as a whole suffers. Notably, PNAS just went the other direction, and now allows papers of up to 10 pages in length in their online-only PNAS Plus edition. This was a very welcome change, I think. The 6-page limit of PNAS was often too limiting, whereas most articles fit comfortably within 10 pages.&lt;/p>
&lt;/div>
&lt;div id="is-there-too-much-pressure-to-publish" class="section level2">
&lt;h2>Is there too much pressure to publish?&lt;/h2>
&lt;p>While there is pressure to publish, frankly I don’t see that there is &lt;em>excessive&lt;/em> pressure to publish. From what I see, for example in conversations with colleagues, the common expectation is reasonable productivity both in terms of quantity and in terms of quality. In terms of quantity, reasonable is usually a number between 1 and 10 papers per year. Publish less, and people start wondering whether you’re working consistently, and in particular whether you’ll keep working in the future. Publish much more than 10 papers per year, and people start looking at you suspiciously. I sat on a grant-review panel once where one applicant claimed his previous 3-year NSF grant had led to ~100 publications. People were very suspicious of this claim and the grant did not get good reviews, even though the science seemed to be reasonable. (I’m not saying the proposal would have been funded if the applicant had had fewer publications, but the high number certainly didn’t help; if it had any effect it was a negative one.) In most areas of Biology, I think 2-3 papers a year will be considered perfectly reasonable for anybody but a senior PI running a large lab. (This includes all papers with your name on, not just first-author papers.)&lt;/p>
&lt;p>In terms of quality, I stick to my earlier recommendation: &lt;a href="https://sevimcengiz.github.io/blog/2013/11/3/no-one-reads-your-paper-either">publish at least one paper a year that has some real substance.&lt;/a> Where exactly that paper is published is secondary, I believe. Publishing the occasional high-profile article in a luxury journal can’t hurt, but I hope that we as scientists can collectively learn to pay a little less attention to where something is published and pay more attention to the content. We shouldn’t hire somebody without having carefully read at least one or two of their papers, and I think the more diligent search committees operate like that already.&lt;/p>
&lt;p>With regards to excessive workload for editors and reviewers, I think there are several things that could be done relatively easily:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>Institute a system of reviewing credits, where you receive one credit for each article you review and you have to spend a number of credits (e.g. 6) to submit an article. This would ensure that everybody who publishes carries their fair share on the reviewing side.&lt;/p>&lt;/li>
&lt;li>&lt;p>Have more graduate students and postdocs review papers. Not every paper needs to be reviewed by three members of the NAS. In fact, I often find that graduate students write better reviews than senior scientists do, because the graduate students take the job much more seriously and put way more effort into it than an established scientist normally would.&lt;/p>&lt;/li>
&lt;li>&lt;p>Have less stringent reviewing criteria, don’t judge impact. Much of the excessive reviewing load actually comes from the pressure to publish in highly selective journals. Thus, many articles make the mandatory trek from Science to Nature to PNAS to PLOS Genetics to PLOS ONE, possibly undergoing four or more separate rounds of review. It’s not uncommon for me to review the same article several times for different journals. And in the end, everything gets published anyway, somewhere. If it was the reviewers’ job to only look for major scientific flaws, then most articles could be published after 1-2 rounds of review, cutting the total review burden way down.&lt;/p>&lt;/li>
&lt;li>&lt;p>Improve tools for post-publication evaluation of articles. At present, all we have is citations and word-of-mouth. (“Have you seen the latest paper by X in PNAS? It’s really not very good.”) I’m sure we can do better than that, and over time we’ll find ways to put modern computing power and crowd-sourcing ideas to good use. &lt;a href="http://www.the-scientist.com/?articles.view/articleNo/37969/title/Post-Publication-Peer-Review-Mainstreamed/">NCBI’s PubMed Commons is a first step in this direction.&lt;/a> I’m sure over the next 10-20 years we’ll see many more innovative ideas to evaluate the quality of scientific work post publication.&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>The value of pre-publication peer review</title><link>https://sevimcengiz.github.io/blog/2013/12/21/the-value-of-pre-publication-peer-review/</link><pubDate>Sat, 21 Dec 2013 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2013/12/21/the-value-of-pre-publication-peer-review/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>I see lot of discussion these days about the value of peer review. Are journals too selective? Are acceptance decisions arbitrary? Does peer review actually catch scientific mistakes or fraudulent practices? Wouldn’t it be better to just put everything out there, say on preprint servers, and separate the wheat from the chaff in post-publication review? I’m not quite ready yet to give up on pre-publication peer review. I think it serves a useful purpose, one I wouldn’t want to do away with. In the following, I discuss four distinct services that peer review provides, and assess the value I personally assign to each of them.&lt;/p>
&lt;div id="peer-review-screens-out-nonsense-and-pseudoscience" class="section level2">
&lt;h2>Peer review screens out nonsense and pseudoscience&lt;/h2>
&lt;p>It’s important that somebody screen all potential scientific publications for actual scientific content. I don’t mind publishing null results, replication studies, or studies that present only a very minor advance. All of these works contain real science, and they may find some use at some point in the future. However, we must never mix science with pseudoscience. Someone has to assure that whatever gets published in a scientific journal is not complete nonsense. Even the preprint archive arxiv.org has &lt;a href="http://arxiv.org/help/endorsement">some sort of a screening and filtering system in place to hold back the crackpots.&lt;/a> In most cases, nonsensical papers would be caught by the editor and not even sent out to review. Nevertheless, we can consider filtering out nonsense to be an essential service of the pre-publication review process.&lt;/p>
&lt;/div>
&lt;div id="peer-review-catches-major-mistakes-andor-fraud" class="section level2">
&lt;h2>Peer review catches major mistakes and/or fraud&lt;/h2>
&lt;p>Many people seem to think that it is the reviewers’ job to catch major mistakes and/or fraud. And when they fail to do so, that is taken as evidence that peer review doesn’t work. I don’t think we can put such a high burden on the reviewers. Ultimately, the burden of producing correct and genuine results lies with the author. Peer review operates under the assumption that fundamentally the authors are honest and reasonably capable scientists. If peer review does happen to catch a major issue with a paper, that’s great, but generally I think that post-publication review is the much better venue to address major flaws or scientific misconduct.&lt;/p>
&lt;/div>
&lt;div id="peer-review-assess-novelty-potential-impact-and-fit-with-the-journal-scope" class="section level2">
&lt;h2>Peer review assess novelty, potential impact, and fit with the journal scope&lt;/h2>
&lt;p>Whether reviewers (or editors) should consider novelty and impact, and whether journals should be selective at all, is probably the most contentious issue in peer review. Traditionally, this has always been part of peer review. However, there are now several journals that explicitly state review should only assess scientific soundness (e.g. &lt;a href="http://www.plosone.org/static/information">PLOS ONE&lt;/a> or &lt;a href="https://peerj.com/about/aims-and-scope/">PeerJ&lt;/a>). I think there are valid arguments for both sides. On the one hand, it is imperative that we have publishing venues that will publish any scientifically sound study. Nobody benefits if a valid study is suppressed just because some reviewers didn’t find it interesting. If there’s no obvious scientific flaw, put it out there and let the readers (and Google) sort it out.&lt;/p>
&lt;p>On the other hand, I think that more selective journals can provide value as well. In my mind, where science has gone off-track is that the most selective journals (which are also considered to be the most prestigious ones, e.g. Nature, Science, Cell, PLOS Biology, PNAS) employ arbitrary selection criteria based primarily on the subjective goal of publishing “the best science.” As a consequence, whether I can publish in such journals depends much more on my marketing skills than on my scientific skills, and also on whether I’m working on a sexy study system.&lt;/p>
&lt;p>By contrast, the next lower tier of selective journals usually employ more objective selection criteria, and those arguably provide a useful value. For example, I’m an Associate Editor for PLOS Computational Biology, a fairly selective journal. The main requirement for publication in PLOS Computational Biology is &lt;a href="http://www.ploscompbiol.org/static/information">that you have produced high quality computational work that yields a novel biological insight.&lt;/a> In my mind, it is fairly straightforward to determine whether a paper satisfies that requirement or not. I also think that any capable computational biologist can jump over that bar. As a consequence, I feel that we’re providing useful selectivity without generating excessive artificial scarcity or making highly arbitrary decisions. If I see that somebody has on their CV a couple of PLOS Computational Biology papers, I can reasonably assume that they are doing consistent, high-quality computational work leading to novel insights into biological systems.&lt;/p>
&lt;/div>
&lt;div id="peer-review-helps-authors-improve-their-articles" class="section level2">
&lt;h2>Peer review helps authors improve their articles&lt;/h2>
&lt;p>In my mind, this last point is the most important point, and the reason why I’m not willing to give up pre-publication review in its entirety. In my experience as author, reviewer, and editor, the most common outcome of the review process other than “reject due to insufficient novelty” is “major revision.” The reviewers agree that the study has merit in principle, but they see a number of possible revisions that would improve the article. I have seen it countless times, both as author and as reviewer or editor, that a study was vastly improved after the first set of reviews. Sometimes reviewers catch an issue the authors hadn’t noticed, sometimes they have a really cool idea that brings the study to the next level, and sometimes they simply tell you that you have to work on your writing if you want to get your point across. Either way, this input is invaluable, and it improves the scientific literature tremendously. If we went to a system that operated entirely on post-publication review, we would probably still see the same kind of comments by reviewers, but there would be very little incentive for the authors to go and revise their papers accordingly.&lt;/p>
&lt;p>One downside to this aspect of peer review is that sometimes reviewers just keep insisting on changes that the authors don’t deem necessary or appropriate. This is another form of peer review gone wrong. The reviewers should make helpful suggestions, but they should not tell the authors how to write their paper. One solution to this issue is to make peer reviews public and leave with the authors the ultimate decision of whether or not they want to publish, as &lt;a href="http://www.biologydirect.com/about">Biology Direct does.&lt;/a> Another possibility is to allow authors to opt-out of re-review, as &lt;a href="http://www.biomedcentral.com/bmcbiol/about#publication">BMC Biology does.&lt;/a>&lt;/p>
&lt;/div></description></item></channel></rss>