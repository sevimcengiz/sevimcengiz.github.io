<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>dplyr on Sevim Cengiz</title><link>https://sevimcengiz.github.io/tags/dplyr/</link><description>Recent content in dplyr on Sevim Cengiz</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 13 Jun 2016 00:00:00 +0000</lastBuildDate><atom:link href="https://sevimcengiz.github.io/tags/dplyr/index.xml" rel="self" type="application/rss+xml"/><item><title>Reading and combining many tidy data files in R</title><link>https://sevimcengiz.github.io/blog/2016/06/13/reading-and-combining-many-tidy-data-files-in-r/</link><pubDate>Mon, 13 Jun 2016 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2016/06/13/reading-and-combining-many-tidy-data-files-in-r/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>&lt;strong>Update June 16, 2019:&lt;/strong> &lt;em>This post is now three years old, and some of the advice given is now outdated. Most importantly, it is much better to use &lt;code>map_dfr()&lt;/code> than &lt;code>map(...) %&amp;gt;% reduce(rbind)&lt;/code>.&lt;/em>&lt;/p>
&lt;p>Everybody who is familiar with the R libraries for processing of tidy data, such as &lt;code>dplyr&lt;/code> and &lt;code>ggplot&lt;/code>, knows how powerful they are and how much one can get done with just a few lines of R code. However, similarly, everybody who has used them has probably spent more time bringing data into the appropriate tidy format than writing analysis and/or plotting code. In particular, one scenario that arises all the time is that even if data files are in tidy format, the entire dataset may be spread out over many individual files, and loading them all in and combining them into one large table can be cumbersome. Here, I want to demonstrate some neat tricks, using the relatively new package &lt;code>purrr&lt;/code> and some recent additions to the package &lt;code>tidyr&lt;/code>, that make loading and combining many data files a piece of cake.&lt;/p>
&lt;p>The code shown here depends on the following R packages:&lt;/p>
&lt;pre class="r">&lt;code>require(readr) # for read_csv()
require(dplyr) # for mutate()
require(tidyr) # for unnest()
require(purrr) # for map(), reduce()&lt;/code>&lt;/pre>
&lt;div id="reading-in-all-files-matching-a-given-name" class="section level2">
&lt;h2>Reading in all files matching a given name&lt;/h2>
&lt;p>As an example, we will consider a scenario where we have population census data for various cities, stored in individual csv files for each city. The data I’m using here comes from &lt;a href="http://factfinder.census.gov/" class="uri">http://factfinder.census.gov/&lt;/a>.&lt;/p>
&lt;p>The first scenario we will consider is one where we want to read all csv files in the current working directory. To achieve this goal, we first list all &lt;code>*.csv&lt;/code> files, using the function &lt;code>dir()&lt;/code>. We find that there are three, for the cities Houston, Los Angeles, and New York:&lt;/p>
&lt;pre class="r">&lt;code># find all file names ending in .csv
files &amp;lt;- dir(pattern = &amp;quot;*.csv&amp;quot;)
files&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;Houston_TX.csv&amp;quot; &amp;quot;Los Angeles_CA.csv&amp;quot; &amp;quot;New York_NY.csv&amp;quot;&lt;/code>&lt;/pre>
&lt;p>We can then read in those files and combine them into one data frame using the &lt;code>purrr&lt;/code> functions &lt;code>map()&lt;/code> and &lt;code>reduce()&lt;/code>:&lt;/p>
&lt;pre class="r">&lt;code>data &amp;lt;- files %&amp;gt;%
map(read_csv) %&amp;gt;% # read in all the files individually, using
# the function read_csv() from the readr package
reduce(rbind) # reduce with rbind into one dataframe
data&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [15 x 3]
##
## location year population
## (chr) (int) (int)
## 1 Houston, TX 2011 2142221
## 2 Houston, TX 2012 2177376
## 3 Houston, TX 2013 2216460
## 4 Houston, TX 2014 2256192
## 5 Houston, TX 2015 2296224
## 6 Los Angeles, CA 2011 3828604
## 7 Los Angeles, CA 2012 3864724
## 8 Los Angeles, CA 2013 3902005
## 9 Los Angeles, CA 2014 3936940
## 10 Los Angeles, CA 2015 3971883
## 11 New York, NY 2011 8287000
## 12 New York, NY 2012 8365069
## 13 New York, NY 2013 8436047
## 14 New York, NY 2014 8495194
## 15 New York, NY 2015 8550405&lt;/code>&lt;/pre>
&lt;p>Often, we want to read the data from a given directory rather than from the current working directory. The ability to define functions on-the-fly in &lt;code>purrr&lt;/code> makes this easy:&lt;/p>
&lt;pre class="r">&lt;code>data_path &amp;lt;- &amp;quot;city_data&amp;quot; # path to the data
files &amp;lt;- dir(data_path, pattern = &amp;quot;*.csv&amp;quot;) # get file names
data &amp;lt;- files %&amp;gt;%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %&amp;gt;%
reduce(rbind)
data&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [15 x 3]
##
## location year population
## (chr) (int) (int)
## 1 Houston, TX 2011 2142221
## 2 Houston, TX 2012 2177376
## 3 Houston, TX 2013 2216460
## 4 Houston, TX 2014 2256192
## 5 Houston, TX 2015 2296224
## 6 Los Angeles, CA 2011 3828604
## 7 Los Angeles, CA 2012 3864724
## 8 Los Angeles, CA 2013 3902005
## 9 Los Angeles, CA 2014 3936940
## 10 Los Angeles, CA 2015 3971883
## 11 New York, NY 2011 8287000
## 12 New York, NY 2012 8365069
## 13 New York, NY 2013 8436047
## 14 New York, NY 2014 8495194
## 15 New York, NY 2015 8550405&lt;/code>&lt;/pre>
&lt;p>Here, the expression &lt;code>~ read_csv(file.path(data_path, .))&lt;/code> is a shortcut for the anonymous function definition &lt;code>function(x) read_csv(file.path(data_path, x))&lt;/code>:&lt;/p>
&lt;pre class="r">&lt;code># this code does the exact same thing as the previous code
data &amp;lt;- files %&amp;gt;%
map(function(x) read_csv(file.path(data_path, x))) %&amp;gt;%
reduce(rbind)
data&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [15 x 3]
##
## location year population
## (chr) (int) (int)
## 1 Houston, TX 2011 2142221
## 2 Houston, TX 2012 2177376
## 3 Houston, TX 2013 2216460
## 4 Houston, TX 2014 2256192
## 5 Houston, TX 2015 2296224
## 6 Los Angeles, CA 2011 3828604
## 7 Los Angeles, CA 2012 3864724
## 8 Los Angeles, CA 2013 3902005
## 9 Los Angeles, CA 2014 3936940
## 10 Los Angeles, CA 2015 3971883
## 11 New York, NY 2011 8287000
## 12 New York, NY 2012 8365069
## 13 New York, NY 2013 8436047
## 14 New York, NY 2014 8495194
## 15 New York, NY 2015 8550405&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="keeping-auxilliary-information-about-the-files-read" class="section level2">
&lt;h2>Keeping auxilliary information about the files read&lt;/h2>
&lt;p>One limitation of the previous approach is that we don’t keep any auxilliary information we may want to, such as the filenames of the files read. To keep the filename alongside the data, we can read the data into a nested dataframe rather than a list, using the &lt;code>mutate()&lt;/code> function from &lt;code>dplyr&lt;/code>. This gives us the following result:&lt;/p>
&lt;pre class="r">&lt;code>data &amp;lt;- data_frame(filename = files) %&amp;gt;% # create a data frame
# holding the file names
mutate(file_contents = map(filename, # read files into
~ read_csv(file.path(data_path, .))) # a new data column
)
data&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [3 x 2]
##
## filename file_contents
## (chr) (chr)
## 1 Houston_TX.csv &amp;lt;tbl_df [5,3]&amp;gt;
## 2 Los Angeles_CA.csv &amp;lt;tbl_df [5,3]&amp;gt;
## 3 New York_NY.csv &amp;lt;tbl_df [5,3]&amp;gt;&lt;/code>&lt;/pre>
&lt;p>To turn this data frame into one useful for downstream analysis, we use the function &lt;code>unnest()&lt;/code> from &lt;code>tidyr&lt;/code>:&lt;/p>
&lt;pre class="r">&lt;code>unnest(data)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [15 x 4]
##
## filename location year population
## (chr) (chr) (int) (int)
## 1 Houston_TX.csv Houston, TX 2011 2142221
## 2 Houston_TX.csv Houston, TX 2012 2177376
## 3 Houston_TX.csv Houston, TX 2013 2216460
## 4 Houston_TX.csv Houston, TX 2014 2256192
## 5 Houston_TX.csv Houston, TX 2015 2296224
## 6 Los Angeles_CA.csv Los Angeles, CA 2011 3828604
## 7 Los Angeles_CA.csv Los Angeles, CA 2012 3864724
## 8 Los Angeles_CA.csv Los Angeles, CA 2013 3902005
## 9 Los Angeles_CA.csv Los Angeles, CA 2014 3936940
## 10 Los Angeles_CA.csv Los Angeles, CA 2015 3971883
## 11 New York_NY.csv New York, NY 2011 8287000
## 12 New York_NY.csv New York, NY 2012 8365069
## 13 New York_NY.csv New York, NY 2013 8436047
## 14 New York_NY.csv New York, NY 2014 8495194
## 15 New York_NY.csv New York, NY 2015 8550405&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="creating-filenames-from-data" class="section level2">
&lt;h2>Creating filenames from data&lt;/h2>
&lt;p>In the previous examples, we have read in all the data files in a given directory. Often, however, we would rather read in specific files based on other data we have. For example, let’s assume we have the following data table:&lt;/p>
&lt;pre class="r">&lt;code>cities &amp;lt;- data_frame(city = c(&amp;quot;New York&amp;quot;, &amp;quot;Houston&amp;quot;),
state = c(&amp;quot;NY&amp;quot;, &amp;quot;TX&amp;quot;),
area = c(305, 599.6))
cities&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [2 x 3]
##
## city state area
## (chr) (chr) (dbl)
## 1 New York NY 305.0
## 2 Houston TX 599.6&lt;/code>&lt;/pre>
&lt;p>We want to use the city and state columns to create appropriate filenames and then load in the corresponding files. The code in its entirety looks as follows:&lt;/p>
&lt;pre class="r">&lt;code>data &amp;lt;- cities %&amp;gt;% # start with the cities table
# create filenames
mutate(filename = paste(city, &amp;quot;_&amp;quot;, state, &amp;quot;.csv&amp;quot;, sep=&amp;quot;&amp;quot;)) %&amp;gt;%
# read in data
mutate(file_contents = map(filename,
~ read_csv(file.path(data_path, .)))
) %&amp;gt;%
select(-filename) %&amp;gt;% # remove filenames, not needed anynmore
unnest() %&amp;gt;% # unnest
select(-location) # remove location column, not needed
# since we have city and state columns
data&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [10 x 5]
##
## city state area year population
## (chr) (chr) (dbl) (int) (int)
## 1 New York NY 305.0 2011 8287000
## 2 New York NY 305.0 2012 8365069
## 3 New York NY 305.0 2013 8436047
## 4 New York NY 305.0 2014 8495194
## 5 New York NY 305.0 2015 8550405
## 6 Houston TX 599.6 2011 2142221
## 7 Houston TX 599.6 2012 2177376
## 8 Houston TX 599.6 2013 2216460
## 9 Houston TX 599.6 2014 2256192
## 10 Houston TX 599.6 2015 2296224&lt;/code>&lt;/pre>
&lt;p>I hope you have found these examples useful, and you will start loading files into nested data frames.&lt;/p>
&lt;/div></description></item><item><title>Teaching a new introductory class in computational biology and bioinformatics</title><link>https://sevimcengiz.github.io/blog/2015/02/04/teaching-a-new-introductory-class-in-computational-biology-and-bioinformatics/</link><pubDate>Wed, 04 Feb 2015 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2015/02/04/teaching-a-new-introductory-class-in-computational-biology-and-bioinformatics/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>This semester, I’m teaching a new introductory class in computational biology and bioinformatics. The class is primarily targeted at undergraduates, and it is split approximately 50:50 between R and python. The R component emphasizes effective data analysis and visualization, using packages such as ggplot2 and dplyr. The python component will introduce students to basic programming concepts, and it will also cover some typical bioinformatics applications.&lt;/p>
&lt;p>Developing a new class is a lot of work, so I’ll probably have much less time for posting here on my blog. However, on the flip side, the entire course content will be posted online, and you can &lt;a href="https://wilkelab.org/classes/SDS348_spring_2015.html">follow along here.&lt;/a> The core of each lecture is an in-class exercise worksheet, and I’m posting the worksheets and the solutions online. Many lectures also have a brief traditional lecture component with slides as well as additional reading materials. I’m developing the course as I go, so there will be new material posted twice a week throughout the spring.&lt;/p></description></item><item><title>A grammar of data manipulation</title><link>https://sevimcengiz.github.io/blog/2014/09/17/a-grammar-of-data-manipulation/</link><pubDate>Wed, 17 Sep 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/09/17/a-grammar-of-data-manipulation/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>It seems that Hadley Wickham, the author of the spectacular &lt;a href="http://ggplot2.org/">ggplot2&lt;/a> library for R, is not content with revolutionizing the world of computational data analysis just once. He keeps doing it. This spring, he released the &lt;a href="http://cran.r-project.org/web/packages/dplyr/index.html">dplyr&lt;/a> package, a package that proposes a grammar of data manipulation. I predict that dplyr will become as important for large-scale data analysis and manipulation as ggplot2 has become for visualization. If you like ggplot2, you will love dplyr.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>&lt;/p>
&lt;p>dplyr is the next iteration of the popular plyr package, only that it is 100 times faster and way more intuitive. It provides a clean interface to work with data sets that are “tidy.” (See also my previous two blog posts on tidy data &lt;a href="https://sevimcengiz.github.io/blog/2014/7/20/keep-your-data-tidy">here&lt;/a> and &lt;a href="https://sevimcengiz.github.io/blog/2014/7/21/keep-your-data-tidy-part-ii">here.&lt;/a>) Let me whet your appetite by showing you a simple analysis I did the other day.&lt;/p>
&lt;p>I wanted to find out how much evolutionary divergence there is between human genes and the corresponding yeast (&lt;em>S. cerevisiae&lt;/em>) orthologs. Specifically, I was interested in the range of sequence identities among genes, i.e., what are the most conserved genes, the most diverged genes, what is the mean divergence, and so on. The analysis has an additional twist in that there are different types of ortholog pairs. There are one-to-one orthologs, where the human gene has exactly one counter-part in the yeast genome, there are one-to-many orthologs, where the gene in one organism has multiple counter-parts in the other organism, and there are many-to-many orthologs, where there are multiple genes in both organisms that are orthologous to each other. Thus, I wanted to carry out my analysis by orthology type.&lt;/p>
&lt;p>I went to &lt;a href="http://www.ensembl.org/index.html">ensembl&lt;/a> and downloaded all human-to-yeast orthologs with their respective sequence identities. The resulting csv file is available &lt;a href="https://dl.dropboxusercontent.com/u/97817736/human_yeast_divergence.csv">here.&lt;/a> We can download this file directly into R, using the RCurl package. We’ll also load the dplyr package, since we’ll need it later:&lt;/p>
&lt;pre class="r">&lt;code>library(RCurl)
library(dplyr)
url &amp;lt;- &amp;quot;https://dl.dropboxusercontent.com/u/97817736/human_yeast_divergence.csv&amp;quot;
data &amp;lt;- read.csv(textConnection(getURL(url)))&lt;/code>&lt;/pre>
&lt;p>Let’s take a look at the data. There are five columns, the gene id for the human gene, the gene id for the
corresponding yeast ortholog, the homology type (one-to-one, one-to-many, many-to-many), the percent identity with respect to both the human (&lt;code>perc.ident.human&lt;/code>) and the yeast (&lt;code>perc.ident.yeast&lt;/code>) gene, and a confidence score that tells us how confident we are that the two genes are actually orthologous.&lt;/p>
&lt;pre class="r">&lt;code>head(data)&lt;/code>&lt;/pre>
&lt;pre>&lt;code> human.gene.ID yeast.gene.ID homology.type perc.ident.human
1 ENSG00000100077 YKL126W ortholog_many2many 19
2 ENSG00000100077 YHR205W ortholog_many2many 18
3 ENSG00000100077 YMR104C ortholog_many2many 18
4 ENSG00000196139 YHR104W ortholog_one2many 33
5 ENSG00000173213 YFL037W ortholog_one2many 71
6 ENSG00000154930 YLR153C ortholog_many2many 43
perc.identity.yeast confidence
1 19 1
2 15 1
3 18 1
4 33 1
5 69 1
6 44 1 &lt;/code>&lt;/pre>
&lt;p>Now we’re ready for some dplyr magic. Let’s assume we want to analyze the data by homology type, and we want to find the minimum, mean, median, and maximum sequence identity for each homology type, as well as the standard deviation of the identity distribution. All this can be achieved with the following code:&lt;/p>
&lt;pre class="r">&lt;code>data %&amp;gt;% group_by(homology.type) %&amp;gt;%
summarize(
min=min(perc.ident.human),
mean=mean(perc.ident.human),
std.dev=sd(perc.ident.human),
max=max(perc.ident.human))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>Source: local data frame [3 x 5]
homology.type min mean std.dev max
1 ortholog_many2many 1 26.36965 16.37955 92
2 ortholog_one2many 0 27.46243 15.18146 86
3 ortholog_one2one 1 33.04183 12.89296 80&lt;/code>&lt;/pre>
&lt;p>The function &lt;code>group_by()&lt;/code> states that we want to carry out the analysis separately for each homology type, and the function &lt;code>summarize()&lt;/code> calculates the summary statistics (min, mean, etc.) for each group. The operator &lt;code>%&amp;gt;%&lt;/code> is a chaining operator, like a pipe in the UNIX command line. It takes the output from the previous operation and feeds it into the subsequence operation.&lt;/p>
&lt;p>As you can see, dplyr syntax focuses entirely on the logical flow of the data analysis. You don’t ever have to worry about bookkeeping, looping over cases, or details of the data storage.&lt;/p>
&lt;p>Let’s do another example. Let’s say we’re only interested in one-to-one orthologs, and we want to find the top 10 least diverged yeast genes and list them in descending order. The following lines achieve this:&lt;/p>
&lt;pre class="r">&lt;code>data %&amp;gt;% filter(homology.type==&amp;#39;ortholog_one2one&amp;#39;) %&amp;gt;%
select(yeast.gene.ID, human.gene.ID, perc.ident.human) %&amp;gt;%
top_n(10) %&amp;gt;%
arrange(desc(perc.ident.human))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>Selecting by perc.ident.human
yeast.gene.ID human.gene.ID perc.ident.human
1 YLR167W ENSG00000143947 80
2 YDR064W ENSG00000110700 77
3 YKL145W ENSG00000161057 75
4 YOR210W ENSG00000177700 75
5 YGL048C ENSG00000087191 74
6 YPL086C ENSG00000134014 74
7 YPR016C ENSG00000242372 72
8 YJR121W ENSG00000110955 72
9 YDL007W ENSG00000100764 71
10 YEL027W ENSG00000185883 70 &lt;/code>&lt;/pre>
&lt;p>The function &lt;code>filter()&lt;/code> selects all rows of the given homology type, the function &lt;code>select()&lt;/code> picks the specific columns we are interested in, the function &lt;code>top_n()&lt;/code> selects the top &lt;em>n&lt;/em> values in the last data column (i.e., column &lt;code>perc.ident.human&lt;/code> in this example), and the function &lt;code>arrange()&lt;/code> sorts the data.&lt;/p>
&lt;p>If these examples have piqued your interest and you want to learn more, I recommend you start by reading the dplyr vignette, which you can find here: &lt;a href="http://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html">http://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html&lt;/a>&lt;br />
There is also an excellent introduction by Kevin Markham, with accompanying 40 minute video, available here: &lt;a href="http://rpubs.com/justmarkham/dplyr-tutorial">http://rpubs.com/justmarkham/dplyr-tutorial&lt;/a>&lt;/p>
&lt;p>And have I mentioned already that dplyr has a &lt;a href="http://cran.rstudio.com/web/packages/dplyr/vignettes/databases.html">database backend,&lt;/a> so you can now easily use all the statistical sophistication that R provides on arbitrarily large, remotely stored data sets.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>And if you aren’t familiar with &lt;a href="http://ggplot2.org/">ggplot2,&lt;/a> spend some time with it. It is fantastic, though it may feel alien initially. If you’re used to traditional visualization approaches, you may think in terms of drawing points and lines onto a canvas. ggplot2 requires you to approach visualization in a completely different way, in terms of mapping features of the data onto aesthetic features of the graph. Once you get this way of thinking, it becomes rather powerful.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>