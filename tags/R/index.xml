<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>R on Sevim Cengiz</title><link>https://sevimcengiz.github.io/tags/R/</link><description>Recent content in R on Sevim Cengiz</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 08 Sep 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://sevimcengiz.github.io/tags/R/index.xml" rel="self" type="application/rss+xml"/><item><title>Writing a blogdown post for the ages</title><link>https://sevimcengiz.github.io/blog/2020/09/08/a-blogdown-post-for-the-ages/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2020/09/08/a-blogdown-post-for-the-ages/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>The R package &lt;a href="https://bookdown.org/yihui/blogdown/">blogdown&lt;/a> has become a widely popular solution to setting up personal blogs. It makes it super easy to set up quite elaborate websites, and to write posts that contain R code, generated output and figures, footnotes, figure references, and math.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> However, one problem with blogdown is that it likes to re-knit &lt;code>.Rmd&lt;/code> files.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> This may be fine if you’re just starting out with your site or if your posts generally don’t contain any sophisticated R code, but in a long-standing blog you’ll eventually run into trouble. First, re-knitting hundreds of posts may be quite slow. And second, if you’ve got a bunch of old posts chances are some will not knit anymore, and then you may have got a serious problem with no simple solution.&lt;/p>
&lt;p>This problem has been recognized for a while, and the proposed solution is usually to knit only on demand. See e.g. &lt;a href="https://yutani.rbind.io/post/2017-10-25-blogdown-custom/">here&lt;/a>. The experimental &lt;a href="https://github.com/r-lib/hugodown">hugodown package&lt;/a> likewise aims to limit any unnecessary re-knitting. Here, I’m taking a different approach. My perspective is that I want to be able to re-knit any time without worrying that I’ll destroy anything of value, and I also want to be able to add code and output to posts containing prior code that doesn’t run anymore today.&lt;/p>
&lt;p>My approach is to copy the knitted markdown code and output back into the &lt;code>.Rmd&lt;/code> file. This requires some amount of manual work, but it’s not that bad, and I value the benefits I get from this approach. Maybe at some point somebody will write a package that can automate this process.&lt;/p>
&lt;p>I do not necessarily recommend the approach I’m taking here. This post is mostly for my own purposes, so I can retrace my steps in the future. If you want to see the source code resulting from this process, you can check out the &lt;a href="https://github.com/clauswilke/clauswilke.github.io/blob/master/content/blog/2020-09-08-a-blogdown-post-for-the-ages/index.Rmd">source for this post&lt;/a> on github.&lt;/p>
&lt;p>To provide an example scenario, I include here one chunk of R code that generates a figure. This code has various features that will likely generate issues in the future or in a blog with many posts:&lt;/p>
&lt;ul>
&lt;li>It depends on a bunch of packages, including one only available from github.&lt;/li>
&lt;li>It uses various fonts that need to be installed locally.&lt;/li>
&lt;li>It is slow to render.&lt;/li>
&lt;/ul>
&lt;p>So it is critical that we can capture the output and don’t ever have to re-render it again.&lt;/p>
&lt;p>Here is the example:&lt;/p>
&lt;pre class="r">&lt;code>library(tidyverse)
library(cowplot)
library(colorspace)
library(sf)
library(ggtext)
# attach data set, requires practicalgg package
# remotes::install_github(&amp;quot;wilkelab/practicalgg&amp;quot;)
data(texas_income, package = &amp;quot;practicalgg&amp;quot;)
ggplot(texas_income, aes(fill = estimate)) +
geom_sf(color = &amp;quot;white&amp;quot;) +
coord_sf(xlim = c(538250, 2125629), crs = 3083) +
scale_fill_continuous_sequential(
palette = &amp;quot;Blues&amp;quot;, rev = TRUE,
na.value = &amp;quot;grey60&amp;quot;,
name = &amp;quot;annual median income (USD)&amp;quot;,
limits = c(18000, 90000),
breaks = 20000*c(1:4),
labels = c(&amp;quot;$20,000&amp;quot;, &amp;quot;$40,000&amp;quot;, &amp;quot;$60,000&amp;quot;, &amp;quot;$80,000&amp;quot;),
guide = guide_colorbar(
direction = &amp;quot;horizontal&amp;quot;,
label.position = &amp;quot;bottom&amp;quot;,
title.position = &amp;quot;top&amp;quot;,
barwidth = grid::unit(3.0, &amp;quot;in&amp;quot;),
barheight = grid::unit(0.2, &amp;quot;in&amp;quot;)
)
) +
labs(caption = &amp;quot;
&amp;lt;span style=&amp;#39;font-family: \&amp;quot;Font Awesome 5 Brands\&amp;quot;&amp;#39;&amp;gt;&amp;amp;#xf099;&amp;lt;/span&amp;gt;
@clauswilke&amp;lt;br&amp;gt;
&amp;lt;span style=&amp;#39;font-family: \&amp;quot;Font Awesome 5 Free Solid\&amp;quot;&amp;#39;&amp;gt;&amp;amp;#xf781;&amp;lt;/span&amp;gt;
clauswilke.com
&amp;quot;) +
theme_map(12, font_family = &amp;quot;Myriad Pro&amp;quot;) +
theme(
legend.title.align = 0.5,
legend.text.align = 0.5,
legend.justification = c(0, 0),
legend.position = c(0.02, 0.1),
plot.caption = element_markdown()
)&lt;/code>&lt;/pre>
&lt;div class="figure">&lt;span id="fig:map-Texas-income">&lt;/span>
&lt;img src="figure-html/map-Texas-income-1.png" alt="Median annual income in Texas counties. Figure redrawn from: Wilke (2019) Fundamentals of Data Visualization, Chapter 4." width="576" />
&lt;p class="caption">
Figure 1: Median annual income in Texas counties. Figure redrawn from: Wilke (2019) Fundamentals of Data Visualization, &lt;a href="https://clauswilke.com/dataviz/color-basics.html">Chapter 4.&lt;/a>
&lt;/p>
&lt;/div>
&lt;p>Next I’ll provide the exact recipe I follow to capture the output from such code.&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>At the top of your &lt;code>.Rmd&lt;/code> file, add an R chunk containing the following:&lt;/p>
&lt;pre>&lt;code>```{r echo = FALSE}
knitr::opts_chunk$set(fig.retina = 2)
```&lt;/code>&lt;/pre>
&lt;p>This will ensure that figures are rendered in high quality. Set &lt;code>echo = FALSE&lt;/code> for this chunk so the code isn’t included in the rendered output.&lt;/p>&lt;/li>
&lt;li>&lt;p>Stop the blogdown server with &lt;code>blogdown::stop_server()&lt;/code>. We don’t want the server to try to create blog posts out of the intermediate files we’ll be creating.&lt;/p>&lt;/li>
&lt;li>&lt;p>Add the following to the yaml section of your post:&lt;/p>
&lt;pre>&lt;code>output:
&amp;nbsp;&amp;nbsp;html_document:
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;keep_md: yes
&lt;/code>&lt;/pre>
&lt;p>If you want to use bookdown-style automated figure references, use this snippet instead:&lt;/p>
&lt;pre>&lt;code>output:
&amp;nbsp;&amp;nbsp;bookdown::html_document2:
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;keep_md: yes
&lt;/code>&lt;/pre>
&lt;p>This requires the bookdown package to be installed.&lt;/p>&lt;/li>
&lt;li>&lt;p>Knit your post. You will end up with a new file &lt;code>index.md&lt;/code> and a new folder called &lt;code>index_files&lt;/code>. The former contains the markdown code that knitr has generated and the latter contains any generated figures.&lt;/p>&lt;/li>
&lt;li>&lt;p>Now you want to copy the generated code and output chunks from &lt;code>index.md&lt;/code> back into &lt;code>index.Rmd&lt;/code>. For each code chunk in your &lt;code>.Rmd&lt;/code> file, there will be one or more markdown chunks, which are fenced with &lt;code>```r ...```&lt;/code>. There will also be markdown or HTML code to include any generated figures. Place all of this material after the respective code chunk from which it originated, but &lt;strong>do not delete&lt;/strong> the original code chunk. We want to keep the original code chunks around in case we do want to re-run some of the R code again in the future, e.g. if the post needs an update.&lt;/p>&lt;/li>
&lt;li>&lt;p>Next, you need to move the generated figures into a safe location. This ensures that they won’t be deleted when blogdown rebuilds the site the next time. I simply move the folder &lt;code>index_files/figure-html&lt;/code> to &lt;code>figure-html&lt;/code>.&lt;/p>&lt;/li>
&lt;li>&lt;p>Edit figure links to reflect the move from the previous step. Figure links may be included either as markdown links, such as &lt;code>![](index_files/figure-html/map-Texas-income-1.png)&lt;/code>, or as html links, such as &lt;code>&amp;lt;img src="https://sevimcengiz.github.io/blog/2020-09-08-a-blogdown-post-for-the-ages/index_files/figure-html/map-Texas-income-1.png" ...&lt;/code>. Which is the case depends on the exact chunk options you used to generate the figure. In either case, delete &lt;code>index_files/&lt;/code> from all figure links.&lt;/p>&lt;/li>
&lt;li>&lt;p>Delete the file &lt;code>index.md&lt;/code>.&lt;/p>&lt;/li>
&lt;li>&lt;p>Remove or comment out the &lt;code>output:&lt;/code> block you added under step 3.&lt;/p>&lt;/li>
&lt;li>&lt;p>Add the following line to the code chunk added under step 1:&lt;br />
&lt;/p>
&lt;pre>&lt;code>knitr::opts_chunk$set(echo = FALSE, eval = FALSE)
&lt;/code>&lt;/pre>
&lt;p>This turns off all the R Markdown chunks in your post.&lt;/p>&lt;/li>
&lt;li>&lt;p>Restart the blogdown server with &lt;code>blogdown::serve_site()&lt;/code>.&lt;/p>&lt;/li>
&lt;/ol>
&lt;p>This may seem like a lot of steps and a lot of fiddling, but it’s really not that bad once you get the hang of it. Most blog posts, even elaborate ones, don’t have that many code chunks or figures, and manually copying and adjusting the markdown code takes much less time than writing the blog post in the first place.&lt;/p>
&lt;p>In the future, if you need to update your post, you can either re-run all code by commenting out the line you added in step 10, or you can selectively turn on individual R chunks by setting their &lt;code>echo&lt;/code> and &lt;code>eval&lt;/code> options to &lt;code>TRUE&lt;/code>. Then you repeat steps 1 through 11, but copying only whichever output needs to be newly copied over. At the end make sure you disable all R chunks once again.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>This post has one figure, Figure &lt;a href="#fig:map-Texas-income">1&lt;/a>. It also has one equation, &lt;span class="math inline">\(a^2 + b^2 = c^2\)&lt;/span>. The equation serves no purpose here.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Throughout, I’m assuming you’re using &lt;code>.Rmd&lt;/code> files. Everything I say should be valid for &lt;code>.Rmarkdown&lt;/code> as well, though I haven’t tested this.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>PCA tidyverse style</title><link>https://sevimcengiz.github.io/blog/2020/09/07/pca-tidyverse-style/</link><pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2020/09/07/pca-tidyverse-style/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Doing a PCA in R is easy: Just run the function &lt;code>prcomp()&lt;/code> on your matrix of scaled numeric predictor variables. There’s just one problem, however. The result is an object of class &lt;code>prcomp&lt;/code> that doesn’t fit nicely into the tidyverse framework, e.g. for visualization. While it’s reasonably easy to extract the relevant info with &lt;a href="https://wilkelab.org/classes/SDS348/2020_spring/worksheets/class9_solutions.html">some base-R manipulations,&lt;/a> I’ve never been happy with this approach. But now, I’ve realized that all the necessary functions to do this tidyverse-style are available in the broom package.&lt;/p>
&lt;p>For our PCA example, we’ll need the packages tidyverse and broom. Note that as of this writing, we need the current development version of broom &lt;a href="https://github.com/tidymodels/broom/issues/923">because of a bug&lt;/a> in &lt;code>tidy.prcomp()&lt;/code>. We’ll also use the cowplot package for plot themes.&lt;/p>
&lt;pre class="r">&lt;code>library(tidyverse)&lt;/code>&lt;/pre>
&lt;pre>&lt;code># ── Attaching packages ────────────────────────────────── tidyverse 1.3.0 ──&lt;/code>&lt;/pre>
&lt;pre>&lt;code># ✓ ggplot2 3.3.2 ✓ purrr 0.3.4
# ✓ tibble 3.0.3 ✓ dplyr 1.0.2
# ✓ tidyr 1.1.2 ✓ stringr 1.4.0
# ✓ readr 1.3.1 ✓ forcats 0.5.0&lt;/code>&lt;/pre>
&lt;pre>&lt;code># ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ──
# x dplyr::filter() masks stats::filter()
# x dplyr::lag() masks stats::lag()&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>library(broom) # devtools::install_github(&amp;quot;tidymodels/broom&amp;quot;)
library(cowplot)&lt;/code>&lt;/pre>
&lt;p>We’ll be analyzing the &lt;code>biopsy&lt;/code> dataset, which comes originally from the MASS package. It is a breast cancer dataset from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. He assessed biopsies of breast tumors for 699 patients; each of nine attributes was scored on a scale of 1 to 10. The true outcome (benign/malignant) is also known.&lt;/p>
&lt;pre class="r">&lt;code>biopsy &amp;lt;- read_csv(&amp;quot;https://wilkelab.org/classes/SDS348/data_sets/biopsy.csv&amp;quot;)&lt;/code>&lt;/pre>
&lt;pre>&lt;code># Parsed with column specification:
# cols(
# clump_thickness = col_double(),
# uniform_cell_size = col_double(),
# uniform_cell_shape = col_double(),
# marg_adhesion = col_double(),
# epithelial_cell_size = col_double(),
# bare_nuclei = col_double(),
# bland_chromatin = col_double(),
# normal_nucleoli = col_double(),
# mitoses = col_double(),
# outcome = col_character()
# )&lt;/code>&lt;/pre>
&lt;p>In general, when performing PCA, we’ll want to do (at least) three things:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Look at the data in PC coordinates.&lt;/li>
&lt;li>Look at the rotation matrix.&lt;/li>
&lt;li>Look at the variance explained by each PC.&lt;/li>
&lt;/ol>
&lt;p>Let’s do these three things in turn.&lt;/p>
&lt;div id="look-at-the-data-in-pc-coordinates" class="section level2">
&lt;h2>Look at the data in PC coordinates&lt;/h2>
&lt;p>We start by running the PCA and storing the result in a variable &lt;code>pca_fit&lt;/code>. There are two issues to consider here. First, the &lt;code>prcomp()&lt;/code> function can only deal with numeric columns, so we need to remove all non-numeric columns from the data. This is straightforward using the &lt;code>where(is.numeric)&lt;/code> tidyselect construct. Second, we normally want to scale the data values to unit variance before PCA. We do so by using the argument &lt;code>scale = TRUE&lt;/code> in &lt;code>prcomp()&lt;/code>.&lt;/p>
&lt;pre class="r">&lt;code>pca_fit &amp;lt;- biopsy %&amp;gt;%
select(where(is.numeric)) %&amp;gt;% # retain only numeric columns
prcomp(scale = TRUE) # do PCA on scaled data&lt;/code>&lt;/pre>
&lt;p>As an alternative to &lt;code>scale = TRUE&lt;/code>, we could also have scaled the data by explicitly invoking the &lt;code>scale()&lt;/code> function.&lt;/p>
&lt;pre class="r">&lt;code>pca_fit &amp;lt;- biopsy %&amp;gt;%
select(where(is.numeric)) %&amp;gt;% # retain only numeric columns
scale() %&amp;gt;% # scale data
prcomp() # do PCA&lt;/code>&lt;/pre>
&lt;p>Now, we want to plot the data in PC coordinates. In general, this means combining the PC coordinates with the original dataset, so we can color points by categorical variables present in the original data but removed for the PCA. We do this with the &lt;code>augment()&lt;/code> function from broom, which takes as arguments the fitted model and the original data. The columns containing the fitted coordinates are called &lt;code>.fittedPC1&lt;/code>, &lt;code>.fittedPC2&lt;/code>, etc.&lt;/p>
&lt;pre class="r">&lt;code>pca_fit %&amp;gt;%
augment(biopsy) %&amp;gt;% # add original dataset back in
ggplot(aes(.fittedPC1, .fittedPC2, color = outcome)) +
geom_point(size = 1.5) +
scale_color_manual(
values = c(malignant = &amp;quot;#D55E00&amp;quot;, benign = &amp;quot;#0072B2&amp;quot;)
) +
theme_half_open(12) + background_grid()&lt;/code>&lt;/pre>
&lt;p>&lt;img src="figure-html/pc-scatter-plot-1.png" />&lt;!-- -->&lt;/p>
&lt;/div>
&lt;div id="look-at-the-data-in-pc-coordinates-1" class="section level2">
&lt;h2>Look at the data in PC coordinates&lt;/h2>
&lt;p>Next, we plot the rotation matrix. The rotation matrix is stored as &lt;code>pca_fit$rotation&lt;/code>, but here we’ll extract it using the &lt;code>tidy()&lt;/code> function from broom. When applied to &lt;code>prcomp&lt;/code> objects, the &lt;code>tidy()&lt;/code> function takes an additional argument &lt;code>matrix&lt;/code>, which we set to &lt;code>matrix = "rotation"&lt;/code> to extract the rotation matrix.&lt;/p>
&lt;pre class="r">&lt;code># extract rotation matrix
pca_fit %&amp;gt;%
tidy(matrix = &amp;quot;rotation&amp;quot;)&lt;/code>&lt;/pre>
&lt;pre>&lt;code># # A tibble: 81 x 3
# column PC value
# &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
# 1 clump_thickness 1 -0.302
# 2 clump_thickness 2 -0.141
# 3 clump_thickness 3 0.866
# 4 clump_thickness 4 -0.108
# 5 clump_thickness 5 0.0803
# 6 clump_thickness 6 -0.243
# 7 clump_thickness 7 -0.00852
# 8 clump_thickness 8 0.248
# 9 clump_thickness 9 -0.00275
# 10 uniform_cell_size 1 -0.381
# # … with 71 more rows&lt;/code>&lt;/pre>
&lt;p>Now in the context of a plot:&lt;/p>
&lt;pre class="r">&lt;code># define arrow style for plotting
arrow_style &amp;lt;- arrow(
angle = 20, ends = &amp;quot;first&amp;quot;, type = &amp;quot;closed&amp;quot;, length = grid::unit(8, &amp;quot;pt&amp;quot;)
)
# plot rotation matrix
pca_fit %&amp;gt;%
tidy(matrix = &amp;quot;rotation&amp;quot;) %&amp;gt;%
pivot_wider(names_from = &amp;quot;PC&amp;quot;, names_prefix = &amp;quot;PC&amp;quot;, values_from = &amp;quot;value&amp;quot;) %&amp;gt;%
ggplot(aes(PC1, PC2)) +
geom_segment(xend = 0, yend = 0, arrow = arrow_style) +
geom_text(
aes(label = column),
hjust = 1, nudge_x = -0.02,
color = &amp;quot;#904C2F&amp;quot;
) +
xlim(-1.25, .5) + ylim(-.5, 1) +
coord_fixed() + # fix aspect ratio to 1:1
theme_minimal_grid(12)&lt;/code>&lt;/pre>
&lt;p>&lt;img src="figure-html/pc-rotation-plot-1.png" />&lt;!-- -->&lt;/p>
&lt;/div>
&lt;div id="look-at-the-variance-explained-by-each-pc" class="section level2">
&lt;h2>Look at the variance explained by each PC&lt;/h2>
&lt;p>Finally, we’ll plot the variance explained by each PC. We can again extract this information using the &lt;code>tidy()&lt;/code> function from broom, now by setting the &lt;code>matrix&lt;/code> argument to &lt;code>matrix = "eigenvalues"&lt;/code>.&lt;/p>
&lt;pre class="r">&lt;code>pca_fit %&amp;gt;%
tidy(matrix = &amp;quot;eigenvalues&amp;quot;)&lt;/code>&lt;/pre>
&lt;pre>&lt;code># # A tibble: 9 x 4
# PC std.dev percent cumulative
# &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
# 1 1 2.43 0.656 0.656
# 2 2 0.881 0.0862 0.742
# 3 3 0.734 0.0599 0.802
# 4 4 0.678 0.0511 0.853
# 5 5 0.617 0.0422 0.895
# 6 6 0.549 0.0335 0.928
# 7 7 0.543 0.0327 0.961
# 8 8 0.511 0.0290 0.990
# 9 9 0.297 0.00982 1&lt;/code>&lt;/pre>
&lt;p>Now in the context of a plot.&lt;/p>
&lt;pre class="r">&lt;code>pca_fit %&amp;gt;%
tidy(matrix = &amp;quot;eigenvalues&amp;quot;) %&amp;gt;%
ggplot(aes(PC, percent)) +
geom_col(fill = &amp;quot;#56B4E9&amp;quot;, alpha = 0.8) +
scale_x_continuous(breaks = 1:9) +
scale_y_continuous(
labels = scales::percent_format(),
expand = expansion(mult = c(0, 0.01))
) +
theme_minimal_hgrid(12)&lt;/code>&lt;/pre>
&lt;p>&lt;img src="figure-html/pc-eigenvalues-1.png" />&lt;!-- -->&lt;/p>
&lt;p>The first component captures 65% of the variation in the data and, as we can see from the first plot in this post, nicely separates the benign samples from the malignant samples.&lt;/p>
&lt;/div></description></item><item><title>Goodbye joyplots</title><link>https://sevimcengiz.github.io/blog/2017/09/15/goodbye-joyplots/</link><pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2017/09/15/goodbye-joyplots/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Anybody who has been paying any attention to the data visualization scene knows that the summer of 2017 was the summer of joyplots. This type of visualization turned viral, probably not in small part fueled by the R package &lt;a href="https://CRAN.R-project.org/package=ggjoy">ggjoy&lt;/a> that I wrote in July. However, I think it’s time to retire both the name “joyplot” and the ggjoy package, and as of today the ggjoy package is officially deprecated. A replacement package &lt;a href="https://CRAN.R-project.org/package=ggridges">ggridges&lt;/a> is in place and provides essentially the same functionality.&lt;/p>
&lt;p>The term “joyplot” was coined by Jenny Bryan in a tweet on April 24, 2017:&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="en">
&lt;p lang="en" dir="ltr">
I hereby propose that we call these “joy plots” &lt;a href="https://twitter.com/hashtag/rstats?src=hash">#rstats&lt;/a> &lt;a href="https://t.co/uuLGpQLAwY">https://t.co/uuLGpQLAwY&lt;/a>
&lt;/p>
— Jenny Bryan (&lt;span class="citation">@JennyBryan&lt;/span>) &lt;a href="https://twitter.com/JennyBryan/status/856674638981550080">April 25, 2017&lt;/a>
&lt;/blockquote>
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>The idea was to honor the band Joy Division, whose 1979 album &lt;a href="https://en.wikipedia.org/wiki/Unknown_Pleasures">&lt;em>Unknown Pleasures&lt;/em>&lt;/a> features on its cover a visualization of radio waves as staggered lines, creating a 3D-like effect. This seemed like a good idea and the name caught on like wildfire.&lt;/p>
&lt;p>Unfortunately, when the name “joyplot” took off, nobody in the datascience community was aware of the origin of the name “Joy Division”. As described in the book &lt;a href="https://en.wikipedia.org/wiki/House_of_Dolls">&lt;em>House of Dolls,&lt;/em>&lt;/a> joy divisions were groups of Jewish women in Nazi concentration camps kept for the sexual pleasure of soldiers. The band Joy Division took their name directly from this book and even quoted from the book in one of their early songs.&lt;/p>
&lt;p>Thus, as joyful as the name “joyplot” sounds to the uninformed, its history is rather dark, and we would do better using a different name. For this reason, I have decided to now call these plots “ridgeline plots”. Indeed, from day one, the ggjoy package contained a &lt;code>geom_ridgeline()&lt;/code>, so I’m just keeping in this tradition. The new ggridges package uses this naming convention throughout, and all functions have been renamed accordingly. For example, &lt;code>geom_joy()&lt;/code> is now called &lt;code>geom_density_ridges()&lt;/code>. A complete list of all name replacements is provided &lt;a href="https://github.com/clauswilke/ggjoy/blob/master/README.md">here&lt;/a>. Porting your code from ggjoy to ggridges should be as simple as a search-and-replace for all those functions in your code. If you run into any trouble, please let me know or open an &lt;a href="https://github.com/clauswilke/ggridges/issues">issue on github&lt;/a>.&lt;/p></description></item><item><title>Reading and combining many tidy data files in R</title><link>https://sevimcengiz.github.io/blog/2016/06/13/reading-and-combining-many-tidy-data-files-in-r/</link><pubDate>Mon, 13 Jun 2016 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2016/06/13/reading-and-combining-many-tidy-data-files-in-r/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>&lt;strong>Update June 16, 2019:&lt;/strong> &lt;em>This post is now three years old, and some of the advice given is now outdated. Most importantly, it is much better to use &lt;code>map_dfr()&lt;/code> than &lt;code>map(...) %&amp;gt;% reduce(rbind)&lt;/code>.&lt;/em>&lt;/p>
&lt;p>Everybody who is familiar with the R libraries for processing of tidy data, such as &lt;code>dplyr&lt;/code> and &lt;code>ggplot&lt;/code>, knows how powerful they are and how much one can get done with just a few lines of R code. However, similarly, everybody who has used them has probably spent more time bringing data into the appropriate tidy format than writing analysis and/or plotting code. In particular, one scenario that arises all the time is that even if data files are in tidy format, the entire dataset may be spread out over many individual files, and loading them all in and combining them into one large table can be cumbersome. Here, I want to demonstrate some neat tricks, using the relatively new package &lt;code>purrr&lt;/code> and some recent additions to the package &lt;code>tidyr&lt;/code>, that make loading and combining many data files a piece of cake.&lt;/p>
&lt;p>The code shown here depends on the following R packages:&lt;/p>
&lt;pre class="r">&lt;code>require(readr) # for read_csv()
require(dplyr) # for mutate()
require(tidyr) # for unnest()
require(purrr) # for map(), reduce()&lt;/code>&lt;/pre>
&lt;div id="reading-in-all-files-matching-a-given-name" class="section level2">
&lt;h2>Reading in all files matching a given name&lt;/h2>
&lt;p>As an example, we will consider a scenario where we have population census data for various cities, stored in individual csv files for each city. The data I’m using here comes from &lt;a href="http://factfinder.census.gov/" class="uri">http://factfinder.census.gov/&lt;/a>.&lt;/p>
&lt;p>The first scenario we will consider is one where we want to read all csv files in the current working directory. To achieve this goal, we first list all &lt;code>*.csv&lt;/code> files, using the function &lt;code>dir()&lt;/code>. We find that there are three, for the cities Houston, Los Angeles, and New York:&lt;/p>
&lt;pre class="r">&lt;code># find all file names ending in .csv
files &amp;lt;- dir(pattern = &amp;quot;*.csv&amp;quot;)
files&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;Houston_TX.csv&amp;quot; &amp;quot;Los Angeles_CA.csv&amp;quot; &amp;quot;New York_NY.csv&amp;quot;&lt;/code>&lt;/pre>
&lt;p>We can then read in those files and combine them into one data frame using the &lt;code>purrr&lt;/code> functions &lt;code>map()&lt;/code> and &lt;code>reduce()&lt;/code>:&lt;/p>
&lt;pre class="r">&lt;code>data &amp;lt;- files %&amp;gt;%
map(read_csv) %&amp;gt;% # read in all the files individually, using
# the function read_csv() from the readr package
reduce(rbind) # reduce with rbind into one dataframe
data&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [15 x 3]
##
## location year population
## (chr) (int) (int)
## 1 Houston, TX 2011 2142221
## 2 Houston, TX 2012 2177376
## 3 Houston, TX 2013 2216460
## 4 Houston, TX 2014 2256192
## 5 Houston, TX 2015 2296224
## 6 Los Angeles, CA 2011 3828604
## 7 Los Angeles, CA 2012 3864724
## 8 Los Angeles, CA 2013 3902005
## 9 Los Angeles, CA 2014 3936940
## 10 Los Angeles, CA 2015 3971883
## 11 New York, NY 2011 8287000
## 12 New York, NY 2012 8365069
## 13 New York, NY 2013 8436047
## 14 New York, NY 2014 8495194
## 15 New York, NY 2015 8550405&lt;/code>&lt;/pre>
&lt;p>Often, we want to read the data from a given directory rather than from the current working directory. The ability to define functions on-the-fly in &lt;code>purrr&lt;/code> makes this easy:&lt;/p>
&lt;pre class="r">&lt;code>data_path &amp;lt;- &amp;quot;city_data&amp;quot; # path to the data
files &amp;lt;- dir(data_path, pattern = &amp;quot;*.csv&amp;quot;) # get file names
data &amp;lt;- files %&amp;gt;%
# read in all the files, appending the path before the filename
map(~ read_csv(file.path(data_path, .))) %&amp;gt;%
reduce(rbind)
data&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [15 x 3]
##
## location year population
## (chr) (int) (int)
## 1 Houston, TX 2011 2142221
## 2 Houston, TX 2012 2177376
## 3 Houston, TX 2013 2216460
## 4 Houston, TX 2014 2256192
## 5 Houston, TX 2015 2296224
## 6 Los Angeles, CA 2011 3828604
## 7 Los Angeles, CA 2012 3864724
## 8 Los Angeles, CA 2013 3902005
## 9 Los Angeles, CA 2014 3936940
## 10 Los Angeles, CA 2015 3971883
## 11 New York, NY 2011 8287000
## 12 New York, NY 2012 8365069
## 13 New York, NY 2013 8436047
## 14 New York, NY 2014 8495194
## 15 New York, NY 2015 8550405&lt;/code>&lt;/pre>
&lt;p>Here, the expression &lt;code>~ read_csv(file.path(data_path, .))&lt;/code> is a shortcut for the anonymous function definition &lt;code>function(x) read_csv(file.path(data_path, x))&lt;/code>:&lt;/p>
&lt;pre class="r">&lt;code># this code does the exact same thing as the previous code
data &amp;lt;- files %&amp;gt;%
map(function(x) read_csv(file.path(data_path, x))) %&amp;gt;%
reduce(rbind)
data&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [15 x 3]
##
## location year population
## (chr) (int) (int)
## 1 Houston, TX 2011 2142221
## 2 Houston, TX 2012 2177376
## 3 Houston, TX 2013 2216460
## 4 Houston, TX 2014 2256192
## 5 Houston, TX 2015 2296224
## 6 Los Angeles, CA 2011 3828604
## 7 Los Angeles, CA 2012 3864724
## 8 Los Angeles, CA 2013 3902005
## 9 Los Angeles, CA 2014 3936940
## 10 Los Angeles, CA 2015 3971883
## 11 New York, NY 2011 8287000
## 12 New York, NY 2012 8365069
## 13 New York, NY 2013 8436047
## 14 New York, NY 2014 8495194
## 15 New York, NY 2015 8550405&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="keeping-auxilliary-information-about-the-files-read" class="section level2">
&lt;h2>Keeping auxilliary information about the files read&lt;/h2>
&lt;p>One limitation of the previous approach is that we don’t keep any auxilliary information we may want to, such as the filenames of the files read. To keep the filename alongside the data, we can read the data into a nested dataframe rather than a list, using the &lt;code>mutate()&lt;/code> function from &lt;code>dplyr&lt;/code>. This gives us the following result:&lt;/p>
&lt;pre class="r">&lt;code>data &amp;lt;- data_frame(filename = files) %&amp;gt;% # create a data frame
# holding the file names
mutate(file_contents = map(filename, # read files into
~ read_csv(file.path(data_path, .))) # a new data column
)
data&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [3 x 2]
##
## filename file_contents
## (chr) (chr)
## 1 Houston_TX.csv &amp;lt;tbl_df [5,3]&amp;gt;
## 2 Los Angeles_CA.csv &amp;lt;tbl_df [5,3]&amp;gt;
## 3 New York_NY.csv &amp;lt;tbl_df [5,3]&amp;gt;&lt;/code>&lt;/pre>
&lt;p>To turn this data frame into one useful for downstream analysis, we use the function &lt;code>unnest()&lt;/code> from &lt;code>tidyr&lt;/code>:&lt;/p>
&lt;pre class="r">&lt;code>unnest(data)&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [15 x 4]
##
## filename location year population
## (chr) (chr) (int) (int)
## 1 Houston_TX.csv Houston, TX 2011 2142221
## 2 Houston_TX.csv Houston, TX 2012 2177376
## 3 Houston_TX.csv Houston, TX 2013 2216460
## 4 Houston_TX.csv Houston, TX 2014 2256192
## 5 Houston_TX.csv Houston, TX 2015 2296224
## 6 Los Angeles_CA.csv Los Angeles, CA 2011 3828604
## 7 Los Angeles_CA.csv Los Angeles, CA 2012 3864724
## 8 Los Angeles_CA.csv Los Angeles, CA 2013 3902005
## 9 Los Angeles_CA.csv Los Angeles, CA 2014 3936940
## 10 Los Angeles_CA.csv Los Angeles, CA 2015 3971883
## 11 New York_NY.csv New York, NY 2011 8287000
## 12 New York_NY.csv New York, NY 2012 8365069
## 13 New York_NY.csv New York, NY 2013 8436047
## 14 New York_NY.csv New York, NY 2014 8495194
## 15 New York_NY.csv New York, NY 2015 8550405&lt;/code>&lt;/pre>
&lt;/div>
&lt;div id="creating-filenames-from-data" class="section level2">
&lt;h2>Creating filenames from data&lt;/h2>
&lt;p>In the previous examples, we have read in all the data files in a given directory. Often, however, we would rather read in specific files based on other data we have. For example, let’s assume we have the following data table:&lt;/p>
&lt;pre class="r">&lt;code>cities &amp;lt;- data_frame(city = c(&amp;quot;New York&amp;quot;, &amp;quot;Houston&amp;quot;),
state = c(&amp;quot;NY&amp;quot;, &amp;quot;TX&amp;quot;),
area = c(305, 599.6))
cities&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [2 x 3]
##
## city state area
## (chr) (chr) (dbl)
## 1 New York NY 305.0
## 2 Houston TX 599.6&lt;/code>&lt;/pre>
&lt;p>We want to use the city and state columns to create appropriate filenames and then load in the corresponding files. The code in its entirety looks as follows:&lt;/p>
&lt;pre class="r">&lt;code>data &amp;lt;- cities %&amp;gt;% # start with the cities table
# create filenames
mutate(filename = paste(city, &amp;quot;_&amp;quot;, state, &amp;quot;.csv&amp;quot;, sep=&amp;quot;&amp;quot;)) %&amp;gt;%
# read in data
mutate(file_contents = map(filename,
~ read_csv(file.path(data_path, .)))
) %&amp;gt;%
select(-filename) %&amp;gt;% # remove filenames, not needed anynmore
unnest() %&amp;gt;% # unnest
select(-location) # remove location column, not needed
# since we have city and state columns
data&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Source: local data frame [10 x 5]
##
## city state area year population
## (chr) (chr) (dbl) (int) (int)
## 1 New York NY 305.0 2011 8287000
## 2 New York NY 305.0 2012 8365069
## 3 New York NY 305.0 2013 8436047
## 4 New York NY 305.0 2014 8495194
## 5 New York NY 305.0 2015 8550405
## 6 Houston TX 599.6 2011 2142221
## 7 Houston TX 599.6 2012 2177376
## 8 Houston TX 599.6 2013 2216460
## 9 Houston TX 599.6 2014 2256192
## 10 Houston TX 599.6 2015 2296224&lt;/code>&lt;/pre>
&lt;p>I hope you have found these examples useful, and you will start loading files into nested data frames.&lt;/p>
&lt;/div></description></item><item><title>cowplot R package now available on CRAN</title><link>https://sevimcengiz.github.io/blog/2015/06/04/cowplot-r-package-now-available-on-cran/</link><pubDate>Thu, 04 Jun 2015 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2015/06/04/cowplot-r-package-now-available-on-cran/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>This week, I finally took the time to clean up the code for my cowplot R package and &lt;a href="http://cran.r-project.org/web/packages/cowplot/index.html">submit it to CRAN.&lt;/a> While the code had been up on &lt;a href="https://github.com/wilkelab/cowplot">github for a while,&lt;/a> and I had &lt;a href="https://sevimcengiz.github.io/blog/2014/10/7/to-grid-or-not-to-grid">blogged about it previously,&lt;/a> nobody had really taken notice as far as I can tell. However, this time, with an official release and better documentation, people seem to like it a lot. The response on Twitter was overwhelming.&lt;/p>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
&lt;a href="https://twitter.com/ClausWilke">&lt;span class="citation">@ClausWilke&lt;/span>&lt;/a> Thank you! Thank you!
&lt;/p>
— Shaun Jackman (&lt;span class="citation">@sjackman&lt;/span>) &lt;a href="https://twitter.com/sjackman/status/606215710549774336">June 3, 2015&lt;/a>
&lt;/blockquote>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
&lt;a href="https://twitter.com/ClausWilke">&lt;span class="citation">@ClausWilke&lt;/span>&lt;/a> &lt;a href="https://twitter.com/hadleywickham">&lt;span class="citation">@hadleywickham&lt;/span>&lt;/a> thank you, and goodbye illustrator, Photoshop, imagemagick, etc.
&lt;/p>
— Stephen Turner (&lt;span class="citation">@genetics_blog&lt;/span>) &lt;a href="https://twitter.com/genetics_blog/status/606221366736654337">June 3, 2015&lt;/a>
&lt;/blockquote>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
&lt;a href="https://twitter.com/ClausWilke">&lt;span class="citation">@ClausWilke&lt;/span>&lt;/a> &lt;a href="https://twitter.com/hadleywickham">&lt;span class="citation">@hadleywickham&lt;/span>&lt;/a> awesome !!! This makes making figures so easy !!
&lt;/p>
— sahil seth (&lt;span class="citation">@sethsa&lt;/span>) &lt;a href="https://twitter.com/sethsa/status/606282681907691520">June 4, 2015&lt;/a>
&lt;/blockquote>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
I love that the name cowplot is based on &lt;a href="https://twitter.com/ClausWilke">&lt;span class="citation">@ClausWilke&lt;/span>&lt;/a>'s initials. New goal: create a package named madplot some day. :)
&lt;/p>
— Meghan Duffy (&lt;span class="citation">@duffy_ma&lt;/span>) &lt;a href="https://twitter.com/duffy_ma/status/606450829604880386">June 4, 2015&lt;/a>
&lt;/blockquote>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
New R pkg makes it easy to custom arrange, label ggplots on a canvas &lt;a href="http://t.co/EQGjq9rOAE">http://t.co/EQGjq9rOAE&lt;/a> By &lt;a href="https://twitter.com/ClausWilke">&lt;span class="citation">@ClausWilke&lt;/span>&lt;/a> &lt;a href="https://twitter.com/hashtag/rstats?src=hash">#rstats&lt;/a> &lt;a href="http://t.co/Wi2kL3RnIw">pic.twitter.com/Wi2kL3RnIw&lt;/a>
&lt;/p>
— Sharon Machlis (&lt;span class="citation">@sharon000&lt;/span>) &lt;a href="https://twitter.com/sharon000/status/606476793558614016">June 4, 2015&lt;/a>
&lt;/blockquote>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
This looks awesome… I've been struggling with this issue a lot the last week or so. &lt;a href="https://t.co/8YxNdocqMd">https://t.co/8YxNdocqMd&lt;/a>
&lt;/p>
— Andrew Kniss (&lt;span class="citation">@WyoWeeds&lt;/span>) &lt;a href="https://twitter.com/WyoWeeds/status/606482691433766914">June 4, 2015&lt;/a>
&lt;/blockquote>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
(A big thank you to &lt;a href="https://twitter.com/ClausWilke">&lt;span class="citation">@ClausWilke&lt;/span>&lt;/a> for developing the cowplot package so I could make this 2-panel figure.) &lt;a href="https://twitter.com/hashtag/Rstats?src=hash">#Rstats&lt;/a> &lt;a href="https://twitter.com/hashtag/ggplot?src=hash">#ggplot&lt;/a>
&lt;/p>
— Andrew Kniss (&lt;span class="citation">@WyoWeeds&lt;/span>) &lt;a href="https://twitter.com/WyoWeeds/status/606572020004839424">June 4, 2015&lt;/a>
&lt;/blockquote>
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Teaching a new introductory class in computational biology and bioinformatics</title><link>https://sevimcengiz.github.io/blog/2015/02/04/teaching-a-new-introductory-class-in-computational-biology-and-bioinformatics/</link><pubDate>Wed, 04 Feb 2015 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2015/02/04/teaching-a-new-introductory-class-in-computational-biology-and-bioinformatics/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>This semester, I’m teaching a new introductory class in computational biology and bioinformatics. The class is primarily targeted at undergraduates, and it is split approximately 50:50 between R and python. The R component emphasizes effective data analysis and visualization, using packages such as ggplot2 and dplyr. The python component will introduce students to basic programming concepts, and it will also cover some typical bioinformatics applications.&lt;/p>
&lt;p>Developing a new class is a lot of work, so I’ll probably have much less time for posting here on my blog. However, on the flip side, the entire course content will be posted online, and you can &lt;a href="https://wilkelab.org/classes/SDS348_spring_2015.html">follow along here.&lt;/a> The core of each lecture is an in-class exercise worksheet, and I’m posting the worksheets and the solutions online. Many lectures also have a brief traditional lecture component with slides as well as additional reading materials. I’m developing the course as I go, so there will be new material posted twice a week throughout the spring.&lt;/p></description></item><item><title>To grid or not to grid</title><link>https://sevimcengiz.github.io/blog/2014/10/07/to-grid-or-not-to-grid/</link><pubDate>Tue, 07 Oct 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/10/07/to-grid-or-not-to-grid/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>I had a twitter discussion with ggplot2 author &lt;a href="http://had.co.nz/">Hadley Wickham&lt;/a> on whether or not to include a grid background in plots. He thinks the default should have a grid, I think the opposite. I believe we both agree that grids make sense for some plots and not for others, so this is just a question about defaults. On that issue, we remain in disagreement.&lt;/p>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
&lt;a href="https://twitter.com/ClausWilke">&lt;span class="citation">@ClausWilke&lt;/span>&lt;/a> I think removing the grid lines is a bad idea. It hampers your ability to make accurate comparisons
&lt;/p>
— Hadley Wickham (&lt;span class="citation">@hadleywickham&lt;/span>) &lt;a href="https://twitter.com/hadleywickham/status/519439694413565953">October 7, 2014&lt;/a>
&lt;/blockquote>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
&lt;a href="https://twitter.com/ClausWilke">&lt;span class="citation">@ClausWilke&lt;/span>&lt;/a> but see (e.g.) fig 3 of &lt;a href="http://t.co/m3A1XgPpCK">http://t.co/m3A1XgPpCK&lt;/a> - the grid is the right default, but may make sense to remove in some cases
&lt;/p>
— Hadley Wickham (&lt;span class="citation">@hadleywickham&lt;/span>) &lt;a href="https://twitter.com/hadleywickham/status/519483409911922688">October 7, 2014&lt;/a>
&lt;/blockquote>
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>One of my preferred methods of data visualization is to take two variables that measure the same quantity in different ways or different systems and then plot one versus the other. As an example, see &lt;a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3678335/figure/RSTB20120334F3/">this figure&lt;/a> from a paper my lab published last year. In this kind of a plot, a grid would be highly distracting. In general, I like to add guiding lines that highlight specific features of the data. For example, if the most important feature in the data is whether &lt;em>y&lt;/em> values fall above or below one, then placing a horizontal line at &lt;em>y&lt;/em> = 1 would be a good idea, and it would likely be more helpful than a generic grid covering the entire plot. However, I do agree that if one does a lot of &lt;a href="http://www.cookbook-r.com/Graphs/Facets_(ggplot2)">faceting,&lt;/a> a grid may be necessary. In fact, yesterday I played around with faceted plots without background grid, and I noticed that they had a tendency to fall apart and not look very convincing. Without grid, the eye has little to go by in these plots.&lt;/p>
&lt;p>So, if I can see the value of a background grid in some cases, why am I not convinced that it is the right default? When it comes to constructing plots, I fundamentally believe in an additive rather than a subtractive model. That is, start with a plot that is as empty as possible, and then add everything you need until you have a clear and informative graph. In a subtractive model, you would start with all sorts of additional visual elements of which you remove those you don’t need. While both approaches can lead to the same end result, it is my experience from ~15 years of supervising students, and from reviewing oodles of papers with poor-quality figures, that most people don’t remove visual noise from a graph unless explicitly prompted to do so. If ggplot2 places a gray background grid, then a gray background grid it is. Therefore, in my &lt;a href="https://github.com/wilkelab/cowplot">own personal plotting package,&lt;/a> whose intended purpose is internal use in my lab, the default is no background grid. I’d rather say on occasion “please add a background grid to this figure” than having to repeat over and over “please remove the background grid.”&lt;/p></description></item><item><title>R Markdown, the easiest and most elegant approach to writing about data analysis with R</title><link>https://sevimcengiz.github.io/blog/2014/10/04/r-markdown-the-easiest-and-most-elegant-approach-to-writing-about-data-analysis-with-r/</link><pubDate>Sat, 04 Oct 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/10/04/r-markdown-the-easiest-and-most-elegant-approach-to-writing-about-data-analysis-with-r/</guid><description>&lt;p>This weekend, I finally spent some time learning &lt;a href="https://rmarkdown.rstudio.com/">R Markdown&lt;/a>. I had been aware of its existence for a while, but I had never bothered to check it out. What a mistake. R Markdown rocks! It&amp;rsquo;s hands down the easiest and most elegant method to creating rich documents that contain data analysis, figures, mathematical formulas, and text. And it&amp;rsquo;s super easy to learn. I wager that anybody who has RStudio installed can create a useful document in 30 minutes or less. So if you use R, and you&amp;rsquo;ve never used R Markdown, give it a try.&lt;/p>
&lt;p>R Markdown provides a literate programming platform for the R language. Literate programming, &lt;a href="https://en.wikipedia.org/wiki/Literate_programming">invented by Donald Knuth,&lt;/a> allows users to write both a program and a document describing the program, at the same time. In the case of R, this means that you can write a document that contains R code, the output that is generated when the R code is run (including graphs), and prose describing the R code and its output. To give you an example, I started writing a tutorial for R&amp;rsquo;s ggplot2 library this weekend, and the original R Markdown file as well as the HTML output generated from that file are &lt;a href="https://github.com/wilkelab/ggplot2_cookbook/blob/master/README.md">available here.&lt;/a>&lt;/p>
&lt;p>What does the word &lt;em>Markdown&lt;/em> stand for? &lt;a href="https://en.wikipedia.org/wiki/Markdown">Markdown&lt;/a> is a minimalist approach to writing strutured documents. It consists of plain text with a few simple directives to mark sections, turn text bold or italics, or insert quotes. If you have ever edited a wikipedia article, you have used Markdown.&lt;/p>
&lt;p>To give you an example, this is Markdown text:&lt;/p>
&lt;pre>&lt;code>We can make text **bold**, *italics*, or `look like code.`
We can also insert links, [e.g. to wikipedia,](http://www.wikipedia.org/)
we can quote things:
&amp;gt; It is time to eat &amp;amp;#8212; Hungry John
or make lists:
1. Item 1
2. Item 2
3. Item 3
&lt;/code>&lt;/pre>
&lt;p>It will be rendered like this:&lt;/p>
&lt;hr>
&lt;p>We can make text &lt;strong>bold&lt;/strong>, &lt;em>italics&lt;/em>, or &lt;code>look like code.&lt;/code> We can also insert links, &lt;a href="http://www.wikipedia.org/">e.g. to wikipedia,&lt;/a> we can quote things:&lt;/p>
&lt;blockquote>
&lt;p>It is time to eat — Hungry John&lt;/p>
&lt;/blockquote>
&lt;p>or make lists:&lt;/p>
&lt;ol>
&lt;li>Item 1&lt;/li>
&lt;li>Item 2&lt;/li>
&lt;li>Item 3&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>R Markdown works the same, only that it adds the option to insert R code blocks. An R code block could look something like this:&lt;/p>
&lt;pre>&lt;code>```{r}
# place R code here, e.g. to make a plot:
library(ggplot2)
x &amp;lt;- 1:10; y &amp;lt;- x^2
qplot(x, y)
```
&lt;/code>&lt;/pre>
&lt;p>When you convert the R Markdown file to HTML, the R code gets executed, the R output captured and inserted into the document, and you&amp;rsquo;ve got everything nicely together, with very little work.&lt;/p>
&lt;p>To create an R Markdown document in RStudio, all you have to do is go to &lt;code>File&lt;/code>, &lt;code>New File&lt;/code>, and then select &lt;code>R Markdown&lt;/code>. Accept the default settings, and R Studio will generate a new R Markdown file with a few lines of example content. To convert the file into HTML, simply click on the &amp;ldquo;Knit HTML&amp;rdquo; button. If you have previously stored your R Markdown file somewhere on your harddisk (with suffix &lt;code>.Rmd&lt;/code>), RStudio will automatically save the generated HTML file in the same location, with the same name and suffix &lt;code>.html&lt;/code>. The HTML file is self-contained, including all images, so it&amp;rsquo;s easy to publish it on a web page or share it with people. RStudio also provides you with the option to publish the document online on the &lt;a href="http://rpubs.com/">RPubs&lt;/a> website. Just click on the &amp;ldquo;Publish&amp;rdquo; button in the HTML view.&lt;/p>
&lt;p>To learn more about R Markdown, go to: &lt;a href="https://rmarkdown.rstudio.com">https://rmarkdown.rstudio.com&lt;/a>&lt;/p></description></item><item><title>A grammar of data manipulation</title><link>https://sevimcengiz.github.io/blog/2014/09/17/a-grammar-of-data-manipulation/</link><pubDate>Wed, 17 Sep 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/09/17/a-grammar-of-data-manipulation/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>It seems that Hadley Wickham, the author of the spectacular &lt;a href="http://ggplot2.org/">ggplot2&lt;/a> library for R, is not content with revolutionizing the world of computational data analysis just once. He keeps doing it. This spring, he released the &lt;a href="http://cran.r-project.org/web/packages/dplyr/index.html">dplyr&lt;/a> package, a package that proposes a grammar of data manipulation. I predict that dplyr will become as important for large-scale data analysis and manipulation as ggplot2 has become for visualization. If you like ggplot2, you will love dplyr.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>&lt;/p>
&lt;p>dplyr is the next iteration of the popular plyr package, only that it is 100 times faster and way more intuitive. It provides a clean interface to work with data sets that are “tidy.” (See also my previous two blog posts on tidy data &lt;a href="https://sevimcengiz.github.io/blog/2014/7/20/keep-your-data-tidy">here&lt;/a> and &lt;a href="https://sevimcengiz.github.io/blog/2014/7/21/keep-your-data-tidy-part-ii">here.&lt;/a>) Let me whet your appetite by showing you a simple analysis I did the other day.&lt;/p>
&lt;p>I wanted to find out how much evolutionary divergence there is between human genes and the corresponding yeast (&lt;em>S. cerevisiae&lt;/em>) orthologs. Specifically, I was interested in the range of sequence identities among genes, i.e., what are the most conserved genes, the most diverged genes, what is the mean divergence, and so on. The analysis has an additional twist in that there are different types of ortholog pairs. There are one-to-one orthologs, where the human gene has exactly one counter-part in the yeast genome, there are one-to-many orthologs, where the gene in one organism has multiple counter-parts in the other organism, and there are many-to-many orthologs, where there are multiple genes in both organisms that are orthologous to each other. Thus, I wanted to carry out my analysis by orthology type.&lt;/p>
&lt;p>I went to &lt;a href="http://www.ensembl.org/index.html">ensembl&lt;/a> and downloaded all human-to-yeast orthologs with their respective sequence identities. The resulting csv file is available &lt;a href="https://dl.dropboxusercontent.com/u/97817736/human_yeast_divergence.csv">here.&lt;/a> We can download this file directly into R, using the RCurl package. We’ll also load the dplyr package, since we’ll need it later:&lt;/p>
&lt;pre class="r">&lt;code>library(RCurl)
library(dplyr)
url &amp;lt;- &amp;quot;https://dl.dropboxusercontent.com/u/97817736/human_yeast_divergence.csv&amp;quot;
data &amp;lt;- read.csv(textConnection(getURL(url)))&lt;/code>&lt;/pre>
&lt;p>Let’s take a look at the data. There are five columns, the gene id for the human gene, the gene id for the
corresponding yeast ortholog, the homology type (one-to-one, one-to-many, many-to-many), the percent identity with respect to both the human (&lt;code>perc.ident.human&lt;/code>) and the yeast (&lt;code>perc.ident.yeast&lt;/code>) gene, and a confidence score that tells us how confident we are that the two genes are actually orthologous.&lt;/p>
&lt;pre class="r">&lt;code>head(data)&lt;/code>&lt;/pre>
&lt;pre>&lt;code> human.gene.ID yeast.gene.ID homology.type perc.ident.human
1 ENSG00000100077 YKL126W ortholog_many2many 19
2 ENSG00000100077 YHR205W ortholog_many2many 18
3 ENSG00000100077 YMR104C ortholog_many2many 18
4 ENSG00000196139 YHR104W ortholog_one2many 33
5 ENSG00000173213 YFL037W ortholog_one2many 71
6 ENSG00000154930 YLR153C ortholog_many2many 43
perc.identity.yeast confidence
1 19 1
2 15 1
3 18 1
4 33 1
5 69 1
6 44 1 &lt;/code>&lt;/pre>
&lt;p>Now we’re ready for some dplyr magic. Let’s assume we want to analyze the data by homology type, and we want to find the minimum, mean, median, and maximum sequence identity for each homology type, as well as the standard deviation of the identity distribution. All this can be achieved with the following code:&lt;/p>
&lt;pre class="r">&lt;code>data %&amp;gt;% group_by(homology.type) %&amp;gt;%
summarize(
min=min(perc.ident.human),
mean=mean(perc.ident.human),
std.dev=sd(perc.ident.human),
max=max(perc.ident.human))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>Source: local data frame [3 x 5]
homology.type min mean std.dev max
1 ortholog_many2many 1 26.36965 16.37955 92
2 ortholog_one2many 0 27.46243 15.18146 86
3 ortholog_one2one 1 33.04183 12.89296 80&lt;/code>&lt;/pre>
&lt;p>The function &lt;code>group_by()&lt;/code> states that we want to carry out the analysis separately for each homology type, and the function &lt;code>summarize()&lt;/code> calculates the summary statistics (min, mean, etc.) for each group. The operator &lt;code>%&amp;gt;%&lt;/code> is a chaining operator, like a pipe in the UNIX command line. It takes the output from the previous operation and feeds it into the subsequence operation.&lt;/p>
&lt;p>As you can see, dplyr syntax focuses entirely on the logical flow of the data analysis. You don’t ever have to worry about bookkeeping, looping over cases, or details of the data storage.&lt;/p>
&lt;p>Let’s do another example. Let’s say we’re only interested in one-to-one orthologs, and we want to find the top 10 least diverged yeast genes and list them in descending order. The following lines achieve this:&lt;/p>
&lt;pre class="r">&lt;code>data %&amp;gt;% filter(homology.type==&amp;#39;ortholog_one2one&amp;#39;) %&amp;gt;%
select(yeast.gene.ID, human.gene.ID, perc.ident.human) %&amp;gt;%
top_n(10) %&amp;gt;%
arrange(desc(perc.ident.human))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>Selecting by perc.ident.human
yeast.gene.ID human.gene.ID perc.ident.human
1 YLR167W ENSG00000143947 80
2 YDR064W ENSG00000110700 77
3 YKL145W ENSG00000161057 75
4 YOR210W ENSG00000177700 75
5 YGL048C ENSG00000087191 74
6 YPL086C ENSG00000134014 74
7 YPR016C ENSG00000242372 72
8 YJR121W ENSG00000110955 72
9 YDL007W ENSG00000100764 71
10 YEL027W ENSG00000185883 70 &lt;/code>&lt;/pre>
&lt;p>The function &lt;code>filter()&lt;/code> selects all rows of the given homology type, the function &lt;code>select()&lt;/code> picks the specific columns we are interested in, the function &lt;code>top_n()&lt;/code> selects the top &lt;em>n&lt;/em> values in the last data column (i.e., column &lt;code>perc.ident.human&lt;/code> in this example), and the function &lt;code>arrange()&lt;/code> sorts the data.&lt;/p>
&lt;p>If these examples have piqued your interest and you want to learn more, I recommend you start by reading the dplyr vignette, which you can find here: &lt;a href="http://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html">http://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html&lt;/a>&lt;br />
There is also an excellent introduction by Kevin Markham, with accompanying 40 minute video, available here: &lt;a href="http://rpubs.com/justmarkham/dplyr-tutorial">http://rpubs.com/justmarkham/dplyr-tutorial&lt;/a>&lt;/p>
&lt;p>And have I mentioned already that dplyr has a &lt;a href="http://cran.rstudio.com/web/packages/dplyr/vignettes/databases.html">database backend,&lt;/a> so you can now easily use all the statistical sophistication that R provides on arbitrarily large, remotely stored data sets.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>And if you aren’t familiar with &lt;a href="http://ggplot2.org/">ggplot2,&lt;/a> spend some time with it. It is fantastic, though it may feel alien initially. If you’re used to traditional visualization approaches, you may think in terms of drawing points and lines onto a canvas. ggplot2 requires you to approach visualization in a completely different way, in terms of mapping features of the data onto aesthetic features of the graph. Once you get this way of thinking, it becomes rather powerful.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Keep your data tidy, Part II</title><link>https://sevimcengiz.github.io/blog/2014/07/21/keep-your-data-tidy-part-ii/</link><pubDate>Mon, 21 Jul 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/07/21/keep-your-data-tidy-part-ii/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>My previous post on &lt;a href="https://sevimcengiz.github.io/blog/2014/7/20/keep-your-data-tidy">tidy data&lt;/a> didn’t at all touch on rule 3, “Each type of observational unit forms a table.” The example I gave had only one observational unit, the weekly temperature measurements. Frequently, however, we have data corresponding to multiple observational units. In this case, it is important that we store them in separate tables, and that we know how to combine these tables for useful analyses.&lt;/p>
&lt;p>First, we need to figure out when we’re dealing with multiple observational units and when we are not. An observational unit is the base unit on which measurements are done. Importantly, multiple measurements can be performed on the same unit. For example, if we’re studying a group of patients, and for each patient we measure height, weight, heart rate, and blood pressure, then the observational unit is the patient, and we are measuring four variables per observational unit. However, if we’re following the patients over time, and we make those four measurements once every month for a year, then the observational units are the patient-months, i.e., we have one observational unit per patient each month.&lt;/p>
&lt;p>In what follows, I’m assuming you have read the &lt;a href="https://sevimcengiz.github.io/blog/2014/7/20/keep-your-data-tidy">previous post&lt;/a> and you are familiar with the temperature example I made there. In this example, since we’re measuring temperature each week, we have one observational unit per city per week. If in addition to temperature we also measured humidity, then we’d have two measurements per observational unit but the number and type of observational units wouldn’t change. Now, however, assume that we’re also recording additional information about the cities that we’re studying, for example their altitude. The altitude will be the same every week. Therefore, altitude is a property of the city, not of the city-week that is the experimental unit for the temperature measurements. Thus, we now have two separate sets of experimental units, the city-weeks for which we measure temperature and the cities themselves for which we measure altitude.&lt;/p>
&lt;p>By rule 3 for tidy data, the altitude measurements do not belong into the table that contains temperature data, they belong into a separate table. For example, this table could look like this:&lt;/p>
&lt;pre>&lt;code>&amp;gt; alt.data
city altitude
1 A 2300
2 B 400
3 C 250&lt;/code>&lt;/pre>
&lt;p>Let’s assume that we want to know whether there is a relationship between the mean temperature for a city and the city’s altitude. How would we do this? First, we calculate the mean temperature, as before, and store it in a new data frame called &lt;code>mean.temp&lt;/code>:&lt;/p>
&lt;pre>&lt;code>&amp;gt; mean.temp &amp;lt;- ddply(temp.data, &amp;quot;city&amp;quot;, summarize, mean.temp=mean(temperature))
&amp;gt; mean.temp
city mean.temp
1 A 13.50
2 B 20.25
3 C 23.75&lt;/code>&lt;/pre>
&lt;p>Now, we need to merge the two data frames &lt;code>alt.data&lt;/code> and &lt;code>mean.temp&lt;/code>. This can be done with the function &lt;code>join()&lt;/code> from the plyr package, or alternatively with the function &lt;code>merge&lt;/code> from base R:&lt;/p>
&lt;pre>&lt;code>&amp;gt; city.data &amp;lt;- join(mean.temp, alt.data)
Joining by: city
&amp;gt; city.data
city mean.temp altitude
1 A 13.50 2300
2 B 20.25 400
3 C 23.75 250&lt;/code>&lt;/pre>
&lt;p>Now we can investigate the relationship between the two variables, for example by fitting a linear model:&lt;/p>
&lt;pre>&lt;code>&amp;gt; summary(lm(altitude ~ mean.temp, data = city.data))
Call:
lm(formula = altitude ~ mean.temp, data = city.data)
Residuals:
1 2 3
121.1 -354.8 233.6
Coefficients:
Estimate Std. Error t value Pr(&amp;gt;|t|)
(Intercept) 5027.01 1177.01 4.271 0.146
mean.temp -210.97 59.95 -3.519 0.176
Residual standard error: 441.7 on 1 degrees of freedom
Multiple R-squared: 0.9253, Adjusted R-squared: 0.8506
F-statistic: 12.38 on 1 and 1 DF, p-value: 0.1763&lt;/code>&lt;/pre>
&lt;p>Both the &lt;code>join&lt;/code> and the &lt;code>merge&lt;/code> function can handle much more complicated situations. For example, by default, both functions merge on all shared variables. Further, you can specify how you want to handle the situation when cases are missing. A “left” join keeps all rows from the first data frame and matches the corresponding ones from the second, a “right” join keeps all rows from the second data frame and matches the corresponding ones from the first, an “inner” join only keeps rows that exist in both data frames, and a “full” join keeps all observations that exist in either data frame. For more details, read the documentation of either function.&lt;/p></description></item><item><title>Keep your data tidy</title><link>https://sevimcengiz.github.io/blog/2014/07/20/keep-your-data-tidy/</link><pubDate>Sun, 20 Jul 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/07/20/keep-your-data-tidy/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>I came across this nice preprint by Hadley Wickham:&lt;/p>
&lt;blockquote>
&lt;p>Hadley Wickham (2014). &lt;a href="http://vita.had.co.nz/papers/tidy-data.pdf">Tidy data.&lt;/a> Submitted.&lt;/p>
&lt;/blockquote>
&lt;p>In this preprint, Wickham describes a way of organizing data he calls “tidy.” He then argues that tidy data and tidy tools (that both input and output tidy data) make data analysis much more efficient than any alternative approach can.&lt;/p>
&lt;p>When are data tidy? When they satisfy these three conditions:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Each variable forms a column.&lt;/li>
&lt;li>Each observation forms a row.&lt;/li>
&lt;li>Each type of observational unit forms a table.&lt;/li>
&lt;/ol>
&lt;p>Every other arrangement of data is called “messy.”&lt;/p>
&lt;p>Let’s look at an example. Assume you’re making temperature measurements once a week in three cities (city A, city B, and city C). Instinctively, most people would probably record the data like this:&lt;/p>
&lt;pre>&lt;code>week city_A city_B city_C
1 14 18 23
2 15 21 24
3 12 25 23
4 13 17 25
...&lt;/code>&lt;/pre>
&lt;p>Or maybe even like this:&lt;/p>
&lt;pre>&lt;code>week 1 2 3 4 ...
city_A 14 15 12 13
city_B 18 21 25 17
city_C 23 24 23 25&lt;/code>&lt;/pre>
&lt;p>While both options may look quite organized, neither corresponds to tidy data. In both cases, Wickham’s rules 1 and 2 are violated. For example, in the first case, the variable &lt;code>temperature&lt;/code> appears in three columns. Consequently, multiple observations appear in each row. In the second case, multiple variables appear in each column and multiple observations appear in each row.&lt;/p>
&lt;p>The tidy version of this data set would look like this:&lt;/p>
&lt;pre>&lt;code>week city temperature
1 A 14
1 B 18
1 C 23
2 A 15
2 B 21
2 C 24
...
&lt;/code>&lt;/pre>
&lt;p>I believe that most people have an instinctive dislike towards tidy data, because tidy data sets tend to have many rows and are difficult to read with the human eye. You can clearly see this in my made-up data set. The messy&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> versions allow us to quickly compare cities’ temperatures in different weeks as well as identify trends over time. The tidy version does not. It also requires many more rows.&lt;/p>
&lt;p>Nevertheless, for computational analysis, having tidy data makes things way more efficient, &lt;em>if you have the right set of tools.&lt;/em> These tools need to consistently input and output tidy data. Fortunately, these tools exist in R. (And Wickham has greatly enhanced R’s capability to keep analyses tidy by writing the packages reshape2, plyr, and ggplot2.) With the appropriate tools, even quite complicated analyses can be expressed in just a few lines of code. And most importantly, there is no need for explicit loops or other bookkeeping code. You just express the semantics of your analysis and the programming language does the rest.&lt;/p>
&lt;p>How does this work? In general, when we want to analyze data, we want to manipulate (specifically filter, transform, aggregate, and sort), model, and visualize. These steps require only a handful of generic tools, such as a generic data aggregation function. Let me show you a few examples, using the &lt;code>summarize()&lt;/code> and &lt;code>ddply()&lt;/code> functions from the plyr package. I assume we store the temperature data in tidy form in a data frame called &lt;code>temp.data&lt;/code>:&lt;/p>
&lt;pre>&lt;code>&amp;gt; temp.data
week city temperature
1 1 A 14
2 1 B 18
3 1 C 23
4 2 A 15
5 2 B 21
6 2 C 24
7 3 A 12
8 3 B 25
9 3 C 23
10 4 A 13
11 4 B 17
12 4 C 25&lt;/code>&lt;/pre>
&lt;p>Let’s calculate the mean temperature in each city:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ddply(temp.data, &amp;quot;city&amp;quot;, summarize, mean=mean(temperature))
city mean
1 A 13.50
2 B 20.25
3 C 23.75&lt;/code>&lt;/pre>
&lt;p>Or in each week:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ddply(temp.data, &amp;quot;week&amp;quot;, summarize, mean=mean(temperature))
week mean
1 1 18.33333
2 2 20.00000
3 3 20.00000
4 4 18.33333&lt;/code>&lt;/pre>
&lt;p>Or let’s find the city with the highest temperature each week:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ddply(temp.data, &amp;quot;week&amp;quot;, summarize, hottest.city=city[which.max(temperature)])
week hottest.city
1 1 C
2 2 C
3 3 B
4 4 C&lt;/code>&lt;/pre>
&lt;p>These are just a few simple examples. For more examples, read Wickham’s paper, and also read his paper on plyr.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a>&lt;/p>
&lt;p>Does any of this matter? I’m sure somebody will tell me: Yeah, but R is an ugly and archaic programming language, and my python code will analyze the data tables that Wickham calls “messy” just fine. My answer is that it only matters if you care about how much time you spend analyzing your data. Read through the example analysis Wickham provides in his Section 5 (“Case study”). This analysis uses less than 30 lines of code, including the code for visualization, to identify and plot causes of death with unusual temporal patterns throughout the day (see his Fig. 4). How many lines of code would you have written to do a comparable analysis? And how long would it have taken you to write this code and debug it?&lt;/p>
&lt;p>At some point, a quantitative advantage becomes a qualitative one. While one can do the exact same analyses with tidy and messy datasets/tools, the tidy approach will generally require much less code, and hence be faster to write, easier to debug, and easier to modify/maintain. In practice, this means that the person using the tidy approach will be able to analyze more data sets, try a larger number of different analysis approaches, and/or make fewer mistakes. Aggregated over the course of several months, this advantage can easily translate into some major new findings made only by the person using the tidy approach.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>Throughout this post, I’m following Wickham in using “messy” as a technical term, meaning “not tidy,” as in “not conforming to the definition of tidy data.”&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Hadley Wickham (2011) &lt;a href="http://vita.had.co.nz/papers/plyr.html">The split-apply-combine strategy for data analysis.&lt;/a> Journal of Statistical Software 40:1–29.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>