<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Statistics on Sevim Cengiz</title><link>https://sevimcengiz.github.io/tags/Statistics/</link><description>Recent content in Statistics on Sevim Cengiz</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 04 Oct 2014 00:00:00 +0000</lastBuildDate><atom:link href="https://sevimcengiz.github.io/tags/Statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>R Markdown, the easiest and most elegant approach to writing about data analysis with R</title><link>https://sevimcengiz.github.io/blog/2014/10/04/r-markdown-the-easiest-and-most-elegant-approach-to-writing-about-data-analysis-with-r/</link><pubDate>Sat, 04 Oct 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/10/04/r-markdown-the-easiest-and-most-elegant-approach-to-writing-about-data-analysis-with-r/</guid><description>&lt;p>This weekend, I finally spent some time learning &lt;a href="https://rmarkdown.rstudio.com/">R Markdown&lt;/a>. I had been aware of its existence for a while, but I had never bothered to check it out. What a mistake. R Markdown rocks! It&amp;rsquo;s hands down the easiest and most elegant method to creating rich documents that contain data analysis, figures, mathematical formulas, and text. And it&amp;rsquo;s super easy to learn. I wager that anybody who has RStudio installed can create a useful document in 30 minutes or less. So if you use R, and you&amp;rsquo;ve never used R Markdown, give it a try.&lt;/p>
&lt;p>R Markdown provides a literate programming platform for the R language. Literate programming, &lt;a href="https://en.wikipedia.org/wiki/Literate_programming">invented by Donald Knuth,&lt;/a> allows users to write both a program and a document describing the program, at the same time. In the case of R, this means that you can write a document that contains R code, the output that is generated when the R code is run (including graphs), and prose describing the R code and its output. To give you an example, I started writing a tutorial for R&amp;rsquo;s ggplot2 library this weekend, and the original R Markdown file as well as the HTML output generated from that file are &lt;a href="https://github.com/wilkelab/ggplot2_cookbook/blob/master/README.md">available here.&lt;/a>&lt;/p>
&lt;p>What does the word &lt;em>Markdown&lt;/em> stand for? &lt;a href="https://en.wikipedia.org/wiki/Markdown">Markdown&lt;/a> is a minimalist approach to writing strutured documents. It consists of plain text with a few simple directives to mark sections, turn text bold or italics, or insert quotes. If you have ever edited a wikipedia article, you have used Markdown.&lt;/p>
&lt;p>To give you an example, this is Markdown text:&lt;/p>
&lt;pre>&lt;code>We can make text **bold**, *italics*, or `look like code.`
We can also insert links, [e.g. to wikipedia,](http://www.wikipedia.org/)
we can quote things:
&amp;gt; It is time to eat &amp;amp;#8212; Hungry John
or make lists:
1. Item 1
2. Item 2
3. Item 3
&lt;/code>&lt;/pre>
&lt;p>It will be rendered like this:&lt;/p>
&lt;hr>
&lt;p>We can make text &lt;strong>bold&lt;/strong>, &lt;em>italics&lt;/em>, or &lt;code>look like code.&lt;/code> We can also insert links, &lt;a href="http://www.wikipedia.org/">e.g. to wikipedia,&lt;/a> we can quote things:&lt;/p>
&lt;blockquote>
&lt;p>It is time to eat — Hungry John&lt;/p>
&lt;/blockquote>
&lt;p>or make lists:&lt;/p>
&lt;ol>
&lt;li>Item 1&lt;/li>
&lt;li>Item 2&lt;/li>
&lt;li>Item 3&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>R Markdown works the same, only that it adds the option to insert R code blocks. An R code block could look something like this:&lt;/p>
&lt;pre>&lt;code>```{r}
# place R code here, e.g. to make a plot:
library(ggplot2)
x &amp;lt;- 1:10; y &amp;lt;- x^2
qplot(x, y)
```
&lt;/code>&lt;/pre>
&lt;p>When you convert the R Markdown file to HTML, the R code gets executed, the R output captured and inserted into the document, and you&amp;rsquo;ve got everything nicely together, with very little work.&lt;/p>
&lt;p>To create an R Markdown document in RStudio, all you have to do is go to &lt;code>File&lt;/code>, &lt;code>New File&lt;/code>, and then select &lt;code>R Markdown&lt;/code>. Accept the default settings, and R Studio will generate a new R Markdown file with a few lines of example content. To convert the file into HTML, simply click on the &amp;ldquo;Knit HTML&amp;rdquo; button. If you have previously stored your R Markdown file somewhere on your harddisk (with suffix &lt;code>.Rmd&lt;/code>), RStudio will automatically save the generated HTML file in the same location, with the same name and suffix &lt;code>.html&lt;/code>. The HTML file is self-contained, including all images, so it&amp;rsquo;s easy to publish it on a web page or share it with people. RStudio also provides you with the option to publish the document online on the &lt;a href="http://rpubs.com/">RPubs&lt;/a> website. Just click on the &amp;ldquo;Publish&amp;rdquo; button in the HTML view.&lt;/p>
&lt;p>To learn more about R Markdown, go to: &lt;a href="https://rmarkdown.rstudio.com">https://rmarkdown.rstudio.com&lt;/a>&lt;/p></description></item><item><title>Common errors in statistical analyses</title><link>https://sevimcengiz.github.io/blog/2013/08/18/common-errors-in-statistical-analyses/</link><pubDate>Sun, 18 Aug 2013 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2013/08/18/common-errors-in-statistical-analyses/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>This is a post I originally wrote for my lab webpage. I’m reproducing it here (with minor edits) as an exercise in getting to know this blogging platform.&lt;/p>
&lt;div id="confusing-significance-and-effect-size" class="section level2">
&lt;h2>Confusing significance and effect size&lt;/h2>
&lt;p>&lt;em>Statistical significance&lt;/em> (a low &lt;em>P&lt;/em> value) measures how certain we are that a given effect exists.&lt;/p>
&lt;p>&lt;em>Effect size&lt;/em> measures the magnitude of an effect. What exactly effect size is depends on the analysis, examples are a correlation coefficient, the difference in means for a t test, or the odds ratio for a contingency-table analysis.&lt;/p>
&lt;p>Many results we encounter in the real world are highly significant but of low magnitude. For example, if you knew with near certainty that a particular dietary supplement extended your life span, on average, by 2 weeks, would you care? Probably not. Even though the finding is highly significant (near certain), the magnitude of the effect is so low that it basically doesn’t matter. Yet it is common in scientific studies, and in press reports about them, to only emphasize the significance of a finding but not the magnitude. Sometimes authors don’t even bother to report effect sizes at all, they only report &lt;em>P&lt;/em> values and point out how significant their results are. This is bad science. The &lt;em>P&lt;/em> value is primarily a measure of the data set size. The larger the data set, the lower the &lt;em>P&lt;/em> value, all else equal. To be important, an effect has to have a large magnitude; just being highly significant is not enough.&lt;/p>
&lt;/div>
&lt;div id="correlation-is-not-causation" class="section level2">
&lt;h2>Correlation is not causation&lt;/h2>
&lt;p>This issue is pretty well known, yet people fall into this trap over and over again. Just because one quantity shows a statistical association (correlation) with another variable doesn’t mean that one of the two variables causes the other variable. This problem is more common in press reporting about scientific studies than in the studies themselves. For example, a study might report an association between cell-phone use and cancer. In the study, the authors might be careful to point out that they don’t know why increased cell-phone use correlates with cancer in their study population, and that the underlying cause might be unrelated to cell-phone use (e.g., for some reason exposure to a carcinogen correlates with cell-phone use in the study population). Yet, inevitably the press release about this study will read “cell-phone use causes cancer.”&lt;/p>
&lt;p>In general, to reliably assign cause and effect, one needs to carry out an &lt;em>experimental study&lt;/em>. In an experimental study, a population is randomly subdivided into treatment and control groups, and the treatment group is subjected to a well-defined experimental manipulation. For example, people are divided into two groups at random, one group is made to use a cell phone for 2 hours each day, the other group is forbidden from using a cell phone ever. After 5 years, count which group developed more cancer. By contrast, studies that only show association but not causation are usually &lt;em>observational studies&lt;/em>. In such studies, we simply observe what variables are associated with each other in a sample.&lt;/p>
&lt;/div>
&lt;div id="focusing-on-tenth-order-effects-and-ignoring-first-order-effects" class="section level2">
&lt;h2>Focusing on tenth-order effects and ignoring first-order effects&lt;/h2>
&lt;p>This issue is not so much an issue of poor statistics but rather of poor placement of emphasis. It is very well explained by Peter Attia &lt;a href="http://eatingacademy.com/nutrition/irisin-the-magic-exercise-hormone">here&lt;/a>, so I’ll not elaborate on it any further here.&lt;/p>
&lt;/div>
&lt;div id="aggregation-by-quantiles-erroneously-amplifies-trend" class="section level2">
&lt;h2>Aggregation by quantiles erroneously amplifies trend&lt;/h2>
&lt;p>In many situations, one would like to know how one quantitative variable relates to another. For example, we might be studying a certain bird species and ask whether the amount of a certain berry that males of that species eat has an effect on the mating success of those males. The canonical (and correct) way to study such questions is via correlation analyses.&lt;/p>
&lt;p>However, it is surprisingly common to see analyses where instead one of the variables is aggregated into quantiles (groups of equal size) and the second variable is presented as an average of the quantiles of the first variable. In the above example, we might classify birds into four groups (quartiles) by their berry consumption (lowest 25%, second lowest 25%, and so on) and then plot the mean mating success within each group as a function of the quartile of berry consumption. Such an analysis is misleading, because it erroneously amplifies any relationship that may exist between the two variables.&lt;/p>
&lt;p>Let’s illustrate this issue with some simulated data, using R. First we generate two variables &lt;em>x&lt;/em> and &lt;em>y&lt;/em>, weakly correlated:&lt;/p>
&lt;pre>&lt;code>n &amp;lt;- 10000 # sample size
x &amp;lt;- rnorm(n) # generate first set of normal variates
y &amp;lt;- 0.1*x + 0.9*rnorm(n) # generate second set, weakly correlated with first
cor.test( x, y )&lt;/code>&lt;/pre>
&lt;p>This is the output from the &lt;code>cor.test()&lt;/code> function:&lt;/p>
&lt;pre>&lt;code># Pearson&amp;#39;s product-moment correlation
#
# data: x and y
# t = 10.485, df = 9998, p-value &amp;lt; 2.2e-16
# alternative hypothesis: true correlation is not equal to 0
# 95 percent confidence interval:
# 0.084862 0.123636
# sample estimates:
# cor
# 0.1042886 &lt;/code>&lt;/pre>
&lt;p>The correlation is highly significant (&lt;em>P&lt;/em> &amp;lt; 2.2e-16) but weak (&lt;em>r&lt;/em> = 0.10). The variable &lt;em>x&lt;/em> explains only 1% (that is the square of the correlation coefficient, &lt;em>r&lt;/em>^2) of the variation in &lt;em>y&lt;/em>. In terms of the birds example, this could mean that while berry consumption is indeed related to mating success, the relationship is so weak as to be virtually meaningless. (Knowing how many berries a given male bird ate tells me pretty much nothing about his specific mating success.)&lt;/p>
&lt;p>Figure &lt;a href="#fig:figure1">1&lt;/a> shows the relationship between &lt;em>x&lt;/em> and &lt;em>y&lt;/em>. As indicated by the correlation analysis, knowing &lt;em>x&lt;/em> doesn’t really tell us anything about &lt;em>y&lt;/em>.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure1">&lt;/span>
&lt;img src="Fig1.png" alt="Relationship between x and y in our made-up example dataset." width="75%" />
&lt;p class="caption">
Figure 1: Relationship between &lt;em>x&lt;/em> and &lt;em>y&lt;/em> in our made-up example dataset.
&lt;/p>
&lt;/div>
&lt;p>Now let’s aggregate the data into quantiles of &lt;em>x&lt;/em> and plot the mean +/- the standard error of &lt;em>y&lt;/em> within each quantile of &lt;em>x&lt;/em>:&lt;/p>
&lt;pre>&lt;code># calculate to which quantile each x belongs
qn &amp;lt;- 10 # number of quantiles
q &amp;lt;- quantile(x, probs = seq(0, 1, 1/qn))
q[qn] &amp;lt;- q[qn] + 1 # make sure the last quantile is larger than max(x)
quant.x &amp;lt;- tapply(x, 1:n, (function(x) sum(x&amp;gt;=q)))
# calculate means and SEs of y per quantile
library( Hmisc ) # for errbar plot
mean.quant &amp;lt;- tapply(y, quant.x, mean)
SE.quant &amp;lt;- tapply(y, quant.x, (function(x) sd(x)/sqrt(length(x))))
errbar(1:qn, mean.quant, mean.quant+SE.quant, mean.quant-SE.quant, xlab=&amp;#39;quantiles(x)&amp;#39;, ylab=&amp;#39;mean(y) for quantile&amp;#39;)
&lt;/code>&lt;/pre>
&lt;p>In this example, we chose 10 quantiles. The resulting graph is shown in Figure &lt;a href="#fig:figure2">2&lt;/a>.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure2">&lt;/span>
&lt;img src="Fig2.png" alt="Mean y (+/- standard error) as a function of quantiles of x." width="75%" />
&lt;p class="caption">
Figure 2: Mean &lt;em>y&lt;/em> (+/- standard error) as a function of quantiles of &lt;em>x&lt;/em>.
&lt;/p>
&lt;/div>
&lt;p>Suddenly, it looks like there is a very clear and quite strong relationship between &lt;em>x&lt;/em> and &lt;em>y&lt;/em>. If you were given only this graph, you might think that knowing how many berries a male eats would tell you a lot about that male’s mating success. Indeed, the top quantile, on average, has an approximately 200% higher y (200% higher mating success) than the bottom quantile.&lt;/p>
&lt;p>Also note the apparent nonlinearity. The top and bottom quantiles seem to have very much increased/reduced y relative to the middle ones. Note that we see no such feature in the scatter plot of the original &lt;em>x&lt;/em> and &lt;em>y&lt;/em> values.&lt;/p>
&lt;p>Finally, the exact same data look quite different depending on the exact number of quantiles. Figure &lt;a href="#fig:figure3">3&lt;/a> shows the same data presented with 6 quantiles.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure3">&lt;/span>
&lt;img src="Fig3.png" alt="Mean y (+/- standard error) as a function of quantiles of x. Now using 6 instead of 10 quantiles." width="75%" />
&lt;p class="caption">
Figure 3: Mean &lt;em>y&lt;/em> (+/- standard error) as a function of quantiles of &lt;em>x&lt;/em>. Now using 6 instead of 10 quantiles.
&lt;/p>
&lt;/div>
&lt;p>And Figure &lt;a href="#fig:figure4">4&lt;/a> shows the same data presented with 20 quantiles.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure4">&lt;/span>
&lt;img src="Fig4.png" alt="Mean y (+/- standard error) as a function of quantiles of x. Now using 20 instead of 10 quantiles." width="75%" />
&lt;p class="caption">
Figure 4: Mean &lt;em>y&lt;/em> (+/- standard error) as a function of quantiles of &lt;em>x&lt;/em>. Now using 20 instead of 10 quantiles.
&lt;/p>
&lt;/div>
&lt;p>As you can see, the same data look quite different depending on the exact number of quantiles we use.&lt;/p>
&lt;p>So, whenever somebody shows you data aggregated into quantiles, ask for an &lt;em>x&lt;/em>–&lt;em>y&lt;/em> scatter plot and a correlation coefficient. And then square the correlation coefficient and evaluate the % variance explained. A squared correlation coefficient below 0.1 (&lt;em>r&lt;/em> &amp;lt; 0.3) means the effect is pretty much non-existent, regardless of how low the &lt;em>P&lt;/em> value is.&lt;/p>
&lt;/div></description></item></channel></rss>