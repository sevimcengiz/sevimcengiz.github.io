<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Academic publishing on Sevim Cengiz</title><link>https://sevimcengiz.github.io/tags/Academic-publishing/</link><description>Recent content in Academic publishing on Sevim Cengiz</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 16 Oct 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://sevimcengiz.github.io/tags/Academic-publishing/index.xml" rel="self" type="application/rss+xml"/><item><title>Springer's abusive licensing demands</title><link>https://sevimcengiz.github.io/blog/2017/10/16/Springers-abusive-licensing-demands/</link><pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2017/10/16/Springers-abusive-licensing-demands/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Today I posted a tweetstorm on publishing an article with &lt;a href="https://f1000research.com/">F1000Research&lt;/a> that was originally commissioned by Springer:&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="en">
&lt;p lang="en" dir="ltr">
&lt;ol style="list-style-type: decimal">
&lt;li>We just published this article in &lt;a href="https://twitter.com/F1000Research?ref_src=twsrc%5Etfw">&lt;span class="citation">@F1000Research&lt;/span>&lt;/a> because of &lt;a href="https://twitter.com/SpringerNature?ref_src=twsrc%5Etfw">&lt;span class="citation">@SpringerNature&lt;/span>&lt;/a>'s abusive licensing practices. &lt;a href="https://t.co/Xx88jkA3ec">https://t.co/Xx88jkA3ec&lt;/a>
&lt;/p>
— Claus Wilke (&lt;span class="citation">@ClausWilke&lt;/span>) &lt;a href="https://twitter.com/ClausWilke/status/919958741734379520?ref_src=twsrc%5Etfw">October 16, 2017&lt;/a>
&lt;/blockquote>
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>&lt;/li>
&lt;/ol>
&lt;p>I received several requests to turn this into a blog post, so here we go. The blog post consists mostly of the text of the tweets, with some minor edits and clarifications.&lt;/p>
&lt;p>We just published &lt;a href="https://f1000research.com/articles/6-1845/v1">this article&lt;/a> in F1000Research because of Springer’s abusive licensing practices. The article was originally written to become a chapter in a Springer book on methods in protein evolution, and it describes key protocols my lab uses to measure site-specific rates in proteins and correlate them with protein structure. We’ve published extensively on this topic in recent years, see e.g. &lt;a href="http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002452">Jack et al., PLOS Biology 2016&lt;/a>.&lt;/p>
&lt;p>However, as a precondition of publishing, Springer wanted us to assign to them the exclusive, permanent copyright to the work (Figure &lt;a href="#fig:figure1">1&lt;/a>). In essence, they wanted us to donate our work to them so they could lock it behind a paywall and make money off of it. I were an unpaid intern at Springer while writing the article, Springer’s request would violate US labor law. But since I’m an independent agent, I guess I’m allowed to make donations to a commercial entity.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure1">&lt;/span>
&lt;img src="figure1.png" alt="The copyright agreement I was asked to sign. Key phrases of the agreement are underlined in red. Most important is the word exclusive, which implies that the authors (us) lose any rights not explicitly enumerated in the remainder of the agreement." width="552" />
&lt;p class="caption">
Figure 1: The copyright agreement I was asked to sign. Key phrases of the agreement are underlined in red. Most important is the word &lt;em>exclusive&lt;/em>, which implies that the authors (us) lose any rights not explicitly enumerated in the remainder of the agreement.
&lt;/p>
&lt;/div>
&lt;p>You might ask me: “What exactly is it you are complaining about here? People sign these agreements all the time, as have you in the past. How is this case any different?” The difference is that in this case, there’s virtually no benefit, even non-monetary, that Springer provides to in return. I understand that people might be willing to sign away their copyright in return for Nature paper that gets them their next job or grant.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> But a chapter in a methods book? That isn’t even peer reviewed? It doesn’t bring me (or my trainees) any prestige, unless the article is widely available and people use it and cite it a lot. But Springer explicitly doesn’t want that, because they make money from people buying the book to read the method.&lt;/p>
&lt;p>I cannot let Springer own key protocols from my lab.&lt;/p>
&lt;p>As if this alone weren’t enough, I also found my conversations with them disingenuous. In particular, when I asked for a non-exclusive agreement, they lied to me about what the agreement says (Figure &lt;a href="#fig:figure2">2&lt;/a>). Note the phrasing “We don’t claim copyright or ownership of the content itself” in this is response by Springer. It is blatantly false. Remember the copyright agreement is exclusive. This means the authors lose all rights except those explicitly stated as retained.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure2">&lt;/span>
&lt;img src="figure2.png" alt="Statement by Springer representative. A Springer representative made this statement when I requested a non-exclusive copyright agreement." width="770" />
&lt;p class="caption">
Figure 2: Statement by Springer representative. A Springer representative made this statement when I requested a non-exclusive copyright agreement.
&lt;/p>
&lt;/div>
&lt;div class="figure">&lt;span id="fig:figure3">&lt;/span>
&lt;img src="figure3.png" alt="Rights retained under the proposed copyright agreement." width="554" />
&lt;p class="caption">
Figure 3: Rights retained under the proposed copyright agreement.
&lt;/p>
&lt;/div>
&lt;p>Let’s take a look at the rights that we would have retained (Figure &lt;a href="#fig:figure3">3&lt;/a>). Pay attention to the repeated occurrence of the word “content”. The person who drafted the copyright agreement knew very well that it would cover the contents of the contribution. That’s the point of copyright. So, according to Figure &lt;a href="#fig:figure1">1&lt;/a>, we would have lost the right to communicate the content to non-scientists. This includes, for example, commercial entities that may want to use our protocols. We would also have lost the right to post a preprint of the work. And it is unclear whether we would have been allowed to post it on personal webpages. That depends on whether the visitors of our personal webpages can be considered to be scientists or not. We would certainly have lost the right to publish an updated version of the protocols at some future date with a different publisher. I wouldn’t even have been allowed to this paper in a future anthology of my most influential papers, assuming it would be distributed at some cost. (Unlikely that I’d want to, but that’s not the point.)&lt;/p>
&lt;p>For all the above reasons, the paper is now with F1000Research, and you can read it free of charge, copy it, and build on it. Enjoy.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>Andrew Rambaut &lt;a href="https://twitter.com/arambaut/status/919980657153138688">pointed out&lt;/a> that Nature (the journal) does not require a permanent, exclusive copyright license anymore. This statement was later disputed by &lt;a href="https://twitter.com/dhimmel/status/920168108962312192">Daniel Himmelstein.&lt;/a>&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Do you have to publish papers to obtain a PhD?</title><link>https://sevimcengiz.github.io/blog/2017/01/06/do-you-have-to-publish-papers-for-a-phd/</link><pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2017/01/06/do-you-have-to-publish-papers-for-a-phd/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>It is common for friction to arise between graduate students and their supervisors (PIs) over how many and what kind of papers the students need to publish before graduating. While on occasion the students’ complaint is that their PI keeps them from publishing,&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> the much more common scenario is one where the PI wants the student to complete &lt;em>x&lt;/em> papers in &lt;em>y&lt;/em> journals while the student just wants to graduate and move on. When these conflicts come to a head, students usually start to inquire what the minimum requirements are before graduation.&lt;/p>
&lt;p>Of course there is only one correct answer to this question:&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="en">
&lt;p lang="en" dir="ltr">
A PhD thesis/dissertation does not in any way require papers be published from the work. &lt;a href="https://t.co/AIGnIcDyY7">https://t.co/AIGnIcDyY7&lt;/a>
&lt;/p>
— Drug Monkey (&lt;span class="citation">@drugmonkeyblog&lt;/span>) &lt;a href="https://twitter.com/drugmonkeyblog/status/817351541275230209">January 6, 2017&lt;/a>
&lt;/blockquote>
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>As much as PIs may want to impose a publication requirement, because it benefits them,&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> they cannot require the students to do something that is entirely outside of the control of both student and PI. A PhD is defined as an independent body of research, performed by the student and assessed by the PhD committee. If the student has done the research proposed during the proposal defense and has written up the resulting work in the form of a thesis, then the committee needs to evaluate that work and, if it is of sufficient quality, grant the degree.&lt;/p>
&lt;p>There are multiple reasons why requiring published papers as a condition of graduation is wrong. First, as all working scientists know, the peer review process can drag out months or even years, and it may take multiple submissions to multiple different journals before a paper is accepted. Much of this process is out of the hands of the author. Even a perfectly well executed and written paper can be held up forever, for all sorts of reasons that have nothing to do with the quality of the work. Thus, it is entirely unreasonable to ask a graduate student to wait until this process is over before being allowed to graduate.&lt;/p>
&lt;p>Second, sometimes a reasonable effort at investigating a question simply doesn’t lead to important new insight. A student may have very carefully studied a system for many years, only to conclusively prove that the interesting results they saw in their first month in the lab were caused by temperature fluctuations in the incubator room. Such work would be appropriate for a PhD thesis but it would likely not be suitable for publication in a major research journal.&lt;/p>
&lt;p>Third, by requiring a published article, the PhD committee is skirting its responsibility to evaluate the student’s work. The committee is saying, in effect, that they cannot judge whether the work is PhD-worthy unless an external body (the editors and reviewers of a scientific journal) has judged the work to be worthy of publication.&lt;/p>
&lt;p>Now, having said all this, I am of course very much in favor of graduate students publishing their work. None of my past graduate students have graduated without at least one first-author paper, and I want all my graduate students to submit a paper as soon as possible, ideally in year one or two of their graduate career. Also, I generally expect a PhD thesis to consist of at least three distinct projects, which should have been developed to the point where they could be submitted as a journal article. But to the question of whether a publication is strictly required, the answer has to be “no.”&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>There’s an inherent conflict of interest between students and PIs, in that minor, pedestrian papers will be of very little value to an established PI but can be of exceptional value to a graduate student who hasn’t published much and wants to apply for a fellowship or postdoc position. There’s a lesson here for prospective students or postdocs: If a lab publishes a steady stream of minor papers, it likely does so out of the PI’s sense of duty towards their mentees. A lab that only publishes in Nature, Science, and Cell will likely not be good for a significant chunk of its trainees, no matter how great it is for those that manage to be first author on one of the celebrated papers.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>What PIs will say is that any papers that haven’t been published by the time the student graduates will likely never be published. This statement is indeed true, in my experience. However, it also demonstrates that the paper is more important to the PI than to the student. In my opinion, any PI who cares so much about a given paper should just complete it themselves.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How to reject a rejection</title><link>https://sevimcengiz.github.io/blog/2017/01/02/how-to-reject-a-rejection/</link><pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2017/01/02/how-to-reject-a-rejection/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>For a junior scientist, it can be a major blow when their manuscript is rejected. They have poured many months to years of their time into this project, have submitted the paper where they think it belongs, and the editor puts an end to their aspirations by rejecting the submission. However, more experienced scientists, in particular those with editorial roles at major journals, know very well that many a rejection is not final. Often, a rejection is only the first step in an ongoing negotiation with the journal, one that frequently ends with the eventual publication of the article. To level the playing field between the junior and the more senior scientists, here I’ll reveal this secret to the world: How to reject a rejection.&lt;/p>
&lt;p>There are basically two strategies that you can pursue, appeal to the editor or resubmit anyways. In the following, I’ll briefly discuss the mechanics of each option and then give my opinion of which option should be used when.&lt;/p>
&lt;div id="appeal-to-the-editor" class="section level2">
&lt;h2>Appeal to the editor&lt;/h2>
&lt;p>An appeal is a request to the editor to overturn the decision. Typically, a successful appeal will change the decision from “reject” to “major revision”, i.e., it will buy you the right to revise and resubmit. Some journals have complex and formal appeals processes while others handle appeals more informally. In all cases, you will initiate the appeal by contacting the editor and explaining why you believe the reviewer criticisms were either unwarranted or can be fully addressed in a revision. The initial contact to the editor could consist of just a brief email explaining the main issues, or it could be accompanied by a detailed point-by-point response to the reviewer comments.&lt;/p>
&lt;p>How a journal handles an appeal depends on the journal’s policies and procedures as well as the specific appeal request you are making. The journal may send out your original manuscript to another reviewer, they may send your point-by-point response to the original reviewers, or they may involve one or more editors who didn’t handle the original submission. They may also ask you for more information, such as a detailed point-by-point response to the reviewer comments (if you haven’t sent one yet) or a revised manuscript draft.&lt;/p>
&lt;p>An appeal can be a long, drawn-out procedure, in particular if you initiate it with just an email to the editor. The editor may take a week or two to respond to your original email, asking you for a detailed point-by-point response. Once you submit that, the editor may have it reviewed by multiple people (the original reviewers, new reviewers, or other editors), and this process may take as long as a typical review would take. After all this time has passed, the editor may then tell you that they need to see a revised manuscript before they can make any sort of decision. The revised manuscript will then again have to be re-reviewed, of course, and this review process will likely prompt further requests for revision, even in the best-case scenario that the appeal is ultimately successful.&lt;/p>
&lt;/div>
&lt;div id="resubmit-anyways" class="section level2">
&lt;h2>Resubmit anyways&lt;/h2>
&lt;p>As an alternative to filing a formal appeal, you can also just go ahead, revise your manuscript, and resubmit. This will have to be under the guise that you have sufficiently revised the manuscript to the point where it can now be considered a new submission. The unethical way of doing this would be to change the title, change the abstract, and hope the editor won’t notice. I do not recommend this approach. The ethical way to proceed is to submit as a new submission but state clearly in the cover letter that an earlier version of this paper was previously reviewed and rejected. You should also submit a detailed response to the reviewer comments. The journal submission system may not have a special option to do so, since it thinks you’re submitting a new article, but you can always just upload your response as a supplemental file and point to it in the cover letter.&lt;/p>
&lt;/div>
&lt;div id="which-option-is-preferable" class="section level2">
&lt;h2>Which option is preferable?&lt;/h2>
&lt;p>Given the two options of either appealing or resubmitting anyways, most people would intuitively choose to appeal. Resubmitting without prior approval feels wrong and somewhat sneaky; most people need to know that they are welcome to resubmit before they feel comfortable doing it. Further, the act of appealing feels right: The reviewers were stupid, the editors didn’t get it, and I want to protest!&lt;/p>
&lt;p>However, if you consider the two options from the perspective of the editor, you’ll see that filing an appeal is almost always the worse option. The appeal, by its very nature, creates an adversarial relationship with the editor. You’re telling the editor they were wrong and need to change their decision. This adversarial relationship can make the editor negatively predisposed towards you. An appeal only makes sense, in my opinion, when the reviews were truly biased or otherwise off (e.g., contained unprofessional ad-hominem attacks), so that there is no way you can revise the manuscript to address the reviewer comments.&lt;/p>
&lt;p>If you’re going the “submit as new manuscript” route, you’re putting the editor in a position where they are more likely to be positively predisposed towards you, even though it may not seem that way. First, note that you’re in effect asking the editor for a favor, namely the favor of connecting this new submission to the history of the previous submission and to use (some of) the previous reviewers. And asking somebody to do you a favor is a great way to get them to like you.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> Second, you’re providing the editor with a submission that is easy to handle. The editor already knows which reviewers to invite, which issues to look out for, and so on. So, as long as you appear to have made serious efforts to address the prior criticisms, the editor will likely be willing to go along and at least send the paper back out to review.&lt;/p>
&lt;/div>
&lt;div id="how-well-does-this-work" class="section level2">
&lt;h2>How well does this work?&lt;/h2>
&lt;p>What are the chances of success? After having read this post, will you now be able to publish all your work in Science and Nature? No, of course not. Appeals and uninvited resubmissions frequently are unsuccessful. However, they succeed often enough that you should at least consider going this route from time to time. If you never resubmit a rejected article you’re leaving money on the table. You can be certain that any PI who routinely publishes in high-profile journals does a lot of appealing and resubmitting of rejected articles.&lt;/p>
&lt;p>But won’t the editors just get annoyed and put you on their blacklist? I think that’s unlikely, unless you become really obnoxious, e.g. by appealing a failed appeal or by not putting an honest effort into revising your manuscript. Remember that editors fundamentally want to work with you and want to give you a positive decision. They’re not editors because they enjoy handing out rejections all day. They are doing this thankless, poorly remunerated job primarily because they want to advance their field and their community.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> Thus, they’d much rather handle a good paper they can accept than a bad paper they have to reject.&lt;/p>
&lt;p>Finally, you should know that many editors reject papers that they expect to be resubmitted. My own rule is that if the reviewers have pointed out a potential major flaw in the work, one that may require a substantial rethinking of the entire paper, then I’d rather reject than ask for major revisions. I do this because to me calling for major revisions creates the expectation that the paper will be accepted after the revisions have been made. And I don’t want to string authors along, make them revise, and then reject at the very end of this long process.&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>This is called the &lt;a href="https://en.wikipedia.org/wiki/Ben_Franklin_effect">Ben Franklin effect,&lt;/a> after Ben Franklin, who asked a rival legislator to lend him a rare book.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Yes, this statement applies even to paid, professional editors.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>This doesn’t mean I never reject revised manuscripts. It just means I try to make my initial editorial decisions such that rejections after revision are rare.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Hiding journal names from your publication list stinks</title><link>https://sevimcengiz.github.io/blog/2015/12/08/hiding-journal-names-from-your-publication-list-stinks/</link><pubDate>Tue, 08 Dec 2015 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2015/12/08/hiding-journal-names-from-your-publication-list-stinks/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Michael Eisen recently announced his new website, which features a new publication list that doesn’t mention journal names anywhere:&lt;/p>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
made a new lab website - completely scrubbed any mention of journal titles - &lt;a href="https://t.co/iTwYvWDwqX">https://t.co/iTwYvWDwqX&lt;/a>
&lt;/p>
— Michⓐel Eisen (&lt;span class="citation">@mbeisen&lt;/span>) &lt;a href="https://twitter.com/mbeisen/status/673419464633749504">December 6, 2015&lt;/a>
&lt;/blockquote>
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>This idea was quickly picked up by others, e.g.:&lt;/p>
&lt;blockquote class="twitter-tweet" lang="en">
&lt;p lang="en" dir="ltr">
Following &lt;a href="https://twitter.com/mbeisen">&lt;span class="citation">@mbeisen&lt;/span>&lt;/a>, removed journal names from website. But also links to cites, almetrics, &amp;amp; preprints. &lt;a href="https://t.co/bPqZgrr2iA">https://t.co/bPqZgrr2iA&lt;/a>
&lt;/p>
— Jeffrey Ross-Ibarra (&lt;span class="citation">@jrossibarra&lt;/span>) &lt;a href="https://twitter.com/jrossibarra/status/673680344982274048">December 7, 2015&lt;/a>
&lt;/blockquote>
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>I spoke out against this idea, since I immediately had the gut-feeling response that something was wrong with it:&lt;/p>
&lt;blockquote class="twitter-tweet" data-conversation="none" lang="en">
&lt;p lang="en" dir="ltr">
&lt;a href="https://twitter.com/yanivbrandvain">&lt;span class="citation">@yanivbrandvain&lt;/span>&lt;/a> Yeah, I'm not on board with hiding journal names. That's &lt;a href="https://twitter.com/mbeisen">&lt;span class="citation">@mbeisen&lt;/span>&lt;/a>'s thing, and now also &lt;a href="https://twitter.com/jrossibarra">&lt;span class="citation">@jrossibarra&lt;/span>&lt;/a>, I guess. &lt;a href="https://twitter.com/jaimedash">&lt;span class="citation">@jaimedash&lt;/span>&lt;/a>
&lt;/p>
— Claus Wilke (&lt;span class="citation">@ClausWilke&lt;/span>) &lt;a href="https://twitter.com/ClausWilke/status/673723100379283456">December 7, 2015&lt;/a>
&lt;/blockquote>
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>However, at the time, I couldn’t quite formulate what I thought the key issue was. I have now given this more thought, and I’ve found various reasons why I think it’s a bad idea to hide journal names. However, I’ve also realized that most of these arguments don’t even matter. As I’ll argue here, hiding journal names from the publication list is directly at odds with the principles of openness and egalitarianism that people like Michael Eisen so strongly promote. Therefore, to put it bluntly, I think this practice stinks.&lt;/p>
&lt;p>We need to realize that in the current world of scientific publishing, removing journal names from the publication list has cause and effect reversed. If we had reached a point where nobody cared about journal names,&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> then removing them from the publication list and listing papers simply by author, title, and DOI would be the logical next step. But since, as of today, there are plenty of people in this world who do care about journal names, hiding them is counterproductive. We can state all we want that “people shouldn’t care about journal names,” but hiding these names from publication lists won’t make it so.&lt;/p>
&lt;p>Think about it this way: If you want to move towards a world where people care more about article content than journal name, who do you need to convince? Those who already agree with you, or those who disagree? Obviously the latter. What would be a meaningful action towards that goal? In my mind, the most important action is to demonstrate that the publication venue doesn’t matter that much, by publishing your best work in journals such as PLOS ONE, PeerJ, or F1000Research, or even by just posting studies on bioRxiv without submitting them to any journal at all. These actions would demonstrate to the world that you’re putting your money where your mouth is and that you don’t care about perceived journal impact. How would the world notice that you’re doing this? They might go to your website and see that you, the esteemed scholar and noted expert in your field, publish in “low-impact” journals and on preprint servers, validating these publication venues in the process.&lt;/p>
&lt;p>By contrast, if you’re hiding the journal names from your website, this important message is not conveyed. At best, people will not notice where you publish. At worst, they may wonder what you’re hiding. And that’s where things are really getting counter-productive. Because, if you are an outspoken proponent of open access, of preprints, of post-publication peer review, of publishing in non-selective journals, then any paper you publish that violates these principles will weaken your message. And if you’re not even stating on your website where you’re publishing, you may be perceived as being dishonest. For example, on &lt;a href="http://www.eisenlab.org//publications.html">Michael Eisen’s publication list&lt;/a>, over the last 3 years, I count one pay-walled Elsevier paper (!), two papers in the highly selective journal eLife, and several papers in the fairly selective journals Genome Research, PLOS Genetics, and PLOS Computational Biology. (These papers are easier to find on his &lt;a href="https://scholar.google.com/citations?hl=en&amp;amp;user=z2foFg4AAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate">Google Scholar page,&lt;/a> since it lists journal names, but they’re all on his web site as well, I checked.) So clearly the Eisen lab does not publish everything they do as post-pub review on F1000Research or as eternal preprint on bioRxiv.&lt;/p>
&lt;p>To be clear: I have no problems with publishing at venues such as eLife, Genome Research, PLOS Computational Biology, or even Science or Nature. I think that the NIH Open Access mandate solves the majority of the access issues.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> What I have a problem with is publishing in such journals and hiding that fact from your website while blogging about the evils of peer review. As long as you participate in the traditional peer-review system, as author, reviewer, or editor, you should be honest and transparent about where you publish.&lt;/p>
&lt;p>There are other reasons why I think hiding journal names is a bad idea, and I may go into them in a future blog post. For now, I’ll just present to you, without further comment, &lt;a href="https://scholar.google.com/citations?hl=en&amp;amp;user=GIjz5dMAAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate">this Google Scholar profile.&lt;/a> In summary, hide journal names once everybody agrees that they don’t matter, but not one day earlier.&lt;/p>
&lt;p>&lt;strong>Update 12/11/2015:&lt;/strong> This discussion was featured in a &lt;a href="http://www.nature.com/news/what-s-in-a-journal-name-1.18987">Nature News article.&lt;/a>&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>I doubt that time will ever come, but let’s assume it will.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>To the extent possible, I make sure that my own papers get submitted to PubMed Central. The most recent papers &lt;a href="http://wilkelab.org/publications/">on my publication list&lt;/a> may not have PMC numbers yet, because it always takes a while until papers make their way into PubMed Central. Also, for papers for which I’m not the corresponding author, I cannot always ensure that they get submitted there.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>The Google Scholar preprint bug redux</title><link>https://sevimcengiz.github.io/blog/2015/10/08/google-scholar-bug-redux/</link><pubDate>Thu, 08 Oct 2015 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2015/10/08/google-scholar-bug-redux/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Regular readers of my blog will know that I regularly complain about Google Scholar’s handling of preprints, see e.g. &lt;a href="https://sevimcengiz.github.io/blog/2014/11/1/the-google-scholar-preprint-bug">here&lt;/a> or &lt;a href="https://sevimcengiz.github.io/blog/2014/12/2/how-google-scholar-discourages-young-scientists-from-posting-preprints">here.&lt;/a> Well, this week, I had the opportunity to &lt;a href="http://scholarlykitchen.sspnet.org/2015/10/05/guest-post-highwires-john-sack-on-online-indexing-of-scholarly-publications-part-1-what-we-all-have-accomplished/#comment-155912">raise my concerns&lt;/a> to Anurag Acharya, the co-founder of Google Scholar. &lt;a href="http://scholarlykitchen.sspnet.org/2015/10/05/guest-post-highwires-john-sack-on-online-indexing-of-scholarly-publications-part-1-what-we-all-have-accomplished/#comment-155918">His initial response&lt;/a> and the subsequent discussion have clarified several things. We now know:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>The bug exists&lt;/li>
&lt;li>The Scholar team is aware of it&lt;/li>
&lt;li>They don’t know how to fix it&lt;/li>
&lt;li>They don’t think it’s a particularly pressing problem&lt;/li>
&lt;li>For any given paper, the problem will go away eventually, after several months or more&lt;/li>
&lt;/ol>
&lt;div id="so-what-is-this-bug-youre-talking-about" class="section level2">
&lt;h2>So what is this bug you’re talking about?&lt;/h2>
&lt;p>In a nutshell, for papers with a preprint, the bug will prevent the final, official journal publication to appear in the Scholar database, often for many months. If you search for the article by title, only the preprint version will show. If you search by DOI, nothing will show. Importantly, other articles from the same issue of the journal will all be properly indexed in Scholar, but the one article that happened to have a preprint will be missing.&lt;/p>
&lt;/div>
&lt;div id="why-should-i-care-if-the-problem-will-fix-itself-eventually" class="section level2">
&lt;h2>Why should I care if the problem will fix itself eventually?&lt;/h2>
&lt;p>Anybody who would like to encourage more scientists to post preprints should care. And any junior scientist should care twice. This bug can have a very real effect on the career of junior scientists, by limiting their visibility or making them appear much less successful or competent than they actually are. Here are a few very real scenarios the bug can cause:&lt;/p>
&lt;ul>
&lt;li>&lt;p>You know that John Smith posted an interesting preprint 2 years ago, and you wonder if that work was ever published. You search Google Scholar and only find the preprint. So you conclude the paper never saw the light of day or maybe is embattled in review. In truth, the paper came out 8 months ago in PNAS, but Google Scholar will hide that version from you.&lt;/p>&lt;/li>
&lt;li>&lt;p>You consider hiring a promising young scientist as a postdoc or maybe even a faculty member. However, as you pull up their Google Scholar profile, you notice that over the last two years they seem to have published only preprints. And several of the articles they list on their cv don’t show up in the Scholar database at all. You conclude the scientist is dishonest and you decline the application.&lt;/p>&lt;/li>
&lt;li>&lt;p>You post a preprint that contains an error. Thankfully, the error gets noticed in review and you fix it for the final publication (and/or post a new version of the preprint). However, Google Scholar keeps showing the old, erroneous version of the preprint, many months after the fix has been made. People keep reading the erroneous version and keep giving you grief over it.&lt;/p>&lt;/li>
&lt;li>&lt;p>An important paper in your field is published, and you would like to know about it. However, since the paper had a preprint, the official article is hidden from Scholar, and Scholar won’t notify you that it came out.&lt;/p>&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div id="but-it-happens-only-very-rarely-right" class="section level2">
&lt;h2>But it happens only very rarely, right?&lt;/h2>
&lt;p>That’s the stance of the Scholar team. It doesn’t mesh with my experience, though. Everybody I know who regularly posts preprints has been bitten by the bug. I cross my fingers every time I post one. And whenever I bring up this issue, some random person mentions that they have experienced the same. Also, my colleague Chris Adami just posted the following:&lt;/p>
&lt;blockquote class="twitter-tweet" data-conversation="none" lang="en">
&lt;p lang="en" dir="ltr">
. &lt;a href="https://twitter.com/ClausWilke">&lt;span class="citation">@ClausWilke&lt;/span>&lt;/a> A cursory look at just the first page of my Google Scholar shows that about half of my articles are affected by this bug.
&lt;/p>
— Christoph Adami (&lt;span class="citation">@ChristophAdami&lt;/span>) &lt;a href="https://twitter.com/ChristophAdami/status/652126221254397952">October 8, 2015&lt;/a>
&lt;/blockquote>
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>While this bug may be rare in some sense of the word “rare,” it happens frequently enough to be a real issue for real scientists and every-day users of Google Scholar.&lt;/p>
&lt;/div>
&lt;div id="is-there-a-workaround" class="section level2">
&lt;h2>Is there a workaround?&lt;/h2>
&lt;p>Not really. You can add papers manually to your Google Scholar profile, but that won’t make them show up in the search results. And they will also not be linked to the actual journal publications, a major drawback in my opinion. If you know of a preprint and are wondering whether it has been published or not, don’t check with Scholar. Check with some other data base, such as PubMed. Or just do a regular Google search. The preprint bug does not affect regular Google, which will find the papers that Google Scholar doesn’t know about.&lt;/p>
&lt;p>I hope that the Google Scholar team will eventually realize that this is an important issue to get right. In the mean time, if you have been bitten by the bug, please let me know, so we can build a record of cases and demonstrate this is an important issue. And, if you’re looking for the official publications of long-standing preprints, look for them using regular Google, not Google Scholar.&lt;/p>
&lt;/div></description></item><item><title>How to prepare an article for resubmission, Part II</title><link>https://sevimcengiz.github.io/blog/2014/12/18/how-to-prepare-an-article-for-resubmission-part-ii/</link><pubDate>Thu, 18 Dec 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/12/18/how-to-prepare-an-article-for-resubmission-part-ii/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>In my previous post on &lt;a href="https://sevimcengiz.github.io/blog/2014/11/16/how-to-prepare-an-article-for-resubmission">how to prepare an article for resubmission,&lt;/a> I failed to mention one important point: In your response to the reviewers, quote the &lt;em>entire&lt;/em> referee report, even the introductory sentences. Don’t just quote the specific comments to which you are replying. This may seem unnecessary but it is in fact crucial, in particular if the introductory sentences were largely positive. (If they were highly critical, you may want to omit them, even though in this case you probably should provide a response.)&lt;/p>
&lt;p>Keep in mind that when the revised manuscript goes back to the editor and the previous reviewers, neither will remember the exact thoughts they had when they previously looked at your manuscript. In addition, the reviewers may never actually have seen the comments of the other reviewers. And finally, most editors and reviewers will look at your response to the reviewer comments before they look at anything else related to your manuscript. Thus, this is your opportunity to remind the editor and the reviewers that your manuscript overall was judged to be interesting and valuable, even if there were some issues to be addressed. By not quoting these comments, you only highlight the critical aspects of the previous reviews. For the same reasons, it is often a good idea to start the response with a brief summary of the overall reviewer sentiments, such as: “Reviewers 1 and 2 thought the manuscript addressed an important topic and had only minor comments. Reviewer 3 was more critical but also acknowledged the timeliness of our work.”&lt;/p></description></item><item><title>Relationship between h index and total citations count</title><link>https://sevimcengiz.github.io/blog/2014/12/08/relationship-between-h-index-and-total-citations-count/</link><pubDate>Mon, 08 Dec 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/12/08/relationship-between-h-index-and-total-citations-count/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>I came across an interesting paper&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> that derives a mathematical relationship between the total number of citations a scientist has received, &lt;span class="math inline">\(N_\text{tot}\)&lt;/span>, and the scientist’s &lt;span class="math inline">\(h\)&lt;/span> index.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> The paper, written by Alexander Yong, argues that for typical scientists, &lt;span class="math inline">\(h\)&lt;/span> is given simply as 0.54 times the square-root of &lt;span class="math inline">\(N_\text{tot}\)&lt;/span>. The paper also derives confidence bounds on this estimate, and it shows that scientists who have written only a few highly-cited works will generally fall below this estimate. While the paper is set up as a critique of the &lt;span class="math inline">\(h\)&lt;/span> index, I think it shows that the &lt;span class="math inline">\(h\)&lt;/span> index works largely as intended. It measures the total amount of citations a researcher has received, but it adequately down-weighs the effect of a few extremely highly cited works in a researcher’s publication list.&lt;/p>
&lt;p>The argument of the paper goes as follows: Let’s consider all the possible ways in which a researcher’s &lt;span class="math inline">\(N_\text{tot}\)&lt;/span> citations may be distributed over a number of publications. On one extreme, the researcher could have written a single article, which has been cited &lt;span class="math inline">\(N_\text{tot}\)&lt;/span> times. On the other extreme, the researcher could have written &lt;span class="math inline">\(N_\text{tot}\)&lt;/span> articles, which all have been cited exactly once. And of course, there are many possibilities between those extremes, where some articles receive more citations and others fewer. The paper then assumes that all these different ways in which &lt;span class="math inline">\(N_\text{tot}\)&lt;/span> citations can be distributed over one or more articles are equally likely, and calculates the expected &lt;span class="math inline">\(h\)&lt;/span> under that assumption. That value, it turns out, is approximately &lt;span class="math inline">\(0.54 \times N_\text{tot}^{1/2}.\)&lt;/span> The paper then tests this relationship for a number of famous mathematicians (Fields-medal winners and members of the National Academy of Sciences) and finds that it generally works quite well, though typically as an upper bound. It is rare for a scientist to have &lt;span class="math inline">\(h\)&lt;/span> exceed the predicted value of &lt;span class="math inline">\(0.54 \times N_\text{tot}^{1/2}.\)&lt;/span> On the flip side, many scientists who have written famous, highly cited books have an &lt;span class="math inline">\(h\)&lt;/span> quite a bit lower than the predicted value, because the books cause the total citation count to be overinflated.&lt;/p>
&lt;p>I wanted to know to what extent this formula worked in a different field. So I tested it on the members of my department. For each faculty member&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a> of the Department of Integrative Biology, I obtained their total number of citations and their &lt;span class="math inline">\(h\)&lt;/span> index from Google Scholar, and then I plotted the observed &lt;span class="math inline">\(h\)&lt;/span> against the predicted &lt;span class="math inline">\(h\)&lt;/span> using Yong’s formula (Figure &lt;a href="#fig:figure1">1&lt;/a>). As you can see, the formula works remarkably well. Almost everybody falls right on top of the line. Importantly, this sample covers a wide range of different career stages.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure1">&lt;/span>
&lt;img src="observed_v_predicted_h.png" alt="Observed vs. predicted \(h\) for 29 faculty members in Integrative Biology. Members of the National Academy are plotted in red." width="80%" />
&lt;p class="caption">
Figure 1: Observed vs. predicted &lt;span class="math inline">\(h\)&lt;/span> for 29 faculty members in Integrative Biology. Members of the National Academy are plotted in red.
&lt;/p>
&lt;/div>
&lt;p>Three faculty members are plotted in red in the figure: those are members of the National Academy, and they are the highest-cited scientists in the department. Interestingly, two have very high total citation counts but, in comparison, not that high of an &lt;span class="math inline">\(h\)&lt;/span> index, while one has the highest overall &lt;span class="math inline">\(h\)&lt;/span> index with comparatively fewer citations. The former two both have written famous books, and many of their citations are to these books. By contrast, the latter scientist stands out by having published a particularly large number of articles that all have been well cited. In fact, that scientist is performing slightly better than the &lt;span class="math inline">\(h = 0.54 \times N_\text{tot}^{1/2}\)&lt;/span> prediction, a truly remarkable result at that high of a total citation count.&lt;/p>
&lt;p>In summary, I find that the predicted relationship between &lt;span class="math inline">\(h\)&lt;/span> and &lt;span class="math inline">\(N_\text{tot}\)&lt;/span> works well in my field. However, since major deviations between this relationship can be observed for scientists with a few extremely highly cited works, I prefer using &lt;span class="math inline">\(h\)&lt;/span> instead of &lt;span class="math inline">\(N_\text{tot}\)&lt;/span> to estimate a scientist’s total impact on their field.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>A. Yong (2014). &lt;a href="https://doi.org/10.1090/noti1164">Critique of Hirsch’s Citation Index: A Combinatorial Fermi Problem.&lt;/a> &lt;em>Notices of the AMS&lt;/em> 61:1040-1050.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>The &lt;span class="math inline">\(h\)&lt;/span> index is the number of papers a scientist has written that have received at least &lt;span class="math inline">\(h\)&lt;/span> citations. For example, if you have &lt;span class="math inline">\(h = 10\)&lt;/span>, then you have written 10 papers that have been cited 10 or more times. You may have written more than 10 papers total, but none of the other papers you may have written has received more than 10 citations yet.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>To be precise, each faculty member with a Google Scholar profile. This covers almost but not exactly the entire department.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How Google Scholar discourages young scientists from posting preprints</title><link>https://sevimcengiz.github.io/blog/2014/12/02/how-google-scholar-discourages-young-scientists-from-posting-preprints/</link><pubDate>Tue, 02 Dec 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/12/02/how-google-scholar-discourages-young-scientists-from-posting-preprints/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>I have previously blogged about the issues that &lt;a href="https://sevimcengiz.github.io/blog/2014/11/1/the-google-scholar-preprint-bug">preprints can cause on Google Scholar.&lt;/a> Today I was reminded that these issues have real-world implication for junior scientists, and that they may discourage junior scientists from posting preprints.&lt;/p>
&lt;p>I had the following conversation with one of my students (paraphrased):&lt;/p>
&lt;p>&lt;em>Me:&lt;/em> So, do you want to post the paper we just submitted as a preprint?&lt;/p>
&lt;p>&lt;em>Student:&lt;/em> No, not really.&lt;/p>
&lt;p>&lt;em>Me:&lt;/em> Are you concerned about keeping your competitive advantage, so you can finish a second paper on the topic before we reveal to the world what we’re up to?&lt;br />
&lt;em>(With this particular paper, I had wondered whether we should submit it as a preprint or not. There are a number of obvious follow-up works we can do relatively quickly, and so could others.)&lt;/em>&lt;/p>
&lt;p>&lt;em>Student:&lt;/em> No, I’m not particularly worried about that. I just don’t want Google Scholar to list my paper as bioRxiv for the next few years, way past the time the paper has actually come out. Several of my current papers are still listed as their preprint version even though they’ve appeared ages ago. Maybe once I have 100 papers and 10,000 citations I won’t care anymore, but at my current stage I can’t afford having Google Scholar obscure my record by listing all my papers in their preprint version only.&lt;/p>
&lt;p>&lt;strong>Update #1, 12/03/2014:&lt;/strong> I’m getting a lot of comments to the effect that one can edit the Google Scholar profile. A couple to responses to that:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>Yes, there are manual workarounds for most issues. That doesn’t mean the default behavior of Scholar is not &lt;em>discouraging&lt;/em>.&lt;/p>&lt;/li>
&lt;li>&lt;p>Merging of articles doesn’t work when the &lt;a href="https://sevimcengiz.github.io/blog/2014/11/1/the-google-scholar-preprint-bug">preprint shadows the final article,&lt;/a> because the final article is simply not visible in the Scholar database.&lt;/p>&lt;/li>
&lt;li>&lt;p>Even if you fix your own profile, that doesn’t fix how the article appears on your co-authors’ profile or in a general search for the article, e.g. by title.&lt;/p>&lt;/li>
&lt;li>&lt;p>It is not clear what happens to citations of shadowed articles. Are they or are they not counted? We don’t know. There’s certainly the worry that they are not.&lt;/p>&lt;/li>
&lt;li>&lt;p>The only way I know to fix shadowed articles &lt;em>on your own profile&lt;/em> is to manually add the reference, then merge, and then undo all of that a year later when Scholar has finally caught up to the existence of your article. It’s cumbersome, prone to errors, and certainly &lt;em>discouraging.&lt;/em>&lt;/p>&lt;/li>
&lt;/ol>
&lt;p>I will not stop posting preprints. But I will also not pretend everything is fine with Google Scholar and preprints when there are some glaring issues. Google Scholar is being used increasingly by departments in hiring and promotion decisions. Scientists should rightfully worry about how their work does or does not appear on Scholar.&lt;/p>
&lt;p>&lt;strong>Update #2, 12/03/2014:&lt;/strong> So it turns out manually editing entries doesn’t work as expected when articles are shadowed by their preprint. You can add the reference, but you cannot make it link to the correct article. Check out &lt;a href="https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=Vssu9d0AAAAJ&amp;amp;sortby=pubdate&amp;amp;citation_for_view=Vssu9d0AAAAJ:nrtMV_XWKgEC">this entry.&lt;/a> The title is not clickable, and the Scholar articles listed at the bottom do not include any links to the actual journal version of the article (or even the latest preprint version).&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>My statements are correct as of 12/03/2014. Eventually Google Scholar will catch up and the links will appear.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How to prepare an article for resubmission</title><link>https://sevimcengiz.github.io/blog/2014/11/16/how-to-prepare-an-article-for-resubmission/</link><pubDate>Sun, 16 Nov 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/11/16/how-to-prepare-an-article-for-resubmission/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>So your latest scientific masterpiece has come back from review with the most likely outcome other than rejection: major revision. The reviewers and the editor think that your work has merit, but they also have a long list of comments and criticism that they expect you to address before the article is acceptable for publication. You read the reviews and you feel like they lay out two years worth of work. How do you best deal with this situation?&lt;/p>
&lt;div id="your-life-will-be-easier-if-you-understand-everybodys-objectives" class="section level2">
&lt;h2>Your life will be easier if you understand everybody’s objectives&lt;/h2>
&lt;p>Let’s first consider the perspective of the three groups of people involved: the editor, the reviewers, and the authors (i.e., you). The editor wants to make sure there are no major problems with your paper, in particular problems that would potentially embarrass her&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> down the line. So the editor will pay close attention to any points the reviewers raise that look like your work might be flawed. She will generally be less worried about whether you actually do every additional analysis the reviewers suggest. A good editor knows that most reviewers will suggest more changes than are strictly necessary to get the paper publication ready.&lt;/p>
&lt;p>The reviewers, primarily, will want to be recognized for their knowledge of the field. They want you to acknowledge that they noticed or knew something you didn’t. Even if it may not seem that way, most reviewer comments are written as constructive criticism, suggestions from the reviewers to you on how you could improve your work. As long as your revisions acknowledge the reviewers’ views, you should be fine. However, on occasion, a reviewer thinks that something you’re doing is fundamentally flawed. In those cases, you may have to put in some extra effort to appease the reviewer.&lt;/p>
&lt;p>I assume you know what your objective is in this interaction, but in case you had doubts I’ll tell you: You want to get the paper published with as little extra work as possible. You thought your paper was done when you first submitted, so any additional work you’re asked to do amounts to pointless busywork from your perspective.&lt;/p>
&lt;p>Now that we know what everybody’s objectives are in this game, let’s discuss some strategies for successful resubmission.&lt;/p>
&lt;/div>
&lt;div id="start-by-drafting-a-response-to-the-reviewers" class="section level2">
&lt;h2>1. Start by drafting a response to the reviewers&lt;/h2>
&lt;p>The absolute worst thing you can do after having received reviewer comments is to run back into the lab and start all the additional experiments the reviewers want you to do. This will drag you down a rabbit hole that you will find difficult to come out of, and you will waste a lot of time doing unnecessary work. You need a clear plan of what to do. The best way to develop that plan is to start drafting a response to the reviewers. Copy all the reviewer comments into a file, mark them in some color other than black (I like blue), and then start adding your responses in black.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> See how many reviewer points you can dispense with by writing a response that requires only very minor edits to your manuscript.&lt;/p>
&lt;p>For the reviewer points that require more extensive rewriting or additional experiments, write out a plan of what you will do to address these points. I like to highlight the parts in the response that I still have to address in the manuscript, and I remove the highlights once I have done so. In this way, I don’t lose track of which edits I have or haven’t done.&lt;/p>
&lt;p>From what I have seen, the winning strategy employed by some of the most experienced and successful scientists is to write a very long, detailed response and keep the actual manuscript edits to a minimum. It’s not unusual to see a 5 page response to the reviewers accompanying very minor revisions in the actual paper, a few sentences added here and there, and a few additional references thrown in for good measure. These scientists have, over the years, developed a good sense of the minimum amount of work they can get away with and still have their revisions accepted.&lt;/p>
&lt;/div>
&lt;div id="realize-that-the-reviewer-is-always-right" class="section level2">
&lt;h2>2. Realize that the reviewer is always right&lt;/h2>
&lt;p>Regardless of how inane a reviewer’s comments may seem, the reviewer is always right. You don’t gain anything from being upset about the reviewer’s incompetence or lack of knowledge in your area. Instead, think why the reviewer may have reacted the way he did. Maybe you didn’t explain something carefully enough, or you assumed something was widely known that actually isn’t. In your response to the reviewers, always acknowledge the validity of the reviewers’ comments, and then either try to explain the issue in the response or modify the manuscript appropriately.&lt;/p>
&lt;/div>
&lt;div id="take-the-reviewer-comments-seriously" class="section level2">
&lt;h2>3. Take the reviewer comments seriously&lt;/h2>
&lt;p>It’s very easy to discount reviewer comments and say “the reviewer knows nothing about this topic.” Often the reviewer knows more than you may think, and you may simply not be understanding the reviewer’s point of view. (I’ve certainly reviewed more than one paper where I felt the authors were simply not getting what I was trying to tell them.) So make a serious effort and try to figure out what exactly it is the reviewer wants and how you can make it happen.&lt;/p>
&lt;/div>
&lt;div id="cite-every-reference-the-reviewers-mention" class="section level2">
&lt;h2>4. Cite every reference the reviewers mention&lt;/h2>
&lt;p>Sometimes it’s very clear that a reviewer wants you to cite a given paper while at other times it may seem like citing certain papers is optional. (Example: “In this context, the authors could consider citing Jones et al. 1975.”) Either way, cite all mentioned papers unless they are totally inappropriate. Regardless of whether the reviewer actually is Jones himself, or only is good friends with Jones, or simply thinks that the Jones et al. paper was a breakthrough for the field, the reviewer clearly cares for Jones et al. 1975. Therefore, he will have a little more respect for you if you demonstrate that you care for Jones et al. 1975 as well.&lt;/p>
&lt;/div>
&lt;div id="openly-admit-to-your-works-limitations-and-shortcomings" class="section level2">
&lt;h2>5. Openly admit to your work’s limitations and shortcomings&lt;/h2>
&lt;p>When reviewers point out that the research performed has certain shortcomings and limitations, junior scientists will often think they have to overcome these limitations before the work can be published. However, more often than not, all that is needed is a clear statement that these limitations exist and should be addressed in future work. Between this strategy and #4 (cite additional papers), you can probably handle at least 60-70% of all reviewer comments without doing any additional experiments or analysis.&lt;/p>
&lt;/div>
&lt;div id="understand-that-reviewer-comments-are-written-as-much-for-the-editor-as-they-are-for-you" class="section level2">
&lt;h2>6. Understand that reviewer comments are written as much for the editor as they are for you&lt;/h2>
&lt;p>The reviewer doesn’t just want to criticize your work, he also wants to make a good impression in front of the editor, who may be a close colleague, former advisor, or general heavyweight in the reviewer’s field of research. For this reason, the reviewer will always come up with at least a handful of points to criticize, just so he doesn’t appear lazy or incompetent. You will have to figure out which of the comments actually address crucial limitations of your paper and which were written just to impress the editor. The latter ones can always be dispatched with a combination of strategies #4 and #5.&lt;/p>
&lt;/div>
&lt;div id="say-no-to-excessive-requests" class="section level2">
&lt;h2>7. Say “No” to excessive requests&lt;/h2>
&lt;p>Finally, be aware that it is perfectly acceptable to not do certain things the reviewers ask for. Unless the validity of your core findings is at doubt, you always have the option of saying something like “these additional analyses are beyond the scope of the current work: or”we agree that the reviewer’s suggestion should be pursued in future work, and we now say so in the Discussion." In case of doubt, don’t do the extra work, just say “No”.&lt;/p>
&lt;p>&lt;strong>Update 12/18/2014:&lt;/strong> Also read &lt;a href="https://sevimcengiz.github.io/blog/2014/12/18/how-to-prepare-an-article-for-resubmission-part-ii">my follow-up post&lt;/a> on this topic.&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>In this story, the editor is female and the reviewers are male.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>There’s a reason for the specific color choices I suggest. Your goal is to visually separate the reviewer comments from the responses. You could do this by making either the reviewer comments or the responses bold or italics. However, extended sections in italics tend to be hard to read, and extended sections in bold tend to be jarring. So colors are the best option. In your color choice, keep in mind that your responses need to be more visually present than the reviewer comments, because you want the reviewers and the editor to focus on your responses, not the reviewer comments, when they evaluate your revision. So your responses need to be in black, and the reviewer comments need to be in a color that doesn’t stand out relative to black. Blue is a good choice. Maybe green or gray would also work. Red or yellow would probably be bad choices.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>The Google Scholar preprint bug</title><link>https://sevimcengiz.github.io/blog/2014/11/01/the-google-scholar-preprint-bug/</link><pubDate>Sat, 01 Nov 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/11/01/the-google-scholar-preprint-bug/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Google Scholar has a serious bug when it comes to preprints. If you have published a preprint of your paper, the later journal publication can be completely invisible to Google Scholar, seemingly absent from their entire database. Even a search for the exact article title will not find the article. And this condition remains for months. (It will eventually fix itself, though. After about a year.) I have now seen this bug in action for several of my papers, and I am confident it’s a reproducible flaw and not a one-off. I reported the issue to the Google Scholar team about a year ago (or at least, I filled in some web form that seemed to be designed to send them feedback) but I have received no response and the bug clearly still exists. I hope that with this blog-post I can draw some attention to this serious issue, so we can have it fixed. Thousands of scientists rely on Google Scholar every day. For many recent articles, this bug will steer these scientists towards outdated, early versions and make the authoritative article versions completely inaccessible.&lt;/p>
&lt;p>As far as I can tell, what triggers the bug for sure is the publication of an updated version of the same article with a changed title or author list. It is possible that the same bug occurs even when title or author list hasn’t changed, but I haven’t noticed it under those conditions. Here is an example of this bug in action. My lab published the following article on BioRxiv on April 24, 2014:&lt;/p>
&lt;blockquote>
&lt;p>Amir Shahmoradi, Dariya K. Sydykova, Stephanie J. Spielman, Eleisha L. Jackson, Eric T. Dawson, Austin G.Meyer, Claus O. Wilke. Predicting evolutionary site variability from structure in viral proteins: buriedness, flexibility, and design. &lt;a href="https://www.biorxiv.org/content/10.1101/004481v1">doi:10.1101/004481&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>If you click on the link, you’ll see that BioRxiv encourages you to check out the latest version of the article, with slightly different title, posted on July 21, 2014:&lt;/p>
&lt;blockquote>
&lt;p>Amir Shahmoradi, Dariya K. Sydykova, Stephanie J. Spielman, Eleisha L. Jackson, Eric T. Dawson, Austin G.Meyer, Claus O. Wilke. Predicting evolutionary site variability from structure in viral proteins: buriedness, packing, flexibility, and design. &lt;a href="https://doi.org/10.1101/004481">doi:10.1101/004481&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>The doi is the same, so clearly those are two subsequent versions of the same article. BioRxiv also knows that the article was published in its final form by the Journal of Molecular Evolution under &lt;a href="https://doi.org/10.1007/s00239-014-9644-x">doi:10.1007/s00239-014-9644-x&lt;/a> on September 13, 2014. Finally, the article has a PubMed entry (&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/25217382">PMID: 25217382&lt;/a>) and if you do a normal Google search using the complete, final article title the &lt;a href="https://www.google.com/search?q=Predicting+evolutionary+site+variability+from+structure+in+viral+proteins%3A+buriedness%2C+packing%2C+flexibility%2C+and+design&amp;amp;oq=Predicting+evolutionary+site+variability+from+structure+in+viral+proteins%3A+buriedness%2C+packing%2C+flexibility%2C+and+design&amp;amp;aqs=chrome..69i57j69i59j69i60l3.549j0j1&amp;amp;sourceid=chrome&amp;amp;es_sm=91&amp;amp;ie=UTF-8">top three hits&lt;/a> are two different preprint postings and the final journal article.&lt;/p>
&lt;p>Now that we have established that the article clearly exists, has been published in its final form for over three months (since July 21, 2014), and is known to regular Google, let’s try to find it on Google Scholar. First, we search for the exact title. Here is the result:&lt;/p>
&lt;p>&lt;img src="Google_Scholar_screen_shot.png" />&lt;/p>
&lt;p>Yes, Google Scholar suggests to me that I misspelled the title and meant to type “busineses” instead of “buriedness.” Importantly, it doesn’t find the correct, latest article! It does find the preprint, though, as the second hit. If I click on “all 8 versions” for the preprint, I get all sorts of different versions of the preprint, but the &lt;a href="https://scholar.google.com/scholar?cluster=1799163645855055780">final, published article is not found&lt;/a> as of this writing (Nov. 1, 2014). The latest version also doesn’t show up among my 2014 publications in my &lt;a href="https://scholar.google.com/citations?hl=en&amp;amp;user=Nc8U6E4AAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate">Google Scholar profile,&lt;/a> only the first preprint version is shown. For all intents and purposes, the July 21 and September 13 versions of the article are completely missing from the Scholar database. I cannot retrieve them in any way through Scholar. Let me say this one more time: We’re not simply talking about Google Scholar creating multiple redundant entries and not noticing that two articles are just different versions of the same article. &lt;strong>We’re talking about an earlier article completely hiding all later versions.&lt;/strong> Note that, as I said at the beginning of the post, this issue will correct itself eventually. The paper for which I first noticed this is now, after a year, correctly indexed in Google Scholar.&lt;/p>
&lt;p>To demonstrate that this is not a one-off, consider &lt;a href="https://doi.org/10.1101/002287">this article,&lt;/a> posted in revised version on October 14. I’ll leave it as an exercise to the reader to verify that this article exists, is known to regular Google, and is completely invisible in Google Scholar.&lt;/p>
&lt;p>In my mind, this is a serious bug that severely interferes with efficient scientific communication. It also is a huge embarrassment for a company that prides itself of being the world’s leading expert in information retrieval. I hope this post will give this bug more visibility, and will eventually lead the Scholar Team to fix the problem.&lt;/p>
&lt;p>&lt;strong>Update:&lt;/strong> In case you’re wondering if this is just an issue of Google Scholar not having indexed the latest issues of J. Mol. Evol. and/or BiorRxiv yet, that is clearly not the case. Other articles from the &lt;a href="https://link.springer.com/journal/239/79/3/page/1">same J. Mol. Evol. issue&lt;/a> can be found by a search for their title, e.g. &lt;a href="https://scholar.google.com/scholar?hl=en&amp;amp;q=Limits+of+Neutral+Drift%3A+Lessons+From+the+In+Vitro+Evolution+of+Two+Ribozymes">this one&lt;/a> or &lt;a href="https://scholar.google.com/scholar?hl=en&amp;amp;q=One+origin+for+metallo-%CE%B2-lactamase+activity%2C+or+two%3F+An+investigation+assessing+a+diverse+set+of+reconstructed+ancestral+sequences+based+on+a+sample+of+phylogenetic+trees">this one&lt;/a> or &lt;a href="https://scholar.google.com/scholar?q=Essential+is+Not+Irreplaceable%3A+Fitness+Dynamics+of+Experimental+E.+coli+RNase+P+RNA+Heterologous+Replacement">this one&lt;/a>.&lt;/p></description></item><item><title>Should peer-review be double-blind?</title><link>https://sevimcengiz.github.io/blog/2014/10/18/should-peer-review-be-double-blind/</link><pubDate>Sat, 18 Oct 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/10/18/should-peer-review-be-double-blind/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>As part of the &lt;a href="https://sevimcengiz.github.io/blog/2014/10/12/in-defense-of-anonymous-peer-review">recent discussion on anonymous peer review,&lt;/a> several people spoke out in favor of double-blind peer review, where neither the authors nor the reviewers know who the others are. I have thought a lot about double-blind peer review, and I’m not entirely convinced, in particular when it comes to grant applications. While double-blind review might solve certain problems and remove certain biases, it would almost certainly amplify other issues, and whether the net effect would be good or bad is unclear. It would also give more power to people such as editors and program managers who operate outside the blinded process.&lt;/p>
&lt;p>So let’s discuss. I’ll first cover journal articles and then grant proposals. The two are very different, and what applies to one does not necessarily apply to the other.&lt;/p>
&lt;div id="journal-articles" class="section level2">
&lt;h2>Journal articles&lt;/h2>
&lt;p>On the face of it, double-blind peer review for journal articles seems like a no-brainer. It allows junior scientists to be judged on the merit of their work alone, and it prevents senior scientists from coasting through peer review on the basis of their good name. However, as always, the devil is in the detail. There are at least three reasons I can think of why double-blind review may not be such a great idea after all.&lt;/p>
&lt;p>First, the real power is with the editors, not the reviewers. It’s the editors who make the final decisions. And this power tends to increase with the perceived rank of the journal; editors of more prestigious journals are more likely to reject papers without review or because of a perceived lack of interest. We all know that you won’t publish in &lt;em>Nature&lt;/em> if the &lt;em>Nature&lt;/em> editors don’t like your work. And we also know that you can survive quite harsh reviewer criticisms if the &lt;em>Nature&lt;/em> editors really want to publish your work.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> Thus, one could make an argument that author names should be blinded to both reviewers and editors. But how would the editors invite unbiased reviewers if they don’t know who has a potential conflict of interest? The only practical way would be to have one editor invite the reviewers and another make the decision. This would be an interesting experiment, but I doubt any journal will go for it any time soon. Thus, as long as author names remain known to the handling editor, I doubt that double-blind review will make much of a difference.&lt;/p>
&lt;p>Second, blinding the authors of an article is incredibly hard. Just removing the author names from the first page of a paper will frequently not do. If the authors work on a unique study system, or make extensive use of their prior work, or have developed a specific software package, or have posted their data on a public repository, reviewers will likely be able to find out who they are. I once reviewed a paper where the authors had not only removed their names from the title page but also from citations to their own work in the reference list. I’m still amazed that somebody would (i) go to these lengths to conceal their identity and (ii) not realize that this action completely obliterated any anonymity they might otherwise have had. Double-blinding may create an illusion of anonymity where none actually exists.&lt;/p>
&lt;p>Third, double-blind review gives more power to scientists who are intent on submitting fraudulent work, because it is going to be harder for reviewers to identify patterns in the perpetrators’ activities. For example, I have my list of crooks with a solid portfolio on &lt;a href="http://retractionwatch.com/">Retraction Watch,&lt;/a> and when I get to review one of their papers I’ll be extra careful. These are usually scientists whose activities slipped by me the first time I reviewed one of their papers, because everything looked fine on the surface. If I regularly had to review their papers in a double-blind fashion, they would probably manage to slip by me more frequently.&lt;/p>
&lt;p>One benefit of double-blind peer review, however, could be that even if the reviewers have a sense of which lab(s) may have been involved in a given study, they still won’t be able to guess the exact author list. For example, I don’t know to what extent reviewers are biased by the gender of the first author if a paper comes from an established lab, but if they are, that bias would likely disappear in double-blind review. Reviewers may guess correctly that the paper comes out of my lab, but they won’t know which of my students wrote it. Evidence in favor of this notion comes from one experiment in which double-blind peer review &lt;a href="http://blogs.nature.com/peer-to-peer/2008/01/doubleblind_peer_review_reveal.html">increased the number of female first authors.&lt;/a> Similarly, when junior PIs continue working on research they begun in an established person’s lab, reviewers won’t be able to tell whether the paper comes from the established lab or the junior PI. This could work to the advantage of junior PIs.&lt;/p>
&lt;p>In summary, the positive and the negative aspects of double-blind review are about even, in my opinion. I have no major concerns about double-blind review, as long as I as an author am not expected to do anything more than remove my name from the author list. I’m not interested in going through my entire paper and making sure not a single sentence (e.g. “We have previously investigated…”) could give a hint at who I am. Also, we now frequently make all our data and code available in a github repository, and I’m not going to go through extra effort to conceal who I am there. Other than that, I’d be happy to support more experiments in double-blind peer review, and I’ll also be happy to support double-blind peer review more strongly if evidence in its favor continues to accumulate.&lt;/p>
&lt;/div>
&lt;div id="grant-proposals" class="section level2">
&lt;h2>Grant proposals&lt;/h2>
&lt;p>Grant proposals are an entirely different beast than journal articles, and I do not think that double-blind proposal review is a good idea. There is a fundamental difference between a journal article and a proposal. An article is the finished product. A grant proposal, by contrast, is only the promise of a future product. If one scientist writes ten times more high-profile papers than another, then she should publish ten times more frequently in high-profile journals, without question. However, just because one scientist is ten times better at writing grant proposals than another doesn’t mean he deserves ten times the funds. In fact, only if that scientist can write ten times as many papers or write papers that are ten times as important (however measured) would he deserve ten times the grant funding.&lt;/p>
&lt;p>I strongly believe that the track record of past performance needs to be considered in proposal review. If you had to hand one person a check over a million dollars, would you rather give the money to somebody who consistently delivers interesting results, even if that person’s grant application doesn’t sound overly exciting, or would you prefer to give the money to somebody who can tell a great story but about whom you know nothing beyond that story. This thought is related to the idea (&lt;a href="http://loop.nigms.nih.gov/2014/07/comment-on-proposed-pilot-to-support-nigms-investigators-overall-research-programs/">which is slowly sinking in with the NIH as well&lt;/a>) that it is generally better to fund people than projects, or at least to have a healthy mix of people-based and project-based funding. By definition, if you’re evaluating people, you cannot blind the evaluators to their identity.&lt;/p>
&lt;p>Now you could argue that the scientific review should be done blinded and the final funding decision be made by the program officer, who can take into account all the other relevant factors, such as track record, current funding of the applicant, etc. However, this would simply put more power into the hands of program officers, who might or might not use that power wisely. It’s certainly not unheard of for program officers in some agencies to preferentially fund their good buddies. The more power a program officer has to override a panel decision the more likely those situations are going to arise.&lt;/p>
&lt;p>Double-blind grant review is also open to several sorts of manipulation by applicants. First, it would be easier than it already is to base an application on dubious, sketchy, or even entirely made-up data, because nobody would ever know.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> Second, applicants could pack their proposals with prior results obtained by the biggest shot in the field, causing the reviewers to think they’re reviewing an application by that lab and rank it higher because of that. You might say that that’s exactly the point, only ideas matter, but I’ve seen too many scientists with great ideas and poor execution to feel comfortable with funding decisions based exclusively on ideas.&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>&lt;/p>
&lt;p>In conclusion, I don’t think that double-blind grant applications are the way to go. There are other ways to minimize biases in the review process. For example, panels could be given statistics on how many women and junior scientists submitted applications to a given competition, and if the composition of the top-ranked applicants deviates substantially from the overall composition of applicants then the panel could be asked to reconsider their rankings. In general, just paying attention to these kinds of biases and monitoring whether certain groups of applicants are disproportionally affected by either positive or negative decisions should prevent the most egregious biases.&lt;/p>
&lt;p>&lt;strong>Update (10/19/2014):&lt;/strong> The article I quoted &lt;a href="http://blogs.nature.com/peer-to-peer/2008/01/doubleblind_peer_review_reveal.html">claiming an increased number of female first authors&lt;/a> under double-blind review has later &lt;a href="http://www.nature.com/news/2008/080604/full/453711c.html">been called into question,&lt;/a> as comparable journals have similarly seen an increase in the number of female first authors during the same time period, without instituting double-blind review. Thanks to &lt;a href="https://twitter.com/mattjhodgkinson">Matt Hodgkinson&lt;/a> for pointing this out.&lt;/p>
&lt;p>&lt;strong>Update #2 (10/19/2014):&lt;/strong> &lt;a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.22784/full">This paper,&lt;/a> pointed out to me by Matt Hodgkinson, provides a thorough review of what is currently known about biases in peer review. It shows mixed evidence on gender bias. In particular with respect to journal articles, current evidence suggests bias isn’t that pronounced (female and male authors have comparable acceptance rates).&lt;/p>
&lt;/div>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>I certainly have reviewed papers for &lt;em>Nature&lt;/em> that I thought should not be published there and the editors overruled me. And this is fine; editors should have the ultimate decision power. I’m an editor myself, and on occasion I accept papers that reviewers say should be rejected. The point remains, though, that an editor who really wants to publish a paper will rarely be deterred by negative reviews, in particular if the reviews don’t call out egregious errors in the work.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>Even under the current system of non-blinded review, grant applicants can include sketchy or made-up data with little risk to their career or reputation. While such activity is obviously fraudulent and will have severe consequences if discovered, the likelihood of discovery is low. First, only three to five other scientists ever see the application, and only for a short period of time. So if an applicant, for example, reuses the same data set in subsequent applications but labels the resulting figure differently, it’s very unlikely anybody would notice. Similarly, if an applicant claims preliminary data support one hypothesis and later publishes a paper supporting a different hypothesis, he could always argue that that is indeed how discovery went: first things looked one way but after more careful study it became clear the other way was right. Only the most blatantly obvious fraud, such as publishing fraudulent data in a paper and then using that paper as preliminary results in an application, has any likelihood of being discovered. For these reasons, a colleague of mine here at UT thinks that results that aren’t published or maybe at least deposited on a public archive should not be allowed in grant applications at all.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>As with everything in life, I think some balance is required here. Grant applicants should have to demonstrate some amount of prior expertise in the work they propose, but they should also be given the benefit of the doubt that some things can be worked out as the research is done.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Double Jeopardy</title><link>https://sevimcengiz.github.io/blog/2014/09/13/double-jeopardy/</link><pubDate>Sat, 13 Sep 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/09/13/double-jeopardy/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>Lately it keeps happening to me that I try to invite somebody to review a paper and they decline, giving as reason that they have reviewed the paper already for a different journal&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> and reviewing the paper again would put the authors into a situation of double jeopardy. This got me thinking. Should reviewers really decline for that reason? As reviewer, I’ve always thought the opposite. For a paper I have reviewed already, if the authors have made a reasonable effort to address my comments and have now chosen a more adequate journal, I can keep my review short and recommend acceptance. Thus, I’m actually preventing a situation of double jeopardy. I keep the authors from facing yet another reviewer with new opinions and requests. So, which is right? Should reviewers recuse themselves if they are asked to review again for a different journal, or should they instead leap at the opportunity and give the authors a break? I’d be interested in your thoughts.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>Where the paper was rejected, presumably.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How glamour journals rose to prominence, and why they may not be needed anymore</title><link>https://sevimcengiz.github.io/blog/2014/01/04/how-glamour-journals-rose-to-prominence-and-why-they-may-not-be-needed-anymore/</link><pubDate>Sat, 04 Jan 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/01/04/how-glamour-journals-rose-to-prominence-and-why-they-may-not-be-needed-anymore/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>In the ongoing discussion about the value of glamour journals such as Nature, Science, and Cell, I think it’s worth looking back and asking: “How did they rise to prominence?” and “Are they still serving the same role they did when they arose?” So let’s take a quick trip into the history of science communication, before the internet. Then we can ask what the internet has changed, and how we could make the best of current technology.&lt;/p>
&lt;p>I belong to the last generation of scientists that experienced science before the internet. I started doing research as an undergrad in 1995. At that time, I saw a web browser for the first time, and I sent my first email. While the internet had been around for a while by 1995,&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> its use was still very limited, and barely anybody outside academia had ever even heard of it. All this would change over the next 4-5 years, and by 2000 the internet started to become ubiquitous.&lt;/p>
&lt;p>I don’t think anybody who got into science after the year 2000 can imagine what keeping up with the literature was like before the internet. I started my postdoc in 2000. During my entire postdoc time (or since), I rarely ever went into a library. By that time, most journals had a solid online presence, including back issues. Articles that weren’t available online could be requested via inter-library loan, and they would arrive electronically. By contrast, during my PhD, I spent a lot of time at the library. I would make weekly trips to browse the latest issues of the scientific journals I was interested in. Because I was working at the interface between physics and biology, I had to visit the physics library and the biology library. For certain articles I also had to go to the chemistry library. I knew exactly which library had which journals, and which journals were available in multiple libraries. (Almost all libraries had Science, for example.) To figure out whether anybody was citing a particular paper, I had to confer with the &lt;a href="http://en.wikipedia.org/wiki/Science_Citation_Index">Science Citation Index,&lt;/a> which was a big book available at some libraries. For any article of interest, it would list by which other articles it had recently been cited. Invariably, the list of citing articles would contain articles in journals that were only available in a different library on campus, or not in any library at my university. So, after reading the Science Citation Index, I’d make the trip to a different library, or submit an inter-library loan request, or make a note for my next scheduled trip to a different library to look up a particular article. In the worst-case scenario, it could take weeks until I saw a particular article, and then I’d often find out the article wasn’t really relevant to what I was doing.&lt;/p>
&lt;p>Compare this to how literature search works today. I look up an article on Google Scholar, click on “Cited by” or “Related articles,” and find relevant related articles in seconds. For every article listed, I can get at least the title and the abstract, and for the vast majority of articles I can get the full text, all within a few seconds and while sitting at home on my couch. I can similarly browse any open-access journal, everything on Pubmed Central, and any paywalled journal my university has access to, from everywhere in the world. For all intents and purposes, the second I know a particular article exists, I can look it up and read it.&lt;/p>
&lt;p>What has any of this to do with glamour journals? I’m going to argue that in the time before the internet, glamour journals and other highly selective journals served a crucial role. In a world where looking up a reference can take between days and weeks, finding potentially interesting &lt;em>references&lt;/em> is much less valuable than finding potentially interesting &lt;em>articles.&lt;/em> Yes, you would go to the trouble of hunting down a particular article if it seemed directly relevant for your own work, but you certainly wouldn’t do so just to generally keep up with a broader field, much less all of science. Therefore, reading a journal such as Science or Nature, or even a more specialized but still fairly selective journal such as Genetics, was the only way to keep up with scientific progress. You went to the library, you took the physical copy of the journal, you browsed through it, you read the interesting articles, you learned something useful, you went home/back to your office.&lt;/p>
&lt;p>By contrast, these days, with nearly any article right at our fingertips, the process of publishing articles and of selecting articles can be decoupled. For example, I don’t consistently browse through the tables of contents of Nature or Science anymore, because I now have other means of discovering interesting articles. Any interesting article in my field I’ll come across sooner or later because Google Scholar recommends it to me, or somebody tells me about it in person, or it gets cited in a paper I read or review. For generally interesting articles, say the latest findings about global warming, I’ll likely see them mentioned on Twitter or Reddit. The advantage of all these methods of finding articles over browsing through tables of contents is that I’m not tied to the venue in which the article was published. I’m just as likely to come across an interesting article published in Nature as one in PLOS ONE. And the moment I learn about an article, I can read it. But imagine a print version of Twitter, in the time before the internet. If I had received per mail, once a week, a list of potentially interesting things published in the most random venues, I would never have followed up with reading any of them. The barrier to doing so would simply have been too high.&lt;/p>
&lt;p>Some people take this reasoning to the extreme and argue that since now everything is easily available online and search engines are powerful, we don’t need selective journals anymore at all. The best science will rise to the top, it will be cited, tweeted, mentioned on reddit, bloggers will write posts about it, and thus we might as well publish everything in PLOS ONE. I am not entirely convinced by this argument, for the following reason: It’s all well and good if other people cite and tweet your work, but what if they don’t? If you think you have done some really outstanding work, work that deserves more attention than your regular bread-and-butter efforts do, in a world where all science is published in PLOS ONE, what options to you have? In a world that has glamour journals, it’s of course obvious what you can do: You write a nice 3-6 page summary of your work, highlighting the most important findings and the broad relevance, you send it to one or more glamour journals, and you hope for the best. If you get through, you’ll have a much higher chance of getting your article cited, tweeted, etc., because people pay attention to the glamour journals, and they like to read short, clearly written articles that highlight key findings and broader relevance. But if there are no glamour journals, then you have no good option of indicating that in your own opionion, this article is more valuable than that article.&lt;/p>
&lt;p>So let me summarize the facts: (i) In the world of the internet, it doesn’t matter where something is physically published, as long as it is easily accessible through a URL. (ii) Glamour journals have lost the original purpose of making important science easily and broadly accessible. (iii) Publication venues that highlight interesting work by commenting and/or linking to it (such as Twitter, Reddit, Nature News and Views, Google Scholar, etc.) are highly valuable. (iv) Short, clearly written articles highlighting key insights and broader relevance are appreciated and highly valuable. (v) Authors have an interest in pointing out what they think are their most important works.&lt;/p>
&lt;p>These facts lead me to the following proposal: Let’s take all original science out of the glamour journals. Instead, allow authors to submit short summaries (maybe 2-3 pages) of work they have already published elsewhere, e.g. in PLOS ONE. These summaries would be reviewed editorially, and also by one or two expert scientists who’d be asked to judge whether the original article appears to be scientifically sound and noteworthy. Editors might reject a summary because it isn’t deemed sufficiently interesting or novel, and there would still be fighting and politicing about getting summaries into these glamour journals, but the system would relieve authors of several pressures: (i) Authors wouldn’t have to rewrite an article multiple times just to hope to get it published eventually in one of the selective journals. (ii) Authors could get their results out and cite them properly while still trying for that glamour slot. (iii) Since the original article of record would be in PLOS ONE or PeerJ or similar, it would become generally accepted to have even the most important work published in these journals. Nobody could look at a publication list and say “Oh, it’s just a bunch of PLOS ONE papers.” (iv) Hiring committees and granting agencies would still have the option to evaluate candidates by the number of summaries they have published in glamour journals, though I would hope they would do so to a lesser degree and pay more attention to the original articles.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>The first web browser, which started the development of the modern internet, &lt;a href="http://en.wikipedia.org/wiki/Mosaic_web_browser">was released in 1993.&lt;/a> The concept of an online journal was unthinkable before the invention of the web browser.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Is there an avalanche of low-quality research, and if so, must we stop it?</title><link>https://sevimcengiz.github.io/blog/2013/12/21/is-there-an-avalanche-of-low-quality-research-and-if-so-must-we-stop-it/</link><pubDate>Sat, 21 Dec 2013 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2013/12/21/is-there-an-avalanche-of-low-quality-research-and-if-so-must-we-stop-it/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>&lt;strong>Update:&lt;/strong> &lt;em>It turns out the article in the Chronicle is not recent, I misread the date on the page. (The Chronicle has two dates on each page, today’s date and the article publication date.) I stand by everything else I say, though.&lt;/em>&lt;/p>
&lt;p>A recent article in the Chronicle of Higher Education argues that &lt;a href="http://chronicle.com/article/We-Must-Stop-the-Avalanche-of/65890/">“we must stop the avalanche of low-quality research.”&lt;/a> The authors decry the rapid growth of the scientific literature, which (as they argue) puts increasing strain on readers, reviewers, and editors without producing much benefit. They argue that this growth is driven by an increasing pressure on scientists to publish more, and the result is increasing amounts of low-quality publications. To address the pressure on scientists, they propose three fixes, of which one is Ok and two are positively inane. Maybe what we really have to stop is the avalanche of low-quality, non-reviewed opinion pieces published on web pages?&lt;/p>
&lt;p>Reading through the article, I found it difficult not to wonder whether the authors had ever heard of the internet or of modern information-processing technology (e.g., Google). Now, to be fair, none of the authors are in the natural sciences. The authors work in English, mechanical engineering, medicine, management, and geography. I don’t really know these areas. My own work is in biology, and I’m also somewhat familiar with the publishing cultures in physics and in computer science. So, everything they say may make sense in their fields, but it doesn’t in mine. I’m not convinced we have a major crisis, and I certainly don’t think their proposed fixes are any good. Let’s take a look at their proposed solutions first.&lt;/p>
&lt;div id="limit-number-of-papers-submitted-for-job-applications-or-promotions" class="section level2">
&lt;h2>Limit number of papers submitted for job applications or promotions&lt;/h2>
&lt;p>The first fix, to limit the number of papers that applicants are allowed to submit for job applications or promotions, is actually somewhat reasonable. Applicants should be judged on the quality of their work, not on the mere quantity of output. However, I am strongly opposed to saying applicants are not even allowed to mention anything beyond their key 3-5 papers. Why should productivity be punished? What if they wrote 10 important papers? Should Ed Witten be limited to list only 5 papers? (To date, he has written &lt;a href="http://scholar.google.com/scholar?hl=en&amp;amp;q=edward+witten">over 30 papers with over 1000 citations each!&lt;/a>) While there are negative outliers in academia, people who produce huge amounts of mindless drivel, I definitely see a correlation between quantity and quality. The most interesting and influential papers are generally written by the most productive researchers. I have previously given arguments for why we would &lt;a href="https://sevimcengiz.github.io/blog/2013/11/3/no-one-reads-your-paper-either">expect such a correlation to exist.&lt;/a>&lt;/p>
&lt;p>Most job search and promotion processes that I am aware of have already found a solution to this problem, by asking applicants to submit both (i) a full list of all publications and (ii) the 3-5 most important papers, possibly with a statement explaining their impact. This is good practice that strikes a balance between quality and quantity, it allows applicants to showcase both how good they are and how consistently productive they are, and most importantly, it is already common practice. So point 1 is a non-issue, from where I stand.&lt;/p>
&lt;/div>
&lt;div id="evaluate-researchers-by-impact-factors" class="section level2">
&lt;h2>Evaluate researchers by impact factors&lt;/h2>
&lt;p>Evaluating researchers by impact factor is such an absurd and untimely suggestion, I can’t help but wonder whether the authors have been living under a rock for the last 10 years. It’s particularly ironic that the Chronicle of Higher Education would publish this statement a mere 11 days after nobel-prize winner Randy Schekman publicly proclaimed that luxury (i.e., high impact-factor) journals such as Nature, Cell, and Science &lt;a href="http://www.theguardian.com/commentisfree/2013/dec/09/how-journals-nature-science-cell-damage-science">“are damaging science.”&lt;/a> Did the authors really not see this article, &lt;a href="http://scholarlykitchen.sspnet.org/2013/12/11/this-takes-the-prize-editor-of-new-luxury-oa-journal-boycotts-luxury-subscription-journals/">nor the widespread outrage it caused over containing a cheap plug for a different luxury journal?&lt;/a> If there is one problem we have in science right now, at least in the biomedical field, it’s an over-reliance on impact factors and publications in high-profile journals. The outcry over Schekman’s article shows how sensitive of an issue this is, and how many scientists are concerned about the growing pressure to publish in only the highest-impact journals. Schekman himself addresses this in &lt;a href="http://theconversation.com/how-to-break-free-from-the-stifling-grip-of-luxury-journals-21669">his response to the criticism he received.&lt;/a> Scientists should be judged on the quality of their work, not on whether or not they published in Nature.&lt;/p>
&lt;/div>
&lt;div id="limit-the-length-of-papers-published" class="section level2">
&lt;h2>Limit the length of papers published&lt;/h2>
&lt;p>I don’t see how imposing page limits connects at all to the issue at hand. Surely, if we want fewer but higher-quality publications, the papers should be longer not shorter. Also, I strongly oppose to the split model with a brief (4-6 page) main article (i.e., advertisement) accompanied by longer supporting materials. Invariably, the supporting materials are not written as carefully as the main article, and the quality of the paper as a whole suffers. Notably, PNAS just went the other direction, and now allows papers of up to 10 pages in length in their online-only PNAS Plus edition. This was a very welcome change, I think. The 6-page limit of PNAS was often too limiting, whereas most articles fit comfortably within 10 pages.&lt;/p>
&lt;/div>
&lt;div id="is-there-too-much-pressure-to-publish" class="section level2">
&lt;h2>Is there too much pressure to publish?&lt;/h2>
&lt;p>While there is pressure to publish, frankly I don’t see that there is &lt;em>excessive&lt;/em> pressure to publish. From what I see, for example in conversations with colleagues, the common expectation is reasonable productivity both in terms of quantity and in terms of quality. In terms of quantity, reasonable is usually a number between 1 and 10 papers per year. Publish less, and people start wondering whether you’re working consistently, and in particular whether you’ll keep working in the future. Publish much more than 10 papers per year, and people start looking at you suspiciously. I sat on a grant-review panel once where one applicant claimed his previous 3-year NSF grant had led to ~100 publications. People were very suspicious of this claim and the grant did not get good reviews, even though the science seemed to be reasonable. (I’m not saying the proposal would have been funded if the applicant had had fewer publications, but the high number certainly didn’t help; if it had any effect it was a negative one.) In most areas of Biology, I think 2-3 papers a year will be considered perfectly reasonable for anybody but a senior PI running a large lab. (This includes all papers with your name on, not just first-author papers.)&lt;/p>
&lt;p>In terms of quality, I stick to my earlier recommendation: &lt;a href="https://sevimcengiz.github.io/blog/2013/11/3/no-one-reads-your-paper-either">publish at least one paper a year that has some real substance.&lt;/a> Where exactly that paper is published is secondary, I believe. Publishing the occasional high-profile article in a luxury journal can’t hurt, but I hope that we as scientists can collectively learn to pay a little less attention to where something is published and pay more attention to the content. We shouldn’t hire somebody without having carefully read at least one or two of their papers, and I think the more diligent search committees operate like that already.&lt;/p>
&lt;p>With regards to excessive workload for editors and reviewers, I think there are several things that could be done relatively easily:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>Institute a system of reviewing credits, where you receive one credit for each article you review and you have to spend a number of credits (e.g. 6) to submit an article. This would ensure that everybody who publishes carries their fair share on the reviewing side.&lt;/p>&lt;/li>
&lt;li>&lt;p>Have more graduate students and postdocs review papers. Not every paper needs to be reviewed by three members of the NAS. In fact, I often find that graduate students write better reviews than senior scientists do, because the graduate students take the job much more seriously and put way more effort into it than an established scientist normally would.&lt;/p>&lt;/li>
&lt;li>&lt;p>Have less stringent reviewing criteria, don’t judge impact. Much of the excessive reviewing load actually comes from the pressure to publish in highly selective journals. Thus, many articles make the mandatory trek from Science to Nature to PNAS to PLOS Genetics to PLOS ONE, possibly undergoing four or more separate rounds of review. It’s not uncommon for me to review the same article several times for different journals. And in the end, everything gets published anyway, somewhere. If it was the reviewers’ job to only look for major scientific flaws, then most articles could be published after 1-2 rounds of review, cutting the total review burden way down.&lt;/p>&lt;/li>
&lt;li>&lt;p>Improve tools for post-publication evaluation of articles. At present, all we have is citations and word-of-mouth. (“Have you seen the latest paper by X in PNAS? It’s really not very good.”) I’m sure we can do better than that, and over time we’ll find ways to put modern computing power and crowd-sourcing ideas to good use. &lt;a href="http://www.the-scientist.com/?articles.view/articleNo/37969/title/Post-Publication-Peer-Review-Mainstreamed/">NCBI’s PubMed Commons is a first step in this direction.&lt;/a> I’m sure over the next 10-20 years we’ll see many more innovative ideas to evaluate the quality of scientific work post publication.&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>The value of pre-publication peer review</title><link>https://sevimcengiz.github.io/blog/2013/12/21/the-value-of-pre-publication-peer-review/</link><pubDate>Sat, 21 Dec 2013 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2013/12/21/the-value-of-pre-publication-peer-review/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>I see lot of discussion these days about the value of peer review. Are journals too selective? Are acceptance decisions arbitrary? Does peer review actually catch scientific mistakes or fraudulent practices? Wouldn’t it be better to just put everything out there, say on preprint servers, and separate the wheat from the chaff in post-publication review? I’m not quite ready yet to give up on pre-publication peer review. I think it serves a useful purpose, one I wouldn’t want to do away with. In the following, I discuss four distinct services that peer review provides, and assess the value I personally assign to each of them.&lt;/p>
&lt;div id="peer-review-screens-out-nonsense-and-pseudoscience" class="section level2">
&lt;h2>Peer review screens out nonsense and pseudoscience&lt;/h2>
&lt;p>It’s important that somebody screen all potential scientific publications for actual scientific content. I don’t mind publishing null results, replication studies, or studies that present only a very minor advance. All of these works contain real science, and they may find some use at some point in the future. However, we must never mix science with pseudoscience. Someone has to assure that whatever gets published in a scientific journal is not complete nonsense. Even the preprint archive arxiv.org has &lt;a href="http://arxiv.org/help/endorsement">some sort of a screening and filtering system in place to hold back the crackpots.&lt;/a> In most cases, nonsensical papers would be caught by the editor and not even sent out to review. Nevertheless, we can consider filtering out nonsense to be an essential service of the pre-publication review process.&lt;/p>
&lt;/div>
&lt;div id="peer-review-catches-major-mistakes-andor-fraud" class="section level2">
&lt;h2>Peer review catches major mistakes and/or fraud&lt;/h2>
&lt;p>Many people seem to think that it is the reviewers’ job to catch major mistakes and/or fraud. And when they fail to do so, that is taken as evidence that peer review doesn’t work. I don’t think we can put such a high burden on the reviewers. Ultimately, the burden of producing correct and genuine results lies with the author. Peer review operates under the assumption that fundamentally the authors are honest and reasonably capable scientists. If peer review does happen to catch a major issue with a paper, that’s great, but generally I think that post-publication review is the much better venue to address major flaws or scientific misconduct.&lt;/p>
&lt;/div>
&lt;div id="peer-review-assess-novelty-potential-impact-and-fit-with-the-journal-scope" class="section level2">
&lt;h2>Peer review assess novelty, potential impact, and fit with the journal scope&lt;/h2>
&lt;p>Whether reviewers (or editors) should consider novelty and impact, and whether journals should be selective at all, is probably the most contentious issue in peer review. Traditionally, this has always been part of peer review. However, there are now several journals that explicitly state review should only assess scientific soundness (e.g. &lt;a href="http://www.plosone.org/static/information">PLOS ONE&lt;/a> or &lt;a href="https://peerj.com/about/aims-and-scope/">PeerJ&lt;/a>). I think there are valid arguments for both sides. On the one hand, it is imperative that we have publishing venues that will publish any scientifically sound study. Nobody benefits if a valid study is suppressed just because some reviewers didn’t find it interesting. If there’s no obvious scientific flaw, put it out there and let the readers (and Google) sort it out.&lt;/p>
&lt;p>On the other hand, I think that more selective journals can provide value as well. In my mind, where science has gone off-track is that the most selective journals (which are also considered to be the most prestigious ones, e.g. Nature, Science, Cell, PLOS Biology, PNAS) employ arbitrary selection criteria based primarily on the subjective goal of publishing “the best science.” As a consequence, whether I can publish in such journals depends much more on my marketing skills than on my scientific skills, and also on whether I’m working on a sexy study system.&lt;/p>
&lt;p>By contrast, the next lower tier of selective journals usually employ more objective selection criteria, and those arguably provide a useful value. For example, I’m an Associate Editor for PLOS Computational Biology, a fairly selective journal. The main requirement for publication in PLOS Computational Biology is &lt;a href="http://www.ploscompbiol.org/static/information">that you have produced high quality computational work that yields a novel biological insight.&lt;/a> In my mind, it is fairly straightforward to determine whether a paper satisfies that requirement or not. I also think that any capable computational biologist can jump over that bar. As a consequence, I feel that we’re providing useful selectivity without generating excessive artificial scarcity or making highly arbitrary decisions. If I see that somebody has on their CV a couple of PLOS Computational Biology papers, I can reasonably assume that they are doing consistent, high-quality computational work leading to novel insights into biological systems.&lt;/p>
&lt;/div>
&lt;div id="peer-review-helps-authors-improve-their-articles" class="section level2">
&lt;h2>Peer review helps authors improve their articles&lt;/h2>
&lt;p>In my mind, this last point is the most important point, and the reason why I’m not willing to give up pre-publication review in its entirety. In my experience as author, reviewer, and editor, the most common outcome of the review process other than “reject due to insufficient novelty” is “major revision.” The reviewers agree that the study has merit in principle, but they see a number of possible revisions that would improve the article. I have seen it countless times, both as author and as reviewer or editor, that a study was vastly improved after the first set of reviews. Sometimes reviewers catch an issue the authors hadn’t noticed, sometimes they have a really cool idea that brings the study to the next level, and sometimes they simply tell you that you have to work on your writing if you want to get your point across. Either way, this input is invaluable, and it improves the scientific literature tremendously. If we went to a system that operated entirely on post-publication review, we would probably still see the same kind of comments by reviewers, but there would be very little incentive for the authors to go and revise their papers accordingly.&lt;/p>
&lt;p>One downside to this aspect of peer review is that sometimes reviewers just keep insisting on changes that the authors don’t deem necessary or appropriate. This is another form of peer review gone wrong. The reviewers should make helpful suggestions, but they should not tell the authors how to write their paper. One solution to this issue is to make peer reviews public and leave with the authors the ultimate decision of whether or not they want to publish, as &lt;a href="http://www.biologydirect.com/about">Biology Direct does.&lt;/a> Another possibility is to allow authors to opt-out of re-review, as &lt;a href="http://www.biomedcentral.com/bmcbiol/about#publication">BMC Biology does.&lt;/a>&lt;/p>
&lt;/div></description></item></channel></rss>