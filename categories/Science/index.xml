<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Science on Sevim Cengiz</title><link>https://sevimcengiz.github.io/categories/Science/</link><description>Recent content in Science on Sevim Cengiz</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 24 Jan 2016 00:00:00 +0000</lastBuildDate><atom:link href="https://sevimcengiz.github.io/categories/Science/index.xml" rel="self" type="application/rss+xml"/><item><title>The one time I failed to parasitize an established clinical researcher</title><link>https://sevimcengiz.github.io/blog/2016/01/24/i-failed-to-parasitize-an-established-clinical-researcher/</link><pubDate>Sun, 24 Jan 2016 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2016/01/24/i-failed-to-parasitize-an-established-clinical-researcher/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>As regular readers of this blog probably know, I’m the paragon of a &lt;a href="https://twitter.com/hashtag/researchparasite">research parasite.&lt;/a> I’m a computational biologist, and all I ever do is publish my own analyses of other people’s data. Except that one time, a few years back, when a senior clinical researcher stopped me in my tracks. Thanks to his careful and guarded stewardship of his data, I have been saved from drawing incorrect conclusions from his data and from publicly embarrassing myself by claiming his analysis is complete nonsense.&lt;/p>
&lt;p>The story happened a few years back, when a senior clinical researcher (let’s call him X) published a paper claiming that egg-yolk consumption is nearly as bad for cardiovascular disease (CVD) as is smoking. As an avid egg lover, I wondered whether I should be concerned. Unfortunately, the manuscript was opaque and used statistical techniques I don’t trust (&lt;a href="https://sevimcengiz.github.io/blog/2013/8/18/common-errors-in-statistical-analyses">quantiling&lt;/a>), so I contacted X and asked to see the raw data. He made me state that I did not have any ties, financial or otherwise, to the egg industry (I don’t), and then he shared his raw data with me under the condition that I would keep the data confidential and that he would be a co-author on any resulting publications. I accepted these conditions and took a look at the data.&lt;/p>
&lt;p>I quickly realized that X and his colleagues had made several statistical mistakes, and that in fact the data exonerated eggs as a culprit for CVD. I analyzed the data using a variety of different approaches, just to be sure, but the simplest and most obvious one was a principal components analysis (PCA). It showed clearly that the egg-consumption axis was orthogonal to the age/smoking/CVD axis. (To this day, it’s one of the cleanest examples I’ve ever seen of a PCA revealing distinct underlying trends in the data.)&lt;/p>
&lt;p>I wrote a brief report and shared it with X, who forwarded it to a colleague of his in the biostatistics department. Let’s call him Y. The response I received was quite astonishing. (I’m paraphrasing from memory here.) First, Y said that “Claus Wilke is a strong student.” I was a tenured professor at the time. Second, Y said that “we don’t use PCA in clinical research.” Never mind that simple correlation tests, ordinary regressions, and regularized regressions all supported the results I had found with the PCA.&lt;/p>
&lt;p>Now, since X and Y thought I was a good student who however didn’t understand how statistics work in the clinical practice, and since I thought X’s paper was a bunch of nonsense, writing a joint paper on this topic was out of the question. That’s alright by me. I don’t need to publish in nutrition, and I’m fine with not having to worry about being assassinated by the egg-substitutes industry. However, X’s paper is still out there, receiving almost 20 citations a year. And, more importantly, this paper and its associated data would serve as an ideal teaching tool for the dangers (and power) of multivariate statistics. Alas, I’ll have to let it go. I should probably just start collecting my own data set of carotid plaque thickness in patients that have or have not eaten eggs for many years.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> If you’re wondering what all of this is about, read &lt;a href="http://www.nejm.org/doi/full/10.1056/NEJMe1516564">this editorial&lt;/a> in the New England Journal of Medicine. You may also want to read &lt;a href="http://jonathanpeelle.net/blog/2016/1/22/translation-to-plain-english-of-selected-portions-of-longo-and-drazens-editorial-on-data-sharing">this translation into English.&lt;/a>&lt;/p></description></item><item><title>Beyond bar and line graphs</title><link>https://sevimcengiz.github.io/blog/2015/04/29/beyond-bar-and-line-graphs/</link><pubDate>Wed, 29 Apr 2015 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2015/04/29/beyond-bar-and-line-graphs/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>PLOS Biology recently published a nice article on data visualization:&lt;/p>
&lt;blockquote>
&lt;p>Weissgerber TL, Milic NM, Winham SJ, Garovic VD (2015) Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm. PLOS Biol 13(4): e1002128. &lt;a href="https://doi.org/10.1371/journal.pbio.1002128">doi:10.1371/journal.pbio.1002128&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>It argues that we can do better than showing data as bars of mean height with error bars indicating standard deviations. In my lab, we use many of the proposed techniques already, see e.g. Fig. 4 in &lt;a href="https://peerj.com/articles/211/">this paper.&lt;/a> However, I’m glad that I now have a simple reference I can point people to when I want them to reconsider their figures.&lt;/p></description></item><item><title>A critique of Chatterjee et al., The Time Scale of Evolutionary Innovation, PLOS Comp. Biol. 2014.</title><link>https://sevimcengiz.github.io/blog/2014/09/24/a-critique-of-chatterjee-et-al-the-time-scale-of-evolutionary-innovation-plos-comp-biol-2014/</link><pubDate>Wed, 24 Sep 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/09/24/a-critique-of-chatterjee-et-al-the-time-scale-of-evolutionary-innovation-plos-comp-biol-2014/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>A paper published by PLOS Comp. Biol. this month, Chatterjee et al., &lt;a href="http://www.ploscompbiol.org/article/authors/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003818">The Time Scale of Evolutionary Innovation&lt;/a>, espouses ideas that are quite similar in spirit to long-standing creationist arguments. I said as much in a &lt;a href="https://twitter.com/ClausWilke/status/512774577076707328">few tweets.&lt;/a> After having made these comments, I have spent quite some time thinking this paper over. And I simply cannot convince myself that it makes an important contribution to evolutionary biology. It is possible that there’s something I’m missing, but to me the paper looks like a very convoluted and mathematically dense way of making a few tired, trivial, and maybe even tautological arguments.&lt;/p>
&lt;p>The paper, written by Krishnendu Chatterjee and Andreas Pavlogiannis of IST Austria and Ben Adlam and Martin Nowak from Harvard University, makes two fundamental claims: 1. Evolutionary adaptation needs exponential time to discover novel phenotypes. 2. An alternative process, the “regeneration process,” can discover novel phenotypes in polynomial time. Further, the paper claims that the regeneration process provides fundamental new insight into the process of evolutionary adaptation.&lt;/p>
&lt;p>I will break down my discussion of this paper into three parts: First, I’ll discuss the claim that evolutionary adaptation needs exponential time. Second, I’ll discuss the regeneration process. Third, I’ll ask whether this paper provides any novel biological insight. Let me also state upfront that this paper contains many pages of dense math, and I have not checked every equation in detail. In fact, I don’t think that’s necessary. I am happy to take the authors’ mathematical derivations at face value. What I’m discussing here are the assumptions the authors make going into their derivations and the conclusions they draw coming out.&lt;/p>
&lt;div id="does-evolutionary-adaptation-need-exponential-time" class="section level2">
&lt;h2>Does evolutionary adaptation need exponential time?&lt;/h2>
&lt;p>Whether or not evolutionary adaptation needs exponential time has been discussed since forever, because, in fact, this is one of the favorite topics of intelligent design creationists. Apparently, Chatterjee et al. are not aware of this discussion, because they cite neither the creationist arguments nor the counter-arguments by evolutionary biologists. I don’t really want to delve into the depths of this long-standing dispute. I’ll just point out the some of the key players and arguments. Then I’ll give a brief argument for why papers such as the current Chatterjee paper make no useful contribution to the question of whether or not evolution has had sufficient time to generate the life forms we see today.&lt;/p>
&lt;p>The exponential-time argument in its essence goes back to the “junkyard tornado” metaphor originally &lt;a href="http://en.wikipedia.org/wiki/Junkyard_tornado">coined by Hoyle&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>The chance that higher life forms might have emerged in this way is comparable to the chance that a tornado sweeping through a junkyard might assemble a Boeing 747 from the materials therein.&lt;/p>
&lt;/blockquote>
&lt;p>The argument was subsequently made more sophisticated by Dembski in his book “No Free Lunch.” In this book, Dembski employs so-called “No-Free-Lunch” (NFL) theorems to argue that evolutionary search cannot be more efficient than just trying solutions at random. NFL theorems were discovered in the field of machine learning, and they state that all search algorithms perform equally poorly when averaged over all possible search spaces (i.e., fitness landscapes). When applied to evolution, the argument becomes that evolutionary adaptation is just as efficient (or rather, inefficient) at finding solutions as is the junkyard tornado. The main counter argument is that &lt;a href="http://www.talkorigins.org/design/faqs/nfl/#irrel">evolution does not operate on all possible fitness landscapes but specifically on biological ones, which tend to be sufficiently smooth.&lt;/a> On a perfectly smooth, single-peaked fitness landscape, evolutionary adaptation finds the peak in logarithmic time (Wilf and Ewens, 2010). On a more rugged peak with some epistasis, search time can be slower, depending on the exact type and amount of epistasis (Ewert et al. 2012). Yet Covert et al. (2013) showed in simulations that certain types of epistasis can actually speed up evolution. (Disclaimer: I’m an author on the Covert paper.)&lt;/p>
&lt;p>I hope you can see where this is going. It is possible to construct mathematical models that produce any number of search times, from very rapid (logarithmic) to extremely slow (exponential or worse). The specific model that Chatterjee et al. propose is that of a fitness landscape in which peaks are surrounded by extended, flat valleys. On such a landscape, evolutionary search does indeed take exponentially long, because there are insufficient fitness gradients that can guide the population towards the fitness peaks. Instead, the population simply drifts randomly until it hits on a peak by accident.&lt;/p>
&lt;p>However, whether any of these mathematical models are actually relevant to the process of evolution as it happens in the natural world is ultimately an empirical question. No amount of mathematical theorizing can produce more insight than we already have. We have pretty good experimental evidence (e.g. Blount et al. 2012) and evidence from non-trivial computer simulations (e.g. Lenski et al. 2003) that evolution can produce quite sophisticated, non-obvious, and non-trivial novel functions within a few thousand generations. We also have increasingly good evidence that protein-coding genes can arise &lt;em>de-novo&lt;/em> from non-coding DNA, through the accumulation of point mutations. For example, Knowles and McLysaght (2009) estimate that 18 such genes have arisen in humans since the human-chimpanzee divergence. Thus, in summary, the available empirical evidence is clearly at odds with the exponential-time argument.&lt;/p>
&lt;/div>
&lt;div id="is-the-proposed-regeneration-process-a-viable-alternative" class="section level2">
&lt;h2>Is the proposed regeneration process a viable alternative?&lt;/h2>
&lt;p>Now, Chatterjee et al. would argue (I assume) that the empirical evidence is at odds with the exponential-time argument because evolution is actually more accurately described by their “regeneration process.” They define the regeneration process as an iterated evolutionary search, where the search starts over and over in a relatively small sphere around the target solution. And, surprise surprise, under this assumption the search does not take exponentially long.&lt;/p>
&lt;p>What is the cause of the regeneration process? The authors mention gene duplications and recombination. Ok, I can see how these processes could keep producing sequences in a particular region of the sequence space, and this region might be within a small sphere of the target. But note that this argument assumes the target happens to be near where these processes produce new sequence variants in the first place. Further, point mutations can similarly be the cause of the regeneration process: If the population happens to sit on a local peak next to the sphere of interest, it will also feed the regeneration process. In other words, the regeneration process simply describes evolution on a fitness landscape that violates the assumptions made in the first part of Chatterjee et al.’s paper. In fact, the whole thing seems tautological. The paper basically says: Evolution can only succeed on fitness landscapes that are structured such that evolution can succeed.&lt;/p>
&lt;p>There’s another issue I have with the way the authors present the regeneration process. They state that “evolution can be seen as a tinkerer playing around with small modifications of existing sequences rather than creating entirely new ones.” This phrasing makes me uncomfortable, because it reminds me of the &lt;a href="http://www.talkorigins.org/indexcc/CH/CH350.html">discussion of “kinds”&lt;/a> in the creationist literature. What exactly is an “entirely new sequence”? I’d argue that this concept is vague to the point of being useless. We know of examples where homologous protein sequences share less similarity (below 1%) than would be expected from two randomly chosen sequences (Kinch and Grishin, 2002). If a sequence has diverged so much that nearly every position has changed, is it entirely new relative to its ancestor? We also know of examples where protein sequences that are clearly evolutionarily related fold into entirely distinct structures, structures that seem to have absolutely no relationship with each other (Grishin 2001). Would this count as a case where an entirely new protein evolved? Finally, we know of cases where a functional, protein-coding sequence arose through a few point mutations from a non-coding sequence (Knowles and McLysaght, 2009). Again, is this an entirely new protein? Evolution, which proceeds by descent with modification, is necessarily a stepwise, local search process. Thus, anything that evolves can be traced back to something that previously existed, and nothing “entirely new” can ever evolve, if we define “entirely new” as “not connected by a number of small modifications to something that existed before.” If by contrast “entirely new” is simply to mean “something that doesn’t obviously, to the human eye, look like something that existed before,” then we have plenty of examples where entirely new things have evolved, and the papers I quote in this paragraph provide some of these examples.&lt;/p>
&lt;/div>
&lt;div id="does-this-paper-provide-novel-biological-insight" class="section level2">
&lt;h2>Does this paper provide novel biological insight?&lt;/h2>
&lt;p>What are the biological insights we can draw from this study? There is no doubt that the authors have done a lot of math, and that this math deserves to be published somewhere. But for publication in PLOS Computational Biology, the stated requirement is that the paper &lt;a href="http://www.ploscompbiol.org/static/information">“provide profound new biological insights.”&lt;/a>&lt;/p>
&lt;p>To be frank, there is little biology in this paper. However, one sentence drew my attention. In the first paragraph of the discussion, Chatterjee et al. state that their “process can also explain the emergence of orphan genes arising from non-coding regions [45].” This sentence got me excited. I’m quite interested in the origin of novel protein structures, and as I mentioned above, there is now pretty solid evidence that proteins can evolve from non-coding sequences. So I expected to find a section somewhere in the paper where they estimated the rate of such occurrences. Presumably, one could estimate the amount and sequence variation of non-coding DNA in a typical genome, and, assuming some model about the density of viable protein structures within that amount of random sequence, estimate the number of novel protein structures expected to evolve from non-coding DNA per genome and number of generations. Hopefully, that estimate would roughly agree with estimates from genomics data, such as the one provided by Knowles and McLysaght (2009).&lt;/p>
&lt;p>Alas, I was sorely disappointed. I searched both the main body of the text and the supporting materials for the word “orphan” and got a total of two hits, one in the sentence I just quoted and one in the title of their reference 45, a review of orphan genes (Tautz and Domazet-Lošo, 2011). To be absolutely sure I hadn’t missed any relevant passages, I also searched for the words “protein,” “peptide,” and “coding,” both in the main text and in the supplement, but couldn’t find any meaningful discussion about the evolution of novel coding sequences through these terms either. I don’t think there is a single piece of evidence in the entire paper to support Chatterjee et al.’s one claim of biological significance, that the proposed regeneration process can explain the origin of orphan genes.&lt;/p>
&lt;/div>
&lt;div id="conclusions" class="section level2">
&lt;h2>Conclusions&lt;/h2>
&lt;p>In summary:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>Chatterjee et al.’s exponential time calculation seems similar in spirit to long-standing arguments raised by intelligent-design creationists such as Dembski and colleagues. Considering that these arguments have been raised and rebutted since forever, I don’t see that Chatterjee et al. make a particularly novel or useful contribution. They also fail to place their work into the context of these earlier arguments.&lt;/p>&lt;/li>
&lt;li>&lt;p>The primary open problem in evolutionary biology remains a detailed characterization of the genotype-phenotype map, and, in particular, the question of how close in genotype are distinct high-fitness phenotypes. The Chatterjee et al. paper makes no attempt at answering this question. Instead it offers the tautology that evolution succeeds on fitness landscapes that are structured such that evolution can succeed.&lt;/p>&lt;/li>
&lt;li>&lt;p>The paper makes the bold claim that it can explain the origin of orphan genes, without actually providing any concrete evidence. I am puzzled how this entirely unsupported claim made it past review.&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div>
&lt;div id="references" class="section level2">
&lt;h2>References&lt;/h2>
&lt;p>Z. D. Blount, J. E. Barrick, C. J. Davidson, R. E. Lenski (2012). &lt;a href="http://www.nature.com/nature/journal/v489/n7417/abs/nature11514.html">Genomic analysis of a key innovation in an experimental &lt;em>Escherichia coli&lt;/em> population.&lt;/a> Nature 489:513–518.&lt;br />
A. W. Covert, R. E. Lenski, C. O. Wilke, C. Ofria (2013). &lt;a href="http://www.pnas.org/content/110/34/E3171.full">Experiments on the role of deleterious mutations as stepping stones in adaptive evolution.&lt;/a> PNAS 110:E3171–E3178.&lt;br />
K. Chatterjee, A. Pavlogiannis, B. Adlam, M. A. Nowak (2014). &lt;a href="http://www.ploscompbiol.org/article/authors/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003818">The time scale of evolutionary innovation.&lt;/a> PLoS Comput. Biol. 10:e1003818.&lt;br />
W. A. Dembski (2007). No Free Lunch: Why Specified Complexity Cannot Be Purchased without Intelligence. Rowman &amp;amp; Littlefield.&lt;br />
W. Ewert W, W. A. Dembski, A. K. Gauger, R. J. Marks II (2012). &lt;a href="http://www.bio-complexity.org/ojs/index.php/main/article/view/BIO-C.2012.4/BIO-C.2012.4">Time and information in evolution.&lt;/a> BIO-Complexity 4:1–7.&lt;br />
N. V. Grishin (2001). &lt;a href="http://www.sciencedirect.com/science/article/pii/S1047847701943357">Fold change in evolution of protein structures.&lt;/a> J. Struct. Biol. 134:167–185.&lt;br />
L. N. Kinch and N. V. Grishin (2002). &lt;a href="http://onlinelibrary.wiley.com/doi/10.1002/prot.10110/full">Expanding the nitrogen regulatory protein superfamily: Homology detection at below random sequence identity.&lt;/a> Proteins: Structure, Function, and Bioinformatics 48:75–84.&lt;br />
D. G. Knowles and A. McLysaght (2009). &lt;a href="http://genome.cshlp.org/content/19/10/1752.full">Recent de novo origin of human protein-coding genes.&lt;/a> Genome Res. 19:1752–1759.
R. E. Lenski, C. Ofria, R. T. Pennock, C. Adami (2003). &lt;a href="http://www.nature.com/nature/journal/v423/n6936/abs/nature01568.html">The evolutionary origin of complex features.&lt;/a> Nature 423:139–144.&lt;br />
D. Tautz and T. Domazet-Lošo (2011). &lt;a href="http://www.nature.com/nrg/journal/v12/n10/full/nrg3053.html">The evolutionary origin of orphan genes.&lt;/a> Nature Reviews Genetics 12:692–702.&lt;br />
H. S. Wilf and W. J. Ewens (2010). &lt;a href="http://www.pnas.org/content/107/52/22454.full">There’s plenty of time for evolution.&lt;/a> PNAS 107:22454–22456.&lt;/p>
&lt;/div></description></item><item><title>Share your preliminary work with other people, even if you think it’s crap</title><link>https://sevimcengiz.github.io/blog/2014/07/08/share-your-preliminary-work-with-other-people-even-if-you-think-its-crap/</link><pubDate>Tue, 08 Jul 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/07/08/share-your-preliminary-work-with-other-people-even-if-you-think-its-crap/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>It’s quite common for me to have students tell me “the analysis didn’t work out” or “the figure looks bad” or “I don’t have any useful results.” And it’s also quite common for the students to be wrong. Sometimes, students have amazing results but are all disappointed because the results aren’t what they had expected. These students fail to see the data for what they are. More commonly, the students may be right in that the data aren’t that great, but usually I can see something in the data that the student didn’t. In either case, it is important that we look at the data together, because jointly we will see more than either of us individually would have seen.&lt;/p>
&lt;p>However, while some students are happy to show me their “failed” analyses and complain about how nothing works, others are more reserved, sometimes to the point of reluctance. The latter students don’t feel comfortable showing me their preliminary results, or sometimes any results at all, unless they think the work is completed.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> These students are probably under the mistaken impression that I will judge them for “failed” analyses, or that I expect a complete analysis, with proper interpretation of all data points, at all times, or that having remaining open questions means I’ll think the student did a poor job. On the contrary, I want to see all these preliminary, incomplete, and confusing results. The data are the data, and I want to be able to draw my own conclusions about them.&lt;/p>
&lt;p>Furthermore, I submit that unless you know for sure you made a mistake (say, your code has a bug and you know it), you will always be better off discussing your work with other people than silently deciding it’s not good enough. And if your results seem too trivial or wrong to talk about them with your advisor, then at least talk them through with a fellow student or postdoc.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> Think about it this way: every time you generate a result or make a figure, and then you delete it before you show it to somebody else, you’re preempting the possibility that somebody else might see something useful in your work. And while a lot of what you’re doing may indeed be worthless, I’d argue that unless you’re a complete disaster, you’ll probably do at least one thing every day that is actually worthwhile. So every day, you should produce some sort of result that you then discuss with somebody else.&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a>&lt;/p>
&lt;p>Importantly, the necessity of sharing preliminary work with other people doesn’t stop once you graduate. Just because you have a PhD doesn’t mean that you’re suddenly able to always see the data exactly for what they are. Sometimes even the most experienced scientists are overly attached to a particular hypothesis or miss some critical detail in their data. That’s why many experienced scientists routinely talk to their colleagues about work in progress, why they present preliminary results at conferences or seminar talks, and why they request comments on manuscript drafts and preprints. In my mind, even traditional peer review serves primarily the purpose of having another set of eyes look over the data and check whether the authors are over- or underhyping their results.&lt;a href="#fn4" class="footnote-ref" id="fnref4">&lt;sup>4&lt;/sup>&lt;/a>&lt;/p>
&lt;p>The only thing that really changes once you graduate and/or complete your postdoc is that suddenly there is no adviser anymore who makes you show him/her your latest work. Now it’s entirely on you to seek out the advice you need and to receive the feedback that will make your work better. And, if you are advising students yourself, it’s now your job to make them talk to you and show you what they’re doing, warts and all.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>If you’re my student and you think this blog post is about you, let me tell you: you’re not the only one.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>But realize that sometimes students talk each other down. Just because your fellow grad students say what you did was stupid doesn’t mean it actually was; they may just lack perspective.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>You might argue that you’re working in a field where results accumulate slowly, hence you can’t possibly discuss results from individual days. E.g., you’re collecting beetles in the field, and you need three field seasons before you have enough data to test your hypothesis. My response is that nevertheless there are tons of preliminary data that you should discuss with somebody. E.g., if you’re collecting beetles every day, you know how many you found each day, where you found them, what physical characteristics they had, etc., and how these quantities change over time. All of these are worthwhile preliminary results that you should look at, graph, and discuss with a third party.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn4">&lt;p>The second case, where reviewers see more in the data than the authors, is more common than you may think. Many papers get really good only after a solid round of peer review, and frequently the reviews in those cases are not “X is wrong” but rather “You have done X but you should also do Y, and in combination you could conclude Z which would be really cool.”&lt;a href="#fnref4" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>What does it take to be a computational biologist?</title><link>https://sevimcengiz.github.io/blog/2014/01/10/what-does-it-take-to-be-a-computational-biologist/</link><pubDate>Fri, 10 Jan 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/01/10/what-does-it-take-to-be-a-computational-biologist/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>I would like to talk about what it takes to be a computational biologist, specifically in comparison to being an experimental biologist. If you’re wondering whether instead of becoming a computational biologist you should become a race-car driver, fighter pilot, or ballet dancer, this post probably won’t help you. But if you’re wondering whether a computational biology lab is a better choice for you than an experimental biology lab, this post should provide you with some useful guidelines. To cut right to the chase, here is the take-home message: To become a computational biologist, you need to &lt;em>want&lt;/em> to become a computational biologist.&lt;/p>
&lt;p>When I interview students who are potentially interested in joining my lab, the interviews inevitably take one of two routes, depending on the students’ background. The students with extensive computational experience usually make a point emphasizing all the techniques, systems, and languages they have learned, to showcase their technical expertise. By contrast, the students with little computational experience are usually rather timid. They say that they might be interested in computational biology but don’t really know much about it, and also that they don’t know if they would be any good at it.&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a>&lt;/p>
&lt;p>The truth is, I don’t really worry much about pre-existing expertise.&lt;a href="#fn2" class="footnote-ref" id="fnref2">&lt;sup>2&lt;/sup>&lt;/a> Sure, I won’t be disappointed if a student knows a lot already, but it plays a rather minimal role in my decision of whether or not to take a particular student.&lt;a href="#fn3" class="footnote-ref" id="fnref3">&lt;sup>3&lt;/sup>&lt;/a> I think that computation can be learned relatively easily, &lt;em>if you really want to learn it,&lt;/em> so what matters much more than your current knowledge is your intention.&lt;a href="#fn4" class="footnote-ref" id="fnref4">&lt;sup>4&lt;/sup>&lt;/a> For this reason, when students express any concern about their ability to be computational biologists, I usually ask them a simple question: Would you rather spend your day in front of a bench pipetting, or would you rather spend your day in front of a computer screen staring at symbols and numbers? Anybody who would rather stare at a computer screen is going to be fine in my lab, and anybody who would rather pipette is not going to have a good time.&lt;/p>
&lt;p>In fact, too much pre-existing computational knowledge can be a disadvantage, when the students think they know things better than they actually do. At least the inexperienced students are a blank slate. They are willing to listen and they accept the conventions of the lab. The more experienced students may have idiosyncratic views on how things should be done, views that may make sense from their perspective but not from the perspective of the person running the lab (i.e., me). For example, a student who insists on using java for a project when the rest of the lab uses python is going to cause problems,&lt;a href="#fn5" class="footnote-ref" id="fnref5">&lt;sup>5&lt;/sup>&lt;/a> even if he has lots of experience with java and none with python. With some experienced students, I spend as much time re-training them as I would have spent with less experienced students training them from scratch.&lt;/p>
&lt;p>So, if you don’t have a lot of computational experience but you would like to do computational work, ask yourself whether you have the patience to hack away for hours in front of a computer screen, until you have solved a problem. If the answer is yes, you’ll be fine. And if you do already have a lot of computational experience, keep an open mind, realize that there is probably still a lot you can learn, and accept that some things are just conventions. Even if you don’t like a particular convention (such as “we use python”) you should accept it if you want to be successful in your lab.&lt;a href="#fn6" class="footnote-ref" id="fnref6">&lt;sup>6&lt;/sup>&lt;/a>&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>Actually, more recently there is also a third type, students who think they have computational experience but they really don’t. MS Word, Excel, or Facebook do not count as computational experience. If you have never written an actual program and never used a command line, you don’t have any meaningful experience.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn2">&lt;p>This statement applies to undergraduate researchers or prospective graduate students. I wouldn’t hire a postdoc without any computational expertise. If somebody has done a purely experimental PhD they are unlikely going to be a good match for my lab at the postdoc stage, since we don’t do any experiments whatsoever.&lt;a href="#fnref2" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn3">&lt;p>I tend to evaluate students primarily on whether they appear to be motivated, whether they appear to be smart, whether I can connect with them, and whether I think they would fit into the lab.&lt;a href="#fnref3" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn4">&lt;p>This point is also made &lt;a href="http://pgbovine.net/tech-privilege.htm">here.&lt;/a> As long as people give you a chance to try yourself at programming and you make an effort, you should be fine.&lt;a href="#fnref4" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn5">&lt;p>As just one example, it will be more difficult for other students to take advantage of that student’s work and vice versa. Further, once the student leaves, the project may be abandoned or somebody else may have to re-engineer it using the lab’s preferred language.&lt;a href="#fnref5" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;li id="fn6">&lt;p>I acknowledge that there may be situations where the convention a lab has chosen is genuinely poor, and where the student truly knows better than the faculty member how to do things properly. However, I think these situations are rare, in particular for labs run by experienced computational biologists. And moreover, if you as a student really have so much computing experience that you can see all the poor decisions your PI is making, why did you join the lab in the first place? You should have seen these issues ahead of time. For example, if I were a prospective graduate student now, and I was told a lab did everything in perl and fortran, I’d run. By contrast, if they used languages I approve of, if they deposited their code on github, and if the code they deposited looked more or less decent, then I should be fine in that lab.&lt;a href="#fnref6" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How glamour journals rose to prominence, and why they may not be needed anymore</title><link>https://sevimcengiz.github.io/blog/2014/01/04/how-glamour-journals-rose-to-prominence-and-why-they-may-not-be-needed-anymore/</link><pubDate>Sat, 04 Jan 2014 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2014/01/04/how-glamour-journals-rose-to-prominence-and-why-they-may-not-be-needed-anymore/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>In the ongoing discussion about the value of glamour journals such as Nature, Science, and Cell, I think it’s worth looking back and asking: “How did they rise to prominence?” and “Are they still serving the same role they did when they arose?” So let’s take a quick trip into the history of science communication, before the internet. Then we can ask what the internet has changed, and how we could make the best of current technology.&lt;/p>
&lt;p>I belong to the last generation of scientists that experienced science before the internet. I started doing research as an undergrad in 1995. At that time, I saw a web browser for the first time, and I sent my first email. While the internet had been around for a while by 1995,&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> its use was still very limited, and barely anybody outside academia had ever even heard of it. All this would change over the next 4-5 years, and by 2000 the internet started to become ubiquitous.&lt;/p>
&lt;p>I don’t think anybody who got into science after the year 2000 can imagine what keeping up with the literature was like before the internet. I started my postdoc in 2000. During my entire postdoc time (or since), I rarely ever went into a library. By that time, most journals had a solid online presence, including back issues. Articles that weren’t available online could be requested via inter-library loan, and they would arrive electronically. By contrast, during my PhD, I spent a lot of time at the library. I would make weekly trips to browse the latest issues of the scientific journals I was interested in. Because I was working at the interface between physics and biology, I had to visit the physics library and the biology library. For certain articles I also had to go to the chemistry library. I knew exactly which library had which journals, and which journals were available in multiple libraries. (Almost all libraries had Science, for example.) To figure out whether anybody was citing a particular paper, I had to confer with the &lt;a href="http://en.wikipedia.org/wiki/Science_Citation_Index">Science Citation Index,&lt;/a> which was a big book available at some libraries. For any article of interest, it would list by which other articles it had recently been cited. Invariably, the list of citing articles would contain articles in journals that were only available in a different library on campus, or not in any library at my university. So, after reading the Science Citation Index, I’d make the trip to a different library, or submit an inter-library loan request, or make a note for my next scheduled trip to a different library to look up a particular article. In the worst-case scenario, it could take weeks until I saw a particular article, and then I’d often find out the article wasn’t really relevant to what I was doing.&lt;/p>
&lt;p>Compare this to how literature search works today. I look up an article on Google Scholar, click on “Cited by” or “Related articles,” and find relevant related articles in seconds. For every article listed, I can get at least the title and the abstract, and for the vast majority of articles I can get the full text, all within a few seconds and while sitting at home on my couch. I can similarly browse any open-access journal, everything on Pubmed Central, and any paywalled journal my university has access to, from everywhere in the world. For all intents and purposes, the second I know a particular article exists, I can look it up and read it.&lt;/p>
&lt;p>What has any of this to do with glamour journals? I’m going to argue that in the time before the internet, glamour journals and other highly selective journals served a crucial role. In a world where looking up a reference can take between days and weeks, finding potentially interesting &lt;em>references&lt;/em> is much less valuable than finding potentially interesting &lt;em>articles.&lt;/em> Yes, you would go to the trouble of hunting down a particular article if it seemed directly relevant for your own work, but you certainly wouldn’t do so just to generally keep up with a broader field, much less all of science. Therefore, reading a journal such as Science or Nature, or even a more specialized but still fairly selective journal such as Genetics, was the only way to keep up with scientific progress. You went to the library, you took the physical copy of the journal, you browsed through it, you read the interesting articles, you learned something useful, you went home/back to your office.&lt;/p>
&lt;p>By contrast, these days, with nearly any article right at our fingertips, the process of publishing articles and of selecting articles can be decoupled. For example, I don’t consistently browse through the tables of contents of Nature or Science anymore, because I now have other means of discovering interesting articles. Any interesting article in my field I’ll come across sooner or later because Google Scholar recommends it to me, or somebody tells me about it in person, or it gets cited in a paper I read or review. For generally interesting articles, say the latest findings about global warming, I’ll likely see them mentioned on Twitter or Reddit. The advantage of all these methods of finding articles over browsing through tables of contents is that I’m not tied to the venue in which the article was published. I’m just as likely to come across an interesting article published in Nature as one in PLOS ONE. And the moment I learn about an article, I can read it. But imagine a print version of Twitter, in the time before the internet. If I had received per mail, once a week, a list of potentially interesting things published in the most random venues, I would never have followed up with reading any of them. The barrier to doing so would simply have been too high.&lt;/p>
&lt;p>Some people take this reasoning to the extreme and argue that since now everything is easily available online and search engines are powerful, we don’t need selective journals anymore at all. The best science will rise to the top, it will be cited, tweeted, mentioned on reddit, bloggers will write posts about it, and thus we might as well publish everything in PLOS ONE. I am not entirely convinced by this argument, for the following reason: It’s all well and good if other people cite and tweet your work, but what if they don’t? If you think you have done some really outstanding work, work that deserves more attention than your regular bread-and-butter efforts do, in a world where all science is published in PLOS ONE, what options to you have? In a world that has glamour journals, it’s of course obvious what you can do: You write a nice 3-6 page summary of your work, highlighting the most important findings and the broad relevance, you send it to one or more glamour journals, and you hope for the best. If you get through, you’ll have a much higher chance of getting your article cited, tweeted, etc., because people pay attention to the glamour journals, and they like to read short, clearly written articles that highlight key findings and broader relevance. But if there are no glamour journals, then you have no good option of indicating that in your own opionion, this article is more valuable than that article.&lt;/p>
&lt;p>So let me summarize the facts: (i) In the world of the internet, it doesn’t matter where something is physically published, as long as it is easily accessible through a URL. (ii) Glamour journals have lost the original purpose of making important science easily and broadly accessible. (iii) Publication venues that highlight interesting work by commenting and/or linking to it (such as Twitter, Reddit, Nature News and Views, Google Scholar, etc.) are highly valuable. (iv) Short, clearly written articles highlighting key insights and broader relevance are appreciated and highly valuable. (v) Authors have an interest in pointing out what they think are their most important works.&lt;/p>
&lt;p>These facts lead me to the following proposal: Let’s take all original science out of the glamour journals. Instead, allow authors to submit short summaries (maybe 2-3 pages) of work they have already published elsewhere, e.g. in PLOS ONE. These summaries would be reviewed editorially, and also by one or two expert scientists who’d be asked to judge whether the original article appears to be scientifically sound and noteworthy. Editors might reject a summary because it isn’t deemed sufficiently interesting or novel, and there would still be fighting and politicing about getting summaries into these glamour journals, but the system would relieve authors of several pressures: (i) Authors wouldn’t have to rewrite an article multiple times just to hope to get it published eventually in one of the selective journals. (ii) Authors could get their results out and cite them properly while still trying for that glamour slot. (iii) Since the original article of record would be in PLOS ONE or PeerJ or similar, it would become generally accepted to have even the most important work published in these journals. Nobody could look at a publication list and say “Oh, it’s just a bunch of PLOS ONE papers.” (iv) Hiring committees and granting agencies would still have the option to evaluate candidates by the number of summaries they have published in glamour journals, though I would hope they would do so to a lesser degree and pay more attention to the original articles.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>The first web browser, which started the development of the modern internet, &lt;a href="http://en.wikipedia.org/wiki/Mosaic_web_browser">was released in 1993.&lt;/a> The concept of an online journal was unthinkable before the invention of the web browser.&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Is there an avalanche of low-quality research, and if so, must we stop it?</title><link>https://sevimcengiz.github.io/blog/2013/12/21/is-there-an-avalanche-of-low-quality-research-and-if-so-must-we-stop-it/</link><pubDate>Sat, 21 Dec 2013 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2013/12/21/is-there-an-avalanche-of-low-quality-research-and-if-so-must-we-stop-it/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>&lt;strong>Update:&lt;/strong> &lt;em>It turns out the article in the Chronicle is not recent, I misread the date on the page. (The Chronicle has two dates on each page, today’s date and the article publication date.) I stand by everything else I say, though.&lt;/em>&lt;/p>
&lt;p>A recent article in the Chronicle of Higher Education argues that &lt;a href="http://chronicle.com/article/We-Must-Stop-the-Avalanche-of/65890/">“we must stop the avalanche of low-quality research.”&lt;/a> The authors decry the rapid growth of the scientific literature, which (as they argue) puts increasing strain on readers, reviewers, and editors without producing much benefit. They argue that this growth is driven by an increasing pressure on scientists to publish more, and the result is increasing amounts of low-quality publications. To address the pressure on scientists, they propose three fixes, of which one is Ok and two are positively inane. Maybe what we really have to stop is the avalanche of low-quality, non-reviewed opinion pieces published on web pages?&lt;/p>
&lt;p>Reading through the article, I found it difficult not to wonder whether the authors had ever heard of the internet or of modern information-processing technology (e.g., Google). Now, to be fair, none of the authors are in the natural sciences. The authors work in English, mechanical engineering, medicine, management, and geography. I don’t really know these areas. My own work is in biology, and I’m also somewhat familiar with the publishing cultures in physics and in computer science. So, everything they say may make sense in their fields, but it doesn’t in mine. I’m not convinced we have a major crisis, and I certainly don’t think their proposed fixes are any good. Let’s take a look at their proposed solutions first.&lt;/p>
&lt;div id="limit-number-of-papers-submitted-for-job-applications-or-promotions" class="section level2">
&lt;h2>Limit number of papers submitted for job applications or promotions&lt;/h2>
&lt;p>The first fix, to limit the number of papers that applicants are allowed to submit for job applications or promotions, is actually somewhat reasonable. Applicants should be judged on the quality of their work, not on the mere quantity of output. However, I am strongly opposed to saying applicants are not even allowed to mention anything beyond their key 3-5 papers. Why should productivity be punished? What if they wrote 10 important papers? Should Ed Witten be limited to list only 5 papers? (To date, he has written &lt;a href="http://scholar.google.com/scholar?hl=en&amp;amp;q=edward+witten">over 30 papers with over 1000 citations each!&lt;/a>) While there are negative outliers in academia, people who produce huge amounts of mindless drivel, I definitely see a correlation between quantity and quality. The most interesting and influential papers are generally written by the most productive researchers. I have previously given arguments for why we would &lt;a href="https://sevimcengiz.github.io/blog/2013/11/3/no-one-reads-your-paper-either">expect such a correlation to exist.&lt;/a>&lt;/p>
&lt;p>Most job search and promotion processes that I am aware of have already found a solution to this problem, by asking applicants to submit both (i) a full list of all publications and (ii) the 3-5 most important papers, possibly with a statement explaining their impact. This is good practice that strikes a balance between quality and quantity, it allows applicants to showcase both how good they are and how consistently productive they are, and most importantly, it is already common practice. So point 1 is a non-issue, from where I stand.&lt;/p>
&lt;/div>
&lt;div id="evaluate-researchers-by-impact-factors" class="section level2">
&lt;h2>Evaluate researchers by impact factors&lt;/h2>
&lt;p>Evaluating researchers by impact factor is such an absurd and untimely suggestion, I can’t help but wonder whether the authors have been living under a rock for the last 10 years. It’s particularly ironic that the Chronicle of Higher Education would publish this statement a mere 11 days after nobel-prize winner Randy Schekman publicly proclaimed that luxury (i.e., high impact-factor) journals such as Nature, Cell, and Science &lt;a href="http://www.theguardian.com/commentisfree/2013/dec/09/how-journals-nature-science-cell-damage-science">“are damaging science.”&lt;/a> Did the authors really not see this article, &lt;a href="http://scholarlykitchen.sspnet.org/2013/12/11/this-takes-the-prize-editor-of-new-luxury-oa-journal-boycotts-luxury-subscription-journals/">nor the widespread outrage it caused over containing a cheap plug for a different luxury journal?&lt;/a> If there is one problem we have in science right now, at least in the biomedical field, it’s an over-reliance on impact factors and publications in high-profile journals. The outcry over Schekman’s article shows how sensitive of an issue this is, and how many scientists are concerned about the growing pressure to publish in only the highest-impact journals. Schekman himself addresses this in &lt;a href="http://theconversation.com/how-to-break-free-from-the-stifling-grip-of-luxury-journals-21669">his response to the criticism he received.&lt;/a> Scientists should be judged on the quality of their work, not on whether or not they published in Nature.&lt;/p>
&lt;/div>
&lt;div id="limit-the-length-of-papers-published" class="section level2">
&lt;h2>Limit the length of papers published&lt;/h2>
&lt;p>I don’t see how imposing page limits connects at all to the issue at hand. Surely, if we want fewer but higher-quality publications, the papers should be longer not shorter. Also, I strongly oppose to the split model with a brief (4-6 page) main article (i.e., advertisement) accompanied by longer supporting materials. Invariably, the supporting materials are not written as carefully as the main article, and the quality of the paper as a whole suffers. Notably, PNAS just went the other direction, and now allows papers of up to 10 pages in length in their online-only PNAS Plus edition. This was a very welcome change, I think. The 6-page limit of PNAS was often too limiting, whereas most articles fit comfortably within 10 pages.&lt;/p>
&lt;/div>
&lt;div id="is-there-too-much-pressure-to-publish" class="section level2">
&lt;h2>Is there too much pressure to publish?&lt;/h2>
&lt;p>While there is pressure to publish, frankly I don’t see that there is &lt;em>excessive&lt;/em> pressure to publish. From what I see, for example in conversations with colleagues, the common expectation is reasonable productivity both in terms of quantity and in terms of quality. In terms of quantity, reasonable is usually a number between 1 and 10 papers per year. Publish less, and people start wondering whether you’re working consistently, and in particular whether you’ll keep working in the future. Publish much more than 10 papers per year, and people start looking at you suspiciously. I sat on a grant-review panel once where one applicant claimed his previous 3-year NSF grant had led to ~100 publications. People were very suspicious of this claim and the grant did not get good reviews, even though the science seemed to be reasonable. (I’m not saying the proposal would have been funded if the applicant had had fewer publications, but the high number certainly didn’t help; if it had any effect it was a negative one.) In most areas of Biology, I think 2-3 papers a year will be considered perfectly reasonable for anybody but a senior PI running a large lab. (This includes all papers with your name on, not just first-author papers.)&lt;/p>
&lt;p>In terms of quality, I stick to my earlier recommendation: &lt;a href="https://sevimcengiz.github.io/blog/2013/11/3/no-one-reads-your-paper-either">publish at least one paper a year that has some real substance.&lt;/a> Where exactly that paper is published is secondary, I believe. Publishing the occasional high-profile article in a luxury journal can’t hurt, but I hope that we as scientists can collectively learn to pay a little less attention to where something is published and pay more attention to the content. We shouldn’t hire somebody without having carefully read at least one or two of their papers, and I think the more diligent search committees operate like that already.&lt;/p>
&lt;p>With regards to excessive workload for editors and reviewers, I think there are several things that could be done relatively easily:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>Institute a system of reviewing credits, where you receive one credit for each article you review and you have to spend a number of credits (e.g. 6) to submit an article. This would ensure that everybody who publishes carries their fair share on the reviewing side.&lt;/p>&lt;/li>
&lt;li>&lt;p>Have more graduate students and postdocs review papers. Not every paper needs to be reviewed by three members of the NAS. In fact, I often find that graduate students write better reviews than senior scientists do, because the graduate students take the job much more seriously and put way more effort into it than an established scientist normally would.&lt;/p>&lt;/li>
&lt;li>&lt;p>Have less stringent reviewing criteria, don’t judge impact. Much of the excessive reviewing load actually comes from the pressure to publish in highly selective journals. Thus, many articles make the mandatory trek from Science to Nature to PNAS to PLOS Genetics to PLOS ONE, possibly undergoing four or more separate rounds of review. It’s not uncommon for me to review the same article several times for different journals. And in the end, everything gets published anyway, somewhere. If it was the reviewers’ job to only look for major scientific flaws, then most articles could be published after 1-2 rounds of review, cutting the total review burden way down.&lt;/p>&lt;/li>
&lt;li>&lt;p>Improve tools for post-publication evaluation of articles. At present, all we have is citations and word-of-mouth. (“Have you seen the latest paper by X in PNAS? It’s really not very good.”) I’m sure we can do better than that, and over time we’ll find ways to put modern computing power and crowd-sourcing ideas to good use. &lt;a href="http://www.the-scientist.com/?articles.view/articleNo/37969/title/Post-Publication-Peer-Review-Mainstreamed/">NCBI’s PubMed Commons is a first step in this direction.&lt;/a> I’m sure over the next 10-20 years we’ll see many more innovative ideas to evaluate the quality of scientific work post publication.&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>The value of pre-publication peer review</title><link>https://sevimcengiz.github.io/blog/2013/12/21/the-value-of-pre-publication-peer-review/</link><pubDate>Sat, 21 Dec 2013 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2013/12/21/the-value-of-pre-publication-peer-review/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>I see lot of discussion these days about the value of peer review. Are journals too selective? Are acceptance decisions arbitrary? Does peer review actually catch scientific mistakes or fraudulent practices? Wouldn’t it be better to just put everything out there, say on preprint servers, and separate the wheat from the chaff in post-publication review? I’m not quite ready yet to give up on pre-publication peer review. I think it serves a useful purpose, one I wouldn’t want to do away with. In the following, I discuss four distinct services that peer review provides, and assess the value I personally assign to each of them.&lt;/p>
&lt;div id="peer-review-screens-out-nonsense-and-pseudoscience" class="section level2">
&lt;h2>Peer review screens out nonsense and pseudoscience&lt;/h2>
&lt;p>It’s important that somebody screen all potential scientific publications for actual scientific content. I don’t mind publishing null results, replication studies, or studies that present only a very minor advance. All of these works contain real science, and they may find some use at some point in the future. However, we must never mix science with pseudoscience. Someone has to assure that whatever gets published in a scientific journal is not complete nonsense. Even the preprint archive arxiv.org has &lt;a href="http://arxiv.org/help/endorsement">some sort of a screening and filtering system in place to hold back the crackpots.&lt;/a> In most cases, nonsensical papers would be caught by the editor and not even sent out to review. Nevertheless, we can consider filtering out nonsense to be an essential service of the pre-publication review process.&lt;/p>
&lt;/div>
&lt;div id="peer-review-catches-major-mistakes-andor-fraud" class="section level2">
&lt;h2>Peer review catches major mistakes and/or fraud&lt;/h2>
&lt;p>Many people seem to think that it is the reviewers’ job to catch major mistakes and/or fraud. And when they fail to do so, that is taken as evidence that peer review doesn’t work. I don’t think we can put such a high burden on the reviewers. Ultimately, the burden of producing correct and genuine results lies with the author. Peer review operates under the assumption that fundamentally the authors are honest and reasonably capable scientists. If peer review does happen to catch a major issue with a paper, that’s great, but generally I think that post-publication review is the much better venue to address major flaws or scientific misconduct.&lt;/p>
&lt;/div>
&lt;div id="peer-review-assess-novelty-potential-impact-and-fit-with-the-journal-scope" class="section level2">
&lt;h2>Peer review assess novelty, potential impact, and fit with the journal scope&lt;/h2>
&lt;p>Whether reviewers (or editors) should consider novelty and impact, and whether journals should be selective at all, is probably the most contentious issue in peer review. Traditionally, this has always been part of peer review. However, there are now several journals that explicitly state review should only assess scientific soundness (e.g. &lt;a href="http://www.plosone.org/static/information">PLOS ONE&lt;/a> or &lt;a href="https://peerj.com/about/aims-and-scope/">PeerJ&lt;/a>). I think there are valid arguments for both sides. On the one hand, it is imperative that we have publishing venues that will publish any scientifically sound study. Nobody benefits if a valid study is suppressed just because some reviewers didn’t find it interesting. If there’s no obvious scientific flaw, put it out there and let the readers (and Google) sort it out.&lt;/p>
&lt;p>On the other hand, I think that more selective journals can provide value as well. In my mind, where science has gone off-track is that the most selective journals (which are also considered to be the most prestigious ones, e.g. Nature, Science, Cell, PLOS Biology, PNAS) employ arbitrary selection criteria based primarily on the subjective goal of publishing “the best science.” As a consequence, whether I can publish in such journals depends much more on my marketing skills than on my scientific skills, and also on whether I’m working on a sexy study system.&lt;/p>
&lt;p>By contrast, the next lower tier of selective journals usually employ more objective selection criteria, and those arguably provide a useful value. For example, I’m an Associate Editor for PLOS Computational Biology, a fairly selective journal. The main requirement for publication in PLOS Computational Biology is &lt;a href="http://www.ploscompbiol.org/static/information">that you have produced high quality computational work that yields a novel biological insight.&lt;/a> In my mind, it is fairly straightforward to determine whether a paper satisfies that requirement or not. I also think that any capable computational biologist can jump over that bar. As a consequence, I feel that we’re providing useful selectivity without generating excessive artificial scarcity or making highly arbitrary decisions. If I see that somebody has on their CV a couple of PLOS Computational Biology papers, I can reasonably assume that they are doing consistent, high-quality computational work leading to novel insights into biological systems.&lt;/p>
&lt;/div>
&lt;div id="peer-review-helps-authors-improve-their-articles" class="section level2">
&lt;h2>Peer review helps authors improve their articles&lt;/h2>
&lt;p>In my mind, this last point is the most important point, and the reason why I’m not willing to give up pre-publication review in its entirety. In my experience as author, reviewer, and editor, the most common outcome of the review process other than “reject due to insufficient novelty” is “major revision.” The reviewers agree that the study has merit in principle, but they see a number of possible revisions that would improve the article. I have seen it countless times, both as author and as reviewer or editor, that a study was vastly improved after the first set of reviews. Sometimes reviewers catch an issue the authors hadn’t noticed, sometimes they have a really cool idea that brings the study to the next level, and sometimes they simply tell you that you have to work on your writing if you want to get your point across. Either way, this input is invaluable, and it improves the scientific literature tremendously. If we went to a system that operated entirely on post-publication review, we would probably still see the same kind of comments by reviewers, but there would be very little incentive for the authors to go and revise their papers accordingly.&lt;/p>
&lt;p>One downside to this aspect of peer review is that sometimes reviewers just keep insisting on changes that the authors don’t deem necessary or appropriate. This is another form of peer review gone wrong. The reviewers should make helpful suggestions, but they should not tell the authors how to write their paper. One solution to this issue is to make peer reviews public and leave with the authors the ultimate decision of whether or not they want to publish, as &lt;a href="http://www.biologydirect.com/about">Biology Direct does.&lt;/a> Another possibility is to allow authors to opt-out of re-review, as &lt;a href="http://www.biomedcentral.com/bmcbiol/about#publication">BMC Biology does.&lt;/a>&lt;/p>
&lt;/div></description></item><item><title>Ten simple rules for reproducible computational research</title><link>https://sevimcengiz.github.io/blog/2013/10/26/ten-simple-rules-for-reproducible-computational-research/</link><pubDate>Sat, 26 Oct 2013 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2013/10/26/ten-simple-rules-for-reproducible-computational-research/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>PLOS Computational Biology just published a new addition to their popular &lt;a href="https://collections.plos.org/ten-simple-rules">“ten simple rules”&lt;/a> series:&lt;/p>
&lt;blockquote>
&lt;p>Sandve GK, Nekrutenko A, Taylor J, Hovig E (2013) Ten Simple Rules for Reproducible Computational Research. PLoS Comput Biol 9(10): e1003285. &lt;a href="https://doi.org/10.1371/journal.pcbi.1003285">doi:10.1371/journal.pcbi.1003285&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>This article is relevant to anybody who wants to do computational research. I’ll make it required reading in my lab. For every single one of these rules, I can think of projects I’ve been involved with&lt;a href="#fn1" class="footnote-ref" id="fnref1">&lt;sup>1&lt;/sup>&lt;/a> that ran into trouble or failed because they violated that rule.&lt;/p>
&lt;p>While all of the rules are important, I’m particularly partial to these four: avoid manual data manipulation, record all intermediate results, always store raw data behind plots, and provide public access to scripts and results. They will prevent a lot of headaches for both you and the people coming after you who’d like to build on your results.&lt;/p>
&lt;div class="footnotes">
&lt;hr />
&lt;ol>
&lt;li id="fn1">&lt;p>All of these projects were run by friends of friends, of course. None of this would ever happen in my lab. 😉&lt;a href="#fnref1" class="footnote-back">↩︎&lt;/a>&lt;/p>&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Common errors in statistical analyses</title><link>https://sevimcengiz.github.io/blog/2013/08/18/common-errors-in-statistical-analyses/</link><pubDate>Sun, 18 Aug 2013 00:00:00 +0000</pubDate><guid>https://sevimcengiz.github.io/blog/2013/08/18/common-errors-in-statistical-analyses/</guid><description>
&lt;script src="https://sevimcengiz.github.io/rmarkdown-libs/header-attrs/header-attrs.js">&lt;/script>
&lt;p>This is a post I originally wrote for my lab webpage. I’m reproducing it here (with minor edits) as an exercise in getting to know this blogging platform.&lt;/p>
&lt;div id="confusing-significance-and-effect-size" class="section level2">
&lt;h2>Confusing significance and effect size&lt;/h2>
&lt;p>&lt;em>Statistical significance&lt;/em> (a low &lt;em>P&lt;/em> value) measures how certain we are that a given effect exists.&lt;/p>
&lt;p>&lt;em>Effect size&lt;/em> measures the magnitude of an effect. What exactly effect size is depends on the analysis, examples are a correlation coefficient, the difference in means for a t test, or the odds ratio for a contingency-table analysis.&lt;/p>
&lt;p>Many results we encounter in the real world are highly significant but of low magnitude. For example, if you knew with near certainty that a particular dietary supplement extended your life span, on average, by 2 weeks, would you care? Probably not. Even though the finding is highly significant (near certain), the magnitude of the effect is so low that it basically doesn’t matter. Yet it is common in scientific studies, and in press reports about them, to only emphasize the significance of a finding but not the magnitude. Sometimes authors don’t even bother to report effect sizes at all, they only report &lt;em>P&lt;/em> values and point out how significant their results are. This is bad science. The &lt;em>P&lt;/em> value is primarily a measure of the data set size. The larger the data set, the lower the &lt;em>P&lt;/em> value, all else equal. To be important, an effect has to have a large magnitude; just being highly significant is not enough.&lt;/p>
&lt;/div>
&lt;div id="correlation-is-not-causation" class="section level2">
&lt;h2>Correlation is not causation&lt;/h2>
&lt;p>This issue is pretty well known, yet people fall into this trap over and over again. Just because one quantity shows a statistical association (correlation) with another variable doesn’t mean that one of the two variables causes the other variable. This problem is more common in press reporting about scientific studies than in the studies themselves. For example, a study might report an association between cell-phone use and cancer. In the study, the authors might be careful to point out that they don’t know why increased cell-phone use correlates with cancer in their study population, and that the underlying cause might be unrelated to cell-phone use (e.g., for some reason exposure to a carcinogen correlates with cell-phone use in the study population). Yet, inevitably the press release about this study will read “cell-phone use causes cancer.”&lt;/p>
&lt;p>In general, to reliably assign cause and effect, one needs to carry out an &lt;em>experimental study&lt;/em>. In an experimental study, a population is randomly subdivided into treatment and control groups, and the treatment group is subjected to a well-defined experimental manipulation. For example, people are divided into two groups at random, one group is made to use a cell phone for 2 hours each day, the other group is forbidden from using a cell phone ever. After 5 years, count which group developed more cancer. By contrast, studies that only show association but not causation are usually &lt;em>observational studies&lt;/em>. In such studies, we simply observe what variables are associated with each other in a sample.&lt;/p>
&lt;/div>
&lt;div id="focusing-on-tenth-order-effects-and-ignoring-first-order-effects" class="section level2">
&lt;h2>Focusing on tenth-order effects and ignoring first-order effects&lt;/h2>
&lt;p>This issue is not so much an issue of poor statistics but rather of poor placement of emphasis. It is very well explained by Peter Attia &lt;a href="http://eatingacademy.com/nutrition/irisin-the-magic-exercise-hormone">here&lt;/a>, so I’ll not elaborate on it any further here.&lt;/p>
&lt;/div>
&lt;div id="aggregation-by-quantiles-erroneously-amplifies-trend" class="section level2">
&lt;h2>Aggregation by quantiles erroneously amplifies trend&lt;/h2>
&lt;p>In many situations, one would like to know how one quantitative variable relates to another. For example, we might be studying a certain bird species and ask whether the amount of a certain berry that males of that species eat has an effect on the mating success of those males. The canonical (and correct) way to study such questions is via correlation analyses.&lt;/p>
&lt;p>However, it is surprisingly common to see analyses where instead one of the variables is aggregated into quantiles (groups of equal size) and the second variable is presented as an average of the quantiles of the first variable. In the above example, we might classify birds into four groups (quartiles) by their berry consumption (lowest 25%, second lowest 25%, and so on) and then plot the mean mating success within each group as a function of the quartile of berry consumption. Such an analysis is misleading, because it erroneously amplifies any relationship that may exist between the two variables.&lt;/p>
&lt;p>Let’s illustrate this issue with some simulated data, using R. First we generate two variables &lt;em>x&lt;/em> and &lt;em>y&lt;/em>, weakly correlated:&lt;/p>
&lt;pre>&lt;code>n &amp;lt;- 10000 # sample size
x &amp;lt;- rnorm(n) # generate first set of normal variates
y &amp;lt;- 0.1*x + 0.9*rnorm(n) # generate second set, weakly correlated with first
cor.test( x, y )&lt;/code>&lt;/pre>
&lt;p>This is the output from the &lt;code>cor.test()&lt;/code> function:&lt;/p>
&lt;pre>&lt;code># Pearson&amp;#39;s product-moment correlation
#
# data: x and y
# t = 10.485, df = 9998, p-value &amp;lt; 2.2e-16
# alternative hypothesis: true correlation is not equal to 0
# 95 percent confidence interval:
# 0.084862 0.123636
# sample estimates:
# cor
# 0.1042886 &lt;/code>&lt;/pre>
&lt;p>The correlation is highly significant (&lt;em>P&lt;/em> &amp;lt; 2.2e-16) but weak (&lt;em>r&lt;/em> = 0.10). The variable &lt;em>x&lt;/em> explains only 1% (that is the square of the correlation coefficient, &lt;em>r&lt;/em>^2) of the variation in &lt;em>y&lt;/em>. In terms of the birds example, this could mean that while berry consumption is indeed related to mating success, the relationship is so weak as to be virtually meaningless. (Knowing how many berries a given male bird ate tells me pretty much nothing about his specific mating success.)&lt;/p>
&lt;p>Figure &lt;a href="#fig:figure1">1&lt;/a> shows the relationship between &lt;em>x&lt;/em> and &lt;em>y&lt;/em>. As indicated by the correlation analysis, knowing &lt;em>x&lt;/em> doesn’t really tell us anything about &lt;em>y&lt;/em>.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure1">&lt;/span>
&lt;img src="Fig1.png" alt="Relationship between x and y in our made-up example dataset." width="75%" />
&lt;p class="caption">
Figure 1: Relationship between &lt;em>x&lt;/em> and &lt;em>y&lt;/em> in our made-up example dataset.
&lt;/p>
&lt;/div>
&lt;p>Now let’s aggregate the data into quantiles of &lt;em>x&lt;/em> and plot the mean +/- the standard error of &lt;em>y&lt;/em> within each quantile of &lt;em>x&lt;/em>:&lt;/p>
&lt;pre>&lt;code># calculate to which quantile each x belongs
qn &amp;lt;- 10 # number of quantiles
q &amp;lt;- quantile(x, probs = seq(0, 1, 1/qn))
q[qn] &amp;lt;- q[qn] + 1 # make sure the last quantile is larger than max(x)
quant.x &amp;lt;- tapply(x, 1:n, (function(x) sum(x&amp;gt;=q)))
# calculate means and SEs of y per quantile
library( Hmisc ) # for errbar plot
mean.quant &amp;lt;- tapply(y, quant.x, mean)
SE.quant &amp;lt;- tapply(y, quant.x, (function(x) sd(x)/sqrt(length(x))))
errbar(1:qn, mean.quant, mean.quant+SE.quant, mean.quant-SE.quant, xlab=&amp;#39;quantiles(x)&amp;#39;, ylab=&amp;#39;mean(y) for quantile&amp;#39;)
&lt;/code>&lt;/pre>
&lt;p>In this example, we chose 10 quantiles. The resulting graph is shown in Figure &lt;a href="#fig:figure2">2&lt;/a>.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure2">&lt;/span>
&lt;img src="Fig2.png" alt="Mean y (+/- standard error) as a function of quantiles of x." width="75%" />
&lt;p class="caption">
Figure 2: Mean &lt;em>y&lt;/em> (+/- standard error) as a function of quantiles of &lt;em>x&lt;/em>.
&lt;/p>
&lt;/div>
&lt;p>Suddenly, it looks like there is a very clear and quite strong relationship between &lt;em>x&lt;/em> and &lt;em>y&lt;/em>. If you were given only this graph, you might think that knowing how many berries a male eats would tell you a lot about that male’s mating success. Indeed, the top quantile, on average, has an approximately 200% higher y (200% higher mating success) than the bottom quantile.&lt;/p>
&lt;p>Also note the apparent nonlinearity. The top and bottom quantiles seem to have very much increased/reduced y relative to the middle ones. Note that we see no such feature in the scatter plot of the original &lt;em>x&lt;/em> and &lt;em>y&lt;/em> values.&lt;/p>
&lt;p>Finally, the exact same data look quite different depending on the exact number of quantiles. Figure &lt;a href="#fig:figure3">3&lt;/a> shows the same data presented with 6 quantiles.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure3">&lt;/span>
&lt;img src="Fig3.png" alt="Mean y (+/- standard error) as a function of quantiles of x. Now using 6 instead of 10 quantiles." width="75%" />
&lt;p class="caption">
Figure 3: Mean &lt;em>y&lt;/em> (+/- standard error) as a function of quantiles of &lt;em>x&lt;/em>. Now using 6 instead of 10 quantiles.
&lt;/p>
&lt;/div>
&lt;p>And Figure &lt;a href="#fig:figure4">4&lt;/a> shows the same data presented with 20 quantiles.&lt;/p>
&lt;div class="figure">&lt;span id="fig:figure4">&lt;/span>
&lt;img src="Fig4.png" alt="Mean y (+/- standard error) as a function of quantiles of x. Now using 20 instead of 10 quantiles." width="75%" />
&lt;p class="caption">
Figure 4: Mean &lt;em>y&lt;/em> (+/- standard error) as a function of quantiles of &lt;em>x&lt;/em>. Now using 20 instead of 10 quantiles.
&lt;/p>
&lt;/div>
&lt;p>As you can see, the same data look quite different depending on the exact number of quantiles we use.&lt;/p>
&lt;p>So, whenever somebody shows you data aggregated into quantiles, ask for an &lt;em>x&lt;/em>–&lt;em>y&lt;/em> scatter plot and a correlation coefficient. And then square the correlation coefficient and evaluate the % variance explained. A squared correlation coefficient below 0.1 (&lt;em>r&lt;/em> &amp;lt; 0.3) means the effect is pretty much non-existent, regardless of how low the &lt;em>P&lt;/em> value is.&lt;/p>
&lt;/div></description></item></channel></rss>